{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL as Probabilistic Inference",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRkIJNd2-5Mp",
        "colab_type": "text"
      },
      "source": [
        "# Reinforcement Learning as Probabilistic Inference\n",
        "\n",
        "\n",
        "**Author**: [Lisa Lee](https://leelisa.com/)\n",
        "\n",
        "This Colab notebook is based on Sergey Levine's tutorial ([Levine 2018](https://arxiv.org/abs/1805.00909)) that formalizes reinforcement learning (RL) as probabilistic inference. We apply this framework to a simple chain environment. We implement the corresponding graphical model for our task, then implement a standard sum-product inference algorithm to infer an optimal policy. We compare the learned soft Q-function $Q(s_t,a_t) = \\log \\beta_t(s_t,a_t)$ and policy $p(a_t \\mid s_t, O_{t:T}) = \\frac{\\beta_t(s_t, a_t)}{\\beta_t(s_t)}$ with that of a classic model-free RL algorithm, by evaluating the policies in the environment.\n",
        "\n",
        "**Acknowledgements**: Thanks to Maruan Al-Shedivat, Ben Eysenbach, and Emilio Parisotto for providing helpful feedback.\n",
        "\n",
        "**Note**: Please be careful not to publish solutions online, as this homework problem may be reused for future iterations of the [10-708 Probabilistic Graphical Model](https://sailinglab.github.io/pgm-spring-2019/) course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7HHnZUE-xzBc",
        "colab_type": "text"
      },
      "source": [
        "## I. Markov Decision Process  <a name=\"mdp\"></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9edK-aDIYZs",
        "colab_type": "text"
      },
      "source": [
        "### 1.1: Definition <a name=\"mdp-definition\"></a>\n",
        "\n",
        "Consider a simple chain environment consisting of $N$ states:\n",
        "\n",
        "```\n",
        "0 <-> 1 <-> ... <-> N-1\n",
        "```\n",
        "\n",
        "Suppose the agent always starts at state 0, and its task is to reach the goal state $N-1$. We formalize this task as a **Markov Decision Process** (MDP) consisting of a tuple $(\\mathcal{S}, \\mathcal{A}, P_0, \\mathcal{T}, r)$, where:\n",
        "\n",
        "* $\\mathcal{S} = \\{0,\\ldots, N-1\\}$ are the agent's possible **states**, indicating the agent's current location.\n",
        "* $\\mathcal{A} = \\{\\diamond, \\leftarrow, \\rightarrow\\}$ are the agent's possible **actions**. The directional actions $\\leftarrow, \\rightarrow$ correspond to moving left or right one state, respectively. The action $\\diamond$ corresponds to 'stay', i.e., staying in the same state.\n",
        "* $P_0: \\mathcal{S} \\rightarrow [0, 1]$ is the agent's **initial state distribution**, where we assume the agent always starts in state 0:\n",
        "$$\n",
        "P_0(s) = \\begin{cases}\n",
        "1 & \\text{if }s = 0\\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "* $\\mathcal{T}(s_{t+1} \\mid s_t, a_t)$  is the **transition dynamics** probability that action $a_t$ in state $s_t$ will lead to the next state $s_{t+1}$. Assume there is a \"action fail\" probability $\\epsilon \\in [0, 1]$ such that, at each time step $t$, the agent stays in the same cell regardless of the action it takes. Note that the action $\\diamond$ has deterministic transition dynamics: $\\mathcal{T}(s' \\mid s, \\diamond)=\\mathbb{1}(s' = s)$ for all $s, s' \\in \\mathcal{S}$.\n",
        "\n",
        "* $r(s_t,a_t) : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is a **reward** function that provides a supervision signal for taking action $a_t$ in state $s_t$, where\n",
        "$$\n",
        "r(s_t,a_t) = \\begin{cases}\n",
        "0 & \\text{if }s_t = N-1 \\\\\n",
        "0 & \\text{if }s_t = N-2\\text{ and }a_t = \\rightarrow \\\\\n",
        "-1 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "In other words, the agent receives 0 reward for taking an action such that its next state $s_{t+1}$ might be the goal, and -1 otherwise.\n",
        "\n",
        "Assume a finite-horizon MDP, and let $T$ be the episodic time horizon. Each episode in the MDP proceeds as follows:\n",
        "\n",
        "1. The agent spawns in an initial state according to $s_1 \\sim P_0(s)$.\n",
        "2. At each time step $t$, the agent observes its current cell state $s_t \\in \\mathcal{S}$, and takes an action $a_t \\sim \\pi(a \\mid s_t)$. The next state is determined by the transition dynamics $s_{t+1} \\sim \\mathcal{T}(s' \\mid s_t, a_t)$. The agent also receives a reward $r_t = r(s_t, a_t)$.\n",
        "4. The episode terminates once the agent reaches the goal, or if $t \\geq T$. (If the episode terminates at time step $t < T$, you can assume that $r_{t'} = 0$, $s_{t'} = s_t$, and $a_{t'} = \\diamond$ for $t' \\in \\{t+1, \\ldots, T\\}$.)\n",
        "\n",
        "Here, $\\pi(a \\mid s):\\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$ is the agent's **policy** which defines a probability distribution over actions $a \\in \\mathcal{A}$ given the current state $s$. A standard RL policy search problem aims to find an **optimal** policy that maximizes the expected cumulative reward:\n",
        "\n",
        "$$\n",
        "\\pi^* = \\underset{\\pi}{\\arg\\max}  \\mathbb{E}_{ \\substack{\n",
        "    s_1 \\sim p_0(S) \\\\\n",
        "    a_t \\sim \\pi(A \\mid s_t) \\\\\n",
        "    s_{t+1} \\sim \\mathcal{T}(S \\mid s_t, a_t)\n",
        "} }  \\left[ \\sum_{t=1}^{T} r(s_t, a_t) \\right]\n",
        "$$\n",
        "\n",
        "where the expectation is taken under the policy's distribution over **trajectories** $\\tau = \\{s_1, a_t, \\ldots, s_T, a_T \\}$ given by\n",
        "\n",
        "$$\n",
        "p_\\pi(\\tau) = p_\\pi(s_1, a_1, \\ldots, s_T, a_T)\n",
        "= P_0(s_1) \\prod_{t=1}^{T} \\pi(a_t \\mid s_t) \\mathcal{T}(s_{t+1} \\mid s_t, a_t)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNVZEKtwsHCe",
        "colab_type": "text"
      },
      "source": [
        "### 1.2: Gym Environment <a name=\"gym-env\"></a>\n",
        "\n",
        "Below, we provide an implementation of the simple chain environment, following the [OpenAI Gym API](https://gym.openai.com/docs/). It uses the following default environment parameters, which you may assume for the remainder of the tutorial:\n",
        "* Number of states is $N=|\\mathcal{S}| = 5$.\n",
        "* The \"action fail\" probability of the transition dynamics $\\mathcal{T}(s_{t+1} \\mid s_t, a_t)$ is $\\epsilon = 0.2$.\n",
        "* The maximum episode length is 50."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UpkQ8S_dXcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "from gym import spaces\n",
        "\n",
        "class ChainEnv(gym.Env):\n",
        "  def __init__(self, num_states=5, action_fail_prob=0.2, max_episode_length=50):\n",
        "    self.action_space = spaces.Discrete(3)  # no-action, left, right\n",
        "    self.state_space = spaces.Discrete(num_states)\n",
        "    self.goal_state = num_states - 1\n",
        "    \n",
        "    self.max_episode_length = max_episode_length\n",
        "\n",
        "    # self.transition_dynamics[s, a, S] = T(S|s,a).\n",
        "    self.transition_dynamics = np.zeros(\n",
        "        (self.state_space.n, self.action_space.n, self.state_space.n))\n",
        "    for s in range(self.state_space.n):\n",
        "      # Action 0: no-action\n",
        "      self.transition_dynamics[s, 0, s] = 1\n",
        "      \n",
        "      # Action 1: left\n",
        "      next_state = max(s - 1, 0)\n",
        "      self.transition_dynamics[s, 1, next_state] += 1 - action_fail_prob\n",
        "      self.transition_dynamics[s, 1, s] += action_fail_prob\n",
        "\n",
        "      # Action 2: right\n",
        "      next_state = min(s + 1, self.state_space.n - 1)\n",
        "      self.transition_dynamics[s, 2, next_state] += 1 - action_fail_prob\n",
        "      self.transition_dynamics[s, 2, s] += action_fail_prob\n",
        "\n",
        "    # self.reward[s, a] = r(s, a)\n",
        "    self.reward = - np.ones((self.state_space.n, self.action_space.n))\n",
        "    self.reward[self.state_space.n - 2, 2] = 0  # r(N-2,->) = 0\n",
        "    self.reward[self.state_space.n - 1] = 0     # r(N-1, a) = 0 for any action a\n",
        "\n",
        "    # Reset MDP to the beginning of an episode.\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.t = 0      # Time step counter\n",
        "    self.state = 0  # Agent always starts in state 0.\n",
        "    return self.state\n",
        "\n",
        "  def _sample_transition(self, state, action):\n",
        "    next_state_prob = self.transition_dynamics[state, action]\n",
        "    next_state = np.argmax(np.random.multinomial(1, next_state_prob))\n",
        "    return next_state\n",
        "\n",
        "  def step(self, action):\n",
        "    self.t += 1\n",
        "    rew = self.reward[self.state, action]\n",
        "    self.state = self._sample_transition(self.state, action)\n",
        "    done = self.state == self.goal_state or self.t >= self.max_episode_length\n",
        "    \n",
        "    # Add any useful diagnostic info here for debugging.\n",
        "    info = {}\n",
        "\n",
        "    return self.state, rew, done, info"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLnRCf-wvC4h",
        "colab_type": "text"
      },
      "source": [
        "### 1.3: Instantiation <a name=\"env-instantiation\"></a>\n",
        "\n",
        "We instantiate ChainEnv, and print some important instance variables:\n",
        "\n",
        "* `env.state_space` is the state space $\\mathcal{S}$.\n",
        "* `env.action_space` is the action space $\\mathcal{A}$.\n",
        "* `env.transition_dynamics[s, a, S]` is the transition dynamics probability $\\mathcal{T}(S \\mid s, a)$ that taking action $a$ in state $s$ results in the next state $S$.\n",
        "* `env.reward[s, a]` is the  reward matrix $r(s, a)$.\n",
        "\n",
        "\n",
        "Note that we represent the reward function $r(s_t, a_t)$ as a $|\\mathcal{S}| \\times |\\mathcal{A}|$ matrix, and the transition dynamics function $\\mathcal{T}(s_{t+1} \\mid s_t, a_t)$ as a $|\\mathcal{S}| \\times |\\mathcal{A}| \\times |\\mathcal{S}|$ matrix. We can do this since the state and action spaces are finite."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZhkPfP3vCkN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = ChainEnv()\n",
        "print('env.state_space.n  = |S| =', env.state_space.n)\n",
        "print('env.action_space.n = |A| =', env.action_space.n)\n",
        "\n",
        "# Print the distribution over next states for a given state and action.\n",
        "s = 0\n",
        "a = 2\n",
        "print('env.transition_dynamics[{0}, {1}, S] = T(S|s={0}, a={1}) = {2}'.format(\n",
        "    s, a, env.transition_dynamics[s, a, :]))\n",
        "\n",
        "# Print the state transition matrix T(s->S') for a given action.\n",
        "a = 1\n",
        "print('env.transition_dynamics[s, {0}, S] = T(S|s, a={0}) = \\n{1}'.format(\n",
        "    a, env.transition_dynamics[:, a, :]))\n",
        "\n",
        "print('env.reward[s, a] = \\n{}'.format(env.reward))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPu_i1236DZ_",
        "colab_type": "text"
      },
      "source": [
        "### 1.4: Evaluating a Policy <a name=\"evaluating-policy\"></a>\n",
        "\n",
        "We represent a policy $\\pi: \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$ as a $\\mathcal{S} \\times \\mathcal{A}$ matrix, where `policy[s, a]`$=\\pi(a \\mid s)$. Recall that a policy defines a probability distribution over actions, so each row of `policy` should sum to 1:\n",
        "$$\n",
        "1 = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\qquad \\text{for any }s \\in \\mathcal{S}.\n",
        "$$\n",
        "\n",
        "Below, we provide a function which runs a given `policy` for 100 episodes, and prints the mean/std of the cumulative reward $\\mathbb{E}\\left[ \\sum_{t=1}^T r(s_t, a_t) \\right]$ and the state visitation frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OGHiyjXQ0G6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "def evaluate(env, policy, num_episodes=100):\n",
        "  \"\"\"\n",
        "  Evaluates the given policy by running it for given num_episodes.\n",
        "\n",
        "  Args:\n",
        "    env (ChainEnv)    : The environment.\n",
        "    num_episodes (int): Number of episodes to simulate\n",
        "    policy (np.array) : |S| x |A| matrix where policy[s, a] = pi(a|s).\n",
        "  \"\"\"\n",
        "  print('Running the following policy[s, a] for {} episodes:\\n{}'.format(\n",
        "      num_episodes, policy))\n",
        "\n",
        "  states_counter = Counter()\n",
        "  returns = []\n",
        "  for _ in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    states_counter[state] += 1\n",
        "\n",
        "    done = False\n",
        "    total_rew = 0\n",
        "    while not done:\n",
        "      # Sample action from the policy for the current state.\n",
        "      action_probs = policy[state]\n",
        "      sample_onehot = np.random.multinomial(1, action_probs)\n",
        "      action = np.argmax(sample_onehot)\n",
        "      \n",
        "      # Execute action, and observe the next state and reward.\n",
        "      state, rew, done, info = env.step(action)\n",
        "\n",
        "      total_rew += rew\n",
        "      states_counter[state] += 1\n",
        "\n",
        "    # Store cumulative reward at the end of each episode.\n",
        "    returns.append(total_rew)\n",
        "\n",
        "  # Compute statistics for episodic returns (cumulative rewards).\n",
        "  avg_return = np.mean(returns)\n",
        "  std_return = np.std(returns)\n",
        "\n",
        "  # Compute normalized state visitation counts.\n",
        "  state_visitation = np.zeros(env.state_space.n)\n",
        "  total_count = sum(states_counter.values())\n",
        "  for state, count in states_counter.items():\n",
        "    state_visitation[state] = count / total_count\n",
        "\n",
        "  print('Average Total Reward: {}, std: {}'.format(avg_return, std_return))\n",
        "  print('State visitation frequency:', state_visitation)\n",
        "  print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzk4st2g7BX4",
        "colab_type": "text"
      },
      "source": [
        "#### 1.4.1: Uniform Policy\n",
        "\n",
        "The code below evaluates a uniformly random policy $\\pi(a \\mid s) = \\frac{1}{|\\mathcal{A}|}$ which chooses each action uniformly at random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-v7C_ZP7j_m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "uniform_policy = np.full((env.state_space.n, env.action_space.n),\n",
        "                         1. / env.action_space.n)\n",
        "evaluate(env, uniform_policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wm5NS8qU-jH3",
        "colab_type": "text"
      },
      "source": [
        "### 1.5: Reinforcement Learning <a name=\"rl\"></a>\n",
        "\n",
        "Below, we provide an implementation of **Q-Learning**, a classic model-free reinforcement learning algorithm that performs a simple value iteration update using the weighted average of the old value and the new information:\n",
        "\n",
        "$$\n",
        "Q(s_t,a_t) \\leftarrow (1 - \\alpha) Q(s_t, a_t) + \\alpha (r_t + \\gamma \\max_a Q(s_{t+1}, a_t))\n",
        "$$\n",
        "\n",
        "Here, $\\alpha \\in (0, 1]$ is the learning rate and $\\gamma \\in [0, 1]$ is the discount factor, which we assume to be fixed hyperparameters.\n",
        "\n",
        "Q-Learning uses an **epsilon-greedy strategy**: At each time step, it chooses a random action $a_t \\in \\mathcal{A}$ with probability $\\epsilon$, and the max action $a_t = \\arg\\max_a Q(s_t, a)$ otherwise.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOUam1jXaXVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_qlearning(env, num_epochs=10):\n",
        "  # Hyperparameters\n",
        "  alpha = 1.0\n",
        "  gamma = 0.9\n",
        "  epsilon = 0.1\n",
        "  \n",
        "  q_table = np.zeros([env.state_space.n, env.action_space.n])\n",
        "  \n",
        "  # Train Q-Learning.\n",
        "  for i in range(1, num_epochs):\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "      # Take the next action.\n",
        "      if np.random.uniform(0, 1) < epsilon:\n",
        "          action = env.action_space.sample()  # Explore action space\n",
        "      else:\n",
        "          action = np.argmax(q_table[state])  # Exploit learned values\n",
        "      next_state, rew, done, info = env.step(action)\n",
        "\n",
        "      old_value = q_table[state, action]\n",
        "      next_max = np.max(q_table[next_state])\n",
        "      new_value = (1 - alpha) * old_value + alpha * (rew + gamma * next_max)\n",
        "      q_table[state, action] = new_value\n",
        "\n",
        "      state = next_state\n",
        "  \n",
        "  # Compute policy pi(a|s) = argmax_a Q(s, a).\n",
        "  argmax_actions = np.argmax(q_table, axis=1)\n",
        "  policy = np.zeros([env.state_space.n, env.action_space.n])\n",
        "  for state, action in enumerate(argmax_actions):\n",
        "    policy[state, action] = 1\n",
        "\n",
        "  return q_table, policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXEPZTVRaUq3",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 1.5.1: Q-Learning\n",
        "Train Q-learning, then evaluate the learned greedy policy $\\pi_\\text{greedy}(a \\mid s) := \\arg\\max_a Q(s, a)$ on the environment.\n",
        "\n",
        "* Describe the policy $\\pi_\\text{greedy}$ in words -- how does it behave?\n",
        "* How does $\\pi_\\text{greedy}$ compare to the uniform policy in Section 1.4.1 in terms of average total reward?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaKsT0YV-jy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "q_table, q_policy = train_qlearning(env)\n",
        "print('Q(s,a):\\n{}\\n'.format(q_table))\n",
        "evaluate(env, q_policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU3cyrfCx3eO",
        "colab_type": "text"
      },
      "source": [
        "## II. Reinforcement Learning as Probabilistic Inference <a name=\"probabilistic-inference\"></a>\n",
        "\n",
        "In this section,  we formalize reinforcement learning as probabilistic inference, following the tutorial from [Levine 2018](https://arxiv.org/pdf/1805.00909.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehHcUQvFx6c1",
        "colab_type": "text"
      },
      "source": [
        "### 2.1: Differences from the Original Tutorial <a name=\"differences\"></a>\n",
        "\n",
        "In all sections of this Colab tutorial (except Exercise 2.3.1), we do **not** restrict the action prior $p(a_t \\mid s_t)$ to be uniform, so the joint optimality-action distribution\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(O_t = 1, a_t \\mid s_t)\n",
        "&= p(O_t = 1 \\mid s_t, a_t) p(a_t \\mid s_t)\\\\\n",
        "&= \\exp(r(s_t, a_t)) p(a_t \\mid s_t)\n",
        "\\end{aligned}\n",
        "$$\n",
        "is not necessarily proportional to $p(O_t = 1 \\mid s_t, a_t) := \\exp\\{ r(s_t, a_t) \\} $.  We also define the backward state-action message as\n",
        "$$\n",
        "\\beta_t(s_t, a_t) := p(O_{t:T}, a_t \\mid s_t),\n",
        "$$\n",
        "whereas [Levine 2018](https://arxiv.org/abs/1805.00909) defines it as $\\beta_t(s_t, a_t) = p(O_{t:T} \\mid s_t, a_t)$. This results in slightly different derivations for the backward message update equations and the optimal policy $p(a_t \\mid s_t, O_{t:T})$, which you will derive in Section 2.4. Thus, be careful when copying derivations from the original tutorial.\n",
        "\n",
        "In Section 2.3, you will prove that any non-uniform action prior $p(a_t \\mid s_t)$ can be incorporated into the reward function. We will later experiment with the Message-Passing algorithm using different action priors $p(a_t \\mid s_t)$ for a fixed reward function $r(s_t, a_t)$, which is equivalent to using a uniform action prior with different reward functions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sj4Rb43c-nFZ",
        "colab_type": "text"
      },
      "source": [
        "### 2.2: Graphical Model for Control <a name=\"pgm\"></a>\n",
        "\n",
        "We define a graphical model that allows us to embed control into the framework of PGMs.\n",
        "\n",
        "A task can be defined by a **reward** function $r(s_t, a_t)$. However, a graphical model has no notion of rewards or costs, so we introduce a binary random variable for **optimality** where $O_t = 1$ indicates that time step $t$ is optimal.\n",
        "\n",
        "![graphical model](https://leelisa.com/blog/assets/2019-02-06-max-entropy-rl/rl-pgm.png)\n",
        "\n",
        "We choose the distribution over $O_t$ to be\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(O_t = 1 \\mid s_t, a_t) &:= \\exp(r(s_t, a_t)) \\\\\n",
        "p(O_t = 1, a_t \\mid s_t)\n",
        "&= p(O_t = 1 \\mid s_t, a_t) p(a_t \\mid s_t) \\\\\n",
        "&= \\exp(r(s_t, a_t)) p(a_t \\mid s_t)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "where $p(a_t \\mid s_t)$ is an **action prior**. This leads to a natural posterior distribution over actions when we condition on $O_t = 1$ for all $t \\in [T]$:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "p(\\tau \\mid o_{1:T})\n",
        "\\propto p(\\tau, o_{1:T})\n",
        "&= P_0(s_1) \\prod_{t=1}^T p(O_t = 1, a_t \\mid s_t) \\mathcal{T}(s_{t+1} \\mid s_t, a_t) \\\\\n",
        "&= P_0(s_1) \\prod_{t=1}^T \\exp(r(s_t, a_t)) p(a_t \\mid s_t) \\mathcal{T}(s_{t+1} \\mid s_t, a_t) \\\\\n",
        "&=\\underbrace{ \\left[ P_0(s_1) \\prod_{t=1}^T \\mathcal{T}(s_{t+1} \\mid s_t, a_t) p(a_t \\mid s_t) \\right] }_{\\text{dynamics}} \\exp  \\underbrace{ \\left( \\sum_{t=1}^T r(s_t, a_t)  \\right)  }_{\\text{total reward}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Observe that, if we know the dynamics, then we can **infer a likely optimal trajectory** using\n",
        "$$\n",
        "\\tau^* = \\underset{\\tau}{\\arg\\max} \\; p(\\tau, o_{1:T}).\n",
        "$$\n",
        "\n",
        "For the remainder of this tutorial, we use $O_{t:T}$ to denote $O_{t:T}=1$ for conciseness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjduvpg3Ywy3",
        "colab_type": "text"
      },
      "source": [
        "### 2.3: Reward vs. Action Prior <a name=\"reward-action-prior\"></a>\n",
        "\n",
        "Note that the joint distribution  $p(\\tau, o_{1:T})$ has an extra factor $p(a_t \\mid s_t)$ compared to Eq. (4) in [Levine 2018](https://arxiv.org/abs/1805.00909), due to the fact that\n",
        "$$\n",
        "\\begin{equation}\n",
        "p(O_t = 1, a_t \\mid s_t) = p(O_t = 1 \\mid s_t, a_t) p(a_t \\mid s_t)\n",
        "\\end{equation}.\n",
        "$$\n",
        "This is a typo in the original tutorial, but the math ends up being OK (by changing some equalities '$=$' to proportionalities '$\\propto$') if we assume a **uniform action prior** $p(a_t \\mid s_t) = \\frac{1}{|\\mathcal{A}|}$, because then $p(O_t = 1, a_t \\mid s_t) \\propto p(O_t = 1 \\mid s_t, a_t)$.\n",
        "\n",
        "[Levine 2018](https://arxiv.org/abs/1805.00909) assumes a uniform action prior, and argues that this assumption does not introduce any loss of generality. In the following exercise, you will show that any non-uniform action prior $p(a_t \\mid s_t)$ can be incorporated into $p(O_t \\mid s_t, a_t) := \\exp\\{ r_1(s_t, a_t) \\}$ via the reward function $r_1(s_t, a_t)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2-Sqz5NyQ-S",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 2.3.1: Non-Uniform Action Priors\n",
        "\n",
        "Let $r(s_t, a_t)$ and $p(a_t \\mid s_t)$ be any given reward function and action prior, respectively.  Show that there exists some reward function $r_1(s_t, a_t)$ such that the posterior distribution $p(\\tau \\mid o_{1:T})$ is equal for the following combinations of reward function and action prior:\n",
        "1. The reward function $r(s_t, a_t)$ and action prior $p(a_t \\mid s_t)$.\n",
        "2. The reward function $r_1(s_t, a_t)$ and a uniform action prior.\n",
        "\n",
        "Write down the expression for $r_1(s_t, a_t)$ in terms of $r(s_t, a_t)$ and $p(a_t \\mid s_t)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-drW6OXDIsyx",
        "colab_type": "text"
      },
      "source": [
        "### 2.4: Message-Passing Derivations <a name=\"message-passing-derivations\"></a>\n",
        "\n",
        "In this section, we will derive a standard sum-product inference algorithm to **infer the optimal policy** $p(a_t \\mid s_t, O_{t:T})$. Define the following **backward messages** for $t \\in \\{T, \\ldots, 1\\}$:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\beta_t(s_t, a_t) &:= p(O_{t:T}, a_t \\mid s_t) \\\\\n",
        "\\beta_t(s_t) &:= p(O_{t:T} \\mid s_t)\n",
        "\\end{aligned}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ms_-5mMHyOEB",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 2.4.1: Derivation of $\\beta_t(s_t)$ Update\n",
        "\n",
        "Show that the backward messages satisfy the following update equation for $\\beta_t(s_t)$:\n",
        "\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\beta_t(s_t)\n",
        "= \\sum_{a_t \\in \\mathcal{A}} \\beta_t(s_t, a_t)\n",
        "\\tag{1}\n",
        "\\end{equation}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIoNgwULyMYZ",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 2.4.2: Derivation of $\\beta_t(s_t, a_t)$ Update\n",
        "\n",
        "Show that the backward messages satisfy the following update equation for $\\beta_t(s_t, a_t)$:\n",
        "$$\n",
        "\\begin{equation}\n",
        "\\beta_t(s_t, a_t)\n",
        "= \\sum_{s_{t+1} \\in \\mathcal{S}} \\beta_{t+1}(s_{t+1}) \\mathcal{T}(s_{t+1} \\mid s_t, a_t) p(O_t, a_t \\mid s_t)\n",
        "\\tag{2}\n",
        "\\end{equation}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3D8We-ByyIEa",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 2.4.3: Derivation of the Optimal Policy\n",
        "\n",
        "Show that the optimal policy $p(a_t \\mid s_t, O_{t:T})$ satisfies\n",
        "$$\n",
        "\\begin{equation}\n",
        "p(a_t \\mid s_t, O_{t:T})\n",
        "= \\frac{ \\beta_t(s_t, a_t) }{ \\beta_t(s_t) }\n",
        "\\tag{3}\n",
        "\\end{equation}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJeqs7LEXxDH",
        "colab_type": "text"
      },
      "source": [
        "### 2.5: Message-Passing Implementation <a name=\"message-passing-implementation\"></a>\n",
        "\n",
        "In this section, we provide code for running the message-passing algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZW_ZtuYDj7O",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "#### 2.5.1: Message-Passing Algorithm\n",
        "\n",
        "Below, we provide an implementation of the recursive message-passing algorithm for computing the backward messages $\\beta_t(a_t, s_t)$ and $\\beta_t(s_t)$. The algorithm starts from the last time step $t=T$,\n",
        "$$\n",
        "\\beta_T(s_T, a_T) = p(O_T, a_T \\mid s_T),\n",
        "$$\n",
        "and proceeds backwards through time to $t=1$, computing the following updates at each time step:\n",
        "\n",
        "1. Update $\\beta_{t}(s_{t}, a_{t})$ using Eq. (2).\n",
        "\n",
        "2. Update $\\beta_t(s_t)$ using Eq. (1).\n",
        "\n",
        "\n",
        "Then the algorithm infers the optimal policy $p(a \\mid s, O_{1:T})$ \n",
        "using Eq. (3).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6sMC28B8pil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_sum_exp(terms):\n",
        "    \"\"\"\n",
        "    Uses log-sum-exp trick to compute\n",
        "      log(\\sum_i exp(terms[i])) = t* + log(\\sum_i exp(terms[i] - t*))\n",
        "    where t* = max(terms).\n",
        "    \"\"\"\n",
        "    max_term = np.max(terms)\n",
        "    diff = np.exp(terms - max_term)\n",
        "    result = max_term + np.log(np.sum(diff))\n",
        "    return result\n",
        "  \n",
        "class MessagePassing():\n",
        "  def __init__(self, env, action_prior=None):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "      env: GridworldEnv object\n",
        "      action_prior: Action prior p(a|s), which is a matrix of shape |S| x |A|.\n",
        "                    If None, we use a uniform action prior: p(s|a) = 1/|A|.\n",
        "    \"\"\"\n",
        "    self.env = env\n",
        "\n",
        "    # self.log_action_prior[s, a] = log p(a|s)\n",
        "    if action_prior is None:\n",
        "      # Uniform action prior: p(s|a) = 1/|A|.\n",
        "      self.log_action_prior = np.full(\n",
        "          (self.env.state_space.n, self.env.action_space.n),\n",
        "          np.log(1. / self.env.action_space.n))\n",
        "    else:\n",
        "      assert (action_prior.shape[0] == self.env.state_space.n and\n",
        "              action_prior.shape[1] == self.env.action_space.n)\n",
        "      eps = 1e-10  # to prevent underflow\n",
        "      self.log_action_prior = (np.log(action_prior + eps) -\n",
        "                               np.log(1 + eps * self.env.action_space.n))\n",
        "\n",
        "    # self.log_transition_dynamics[s, a, S] = log T(S|s,a)\n",
        "    self.log_transition_dynamics = np.log(env.transition_dynamics)\n",
        "    \n",
        "    # self.log_optimality_dist[s, a] = log p(O|s,a) = r(s,a)\n",
        "    self.log_optimality_dist = env.reward\n",
        "\n",
        "    # self.log_optimality_action_dist[s, a] = log p(O,a|s)\n",
        "    #                                       = log p(O|s,a) + log p(a|s).\n",
        "    self.log_optimality_action_dist = (self.log_optimality_dist +\n",
        "                                       self.log_action_prior)\n",
        "\n",
        "    # self.log_state_action_message[s, a] = log beta(s, a).\n",
        "    # It is initialized to the log optimality distribution, log p(O_T,a_T|s_T).\n",
        "    self.log_state_action_message = (self.log_optimality_dist +\n",
        "                                     self.log_action_prior)\n",
        "\n",
        "    # self.state_message[s] = beta(s).\n",
        "    self.log_state_message = self._compute_state_message_update()\n",
        "\n",
        "    # self.policy[s, a] = p(a|s,O) = beta(s,a) / beta(s).\n",
        "    self.policy = self.compute_policy()\n",
        "\n",
        "  def _compute_state_message_update(self):\n",
        "    \"\"\"\n",
        "    Computes the state-message update in Eq. (1) in log-space:\n",
        "      log beta(s) = log(\\sum_a exp(log beta(s,a))).\n",
        "    \"\"\"\n",
        "    log_state_message = np.zeros(self.env.state_space.n)\n",
        "    for s in range(self.env.state_space.n):\n",
        "      log_state_message[s] = log_sum_exp(self.log_state_action_message[s, :])\n",
        "    return log_state_message\n",
        "\n",
        "  def _compute_state_action_message_update(self):\n",
        "    \"\"\"\n",
        "    Computes the state-action-message update in Eq. (2) in log-space:\n",
        "      log beta(s,a) = log(\\sum_S exp(log beta(S) + log T(S|s,a) + log p(O,a|s)))\n",
        "    \"\"\"\n",
        "    log_state_action_message = np.zeros(\n",
        "        (self.env.state_space.n, self.env.action_space.n))\n",
        "    for s in range(self.env.state_space.n):\n",
        "      for a in range(self.env.action_space.n):\n",
        "        terms = (self.log_state_message +\n",
        "                 self.log_transition_dynamics[s, a] +\n",
        "                 self.log_optimality_action_dist[s, a])\n",
        "        log_state_action_message[s, a] = log_sum_exp(terms)\n",
        "    return log_state_action_message\n",
        "\n",
        "  def update_messages(self):\n",
        "    \"\"\"\n",
        "    Performs a single step of the backward message-passing algorithm.\n",
        "    \"\"\"\n",
        "    # beta(s,a) = \\sum_S beta(S) * p(S|s,a) * p(O,a|s)\n",
        "    self.log_state_action_message = self._compute_state_action_message_update()\n",
        "    \n",
        "    # beta(s) = \\sum_a beta(s,a) * p(a|s)\n",
        "    self.log_state_message = self._compute_state_message_update()\n",
        "\n",
        "    return self.log_state_action_message, self.log_state_message\n",
        "\n",
        "  def compute_policy(self):\n",
        "    \"\"\"\n",
        "    Computes policy using the given backward messages:\n",
        "      p(a|s,O) = beta(s,a) / beta(s).\n",
        "    \"\"\"\n",
        "    log_policy = np.zeros((env.state_space.n, env.action_space.n))\n",
        "    for s in range(env.state_space.n):\n",
        "      log_policy[s] = (self.log_state_action_message[s] -\n",
        "                       self.log_state_message[s])\n",
        "    policy = np.exp(log_policy)\n",
        "    return policy\n",
        "\n",
        "  def get_log_messages(self):\n",
        "    \"\"\"\n",
        "    Returns the soft value function and soft Q-function:\n",
        "      V(s)   := log beta(s)\n",
        "      Q(s,a) := log beta(s,a).\n",
        "    \"\"\"\n",
        "    return self.log_state_message, self.log_state_action_message"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tn0e27eAC-R9",
        "colab_type": "text"
      },
      "source": [
        "#### 2.5.2: Script to run Message-Passing\n",
        "\n",
        "We provide a function that runs message passing for $T$ steps using the given action prior $p(a_t \\mid s_t)$. It prints the provided action prior, and the learned soft value function and soft Q-function:\n",
        "$$\n",
        "\\begin{aligned}\n",
        "V(s) &:= \\log \\beta_1(s) \\\\\n",
        "Q(s,a) &:= \\log \\beta_1(s, a)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "Finally, it evaluates the policy on ChainEnv and prints the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h-PcpL3CDA3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_message_passing(env, action_prior, T=50):\n",
        "  mp = MessagePassing(env, action_prior=action_prior)\n",
        "  for _ in range(T):\n",
        "    state_action_message, state_messages = mp.update_messages()\n",
        "  \n",
        "  # Print the soft value function V(s) and soft Q-function Q(s,a).\n",
        "  V, Q = mp.get_log_messages()\n",
        "\n",
        "  # Evaluate the learned policy for num_episodes.\n",
        "  policy = mp.compute_policy()\n",
        "  \n",
        "  print('Action prior p(a|s):\\n{}\\n'.format(action_prior))\n",
        "  print('V(s):\\n{}\\n'.format(V))\n",
        "  print('Q(s,a):\\n{}\\n'.format(Q))\n",
        "  evaluate(env, policy=policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "761qbRF0xLSx",
        "colab_type": "text"
      },
      "source": [
        "### 2.6: Message-Passing Experiments <a name=\"message-passing-experiments\"></a>\n",
        "\n",
        "In this section, we will experiment running Message-Passing with different action priors $p(a_t \\mid s_t)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY2GWRljEUNZ",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 2.6.1: Uniform Action Prior\n",
        "\n",
        "Run message-passing algorithm using a **uniform** action prior $p(a_t \\mid s_t) = \\frac{1}{|\\mathcal{A}|}$.\n",
        "\n",
        "* How does the learned policy compare to the Q-Learning policy $\\pi_\\text{greedy}$ from Exercise 1.5.1 in terms of behavior and average total reward?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pS9oxbWtTqQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run message-passing using a uniform action prior.\n",
        "uniform_policy = np.full((env.state_space.n, env.action_space.n),\n",
        "                         1. / env.action_space.n)\n",
        "run_message_passing(env, action_prior=uniform_policy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kicwFHafR6eY",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 2.6.2: Soft Action Prior\n",
        "\n",
        "The following function `construct_policy()` returns a policy $\\pi(a \\mid s; \\phi)$ such that, regardless of the state, it takes the 'right' action with probability $\\phi$, and all other actions uniformly at random. That is:\n",
        "$$\n",
        "\\pi(a \\mid s;\\phi) = \\begin{cases}\n",
        "\\phi & \\text{if }a=\\rightarrow\\\\\n",
        "\\frac{1 - \\phi}{|\\mathcal{A}| - 1}& \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Run message-passing algorithm using a \"soft\" action prior $\\pi(a \\mid s;\\phi)$ for $\\phi = 0.5$.\n",
        "\n",
        "* How does the learned policy compare to the one from Exercise 2.6.1 (using uniform action prior) in terms of behavior and average total reward?\n",
        "* (True or False) If $\\phi > \\frac{1}{|\\mathcal{A}|}$, then using the action prior $\\pi(a \\mid s ; \\phi)$ instead of a uniform action prior is equivalent to changing the reward function $r(s_t, a_t)$ such that the agent receives relatively greater reward for taking the action $a_t = \\rightarrow$ in any state, and less reward otherwise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1Bw5G3aw0FY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def construct_policy(env, phi=0.5):\n",
        "  \"\"\"\n",
        "  Returns a policy such that, regardless of the state, it takes the given action\n",
        "  with probability phi, and all other actions uniformly at random.\n",
        "  \"\"\"\n",
        "  right_action = 2\n",
        "  policy = np.zeros((env.state_space.n, env.action_space.n))\n",
        "  for s in range(env.state_space.n):\n",
        "    policy[s, right_action] = phi\n",
        "    for a in range(env.action_space.n):\n",
        "      if a != right_action:\n",
        "        policy[s, a] = (1. - phi) / (env.action_space.n - 1)\n",
        "  return policy\n",
        "\n",
        "# A \"soft\" policy that chooses 'right' with probability 0.5.\n",
        "pi = construct_policy(env, phi=0.5)\n",
        "run_message_passing(env, action_prior=pi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBMhMS2LA5mn",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 2.6.3: Hard Action Prior\n",
        "\n",
        "Run message-passing algorithm using a \"hard\" action prior $\\pi(s,a;\\phi)$ for $\\phi = 1.0$.\n",
        "\n",
        "* How does the learned policy compare to the Q-Learning policy $\\pi_\\text{greedy}$ from Exercise 1.5.1 in terms of behavior and average total reward?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sClmOw4e-ymT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A \"hard\" policy that chooses 'right' with probability 1.0.\n",
        "pi = construct_policy(env, phi=1.0)\n",
        "run_message_passing(env, action_prior=pi)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-Mrq2dC6p3D",
        "colab_type": "text"
      },
      "source": [
        "## III. Concluding Remarks <a name=\"concluding-remarks\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q0EyKQp_s67b",
        "colab_type": "text"
      },
      "source": [
        "### 3.1: Q-Learning vs. Message-Passing <a name=\"q-learning-vs-message-passing\">\n",
        "\n",
        "In this section, we will address how Q-Learning and Message-Passing are different, and when might one be preferred."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg5363awyXs5",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 3.1.1: Unknown Transition Dynamics\n",
        "\n",
        "Suppose we don't know the transition dynamics $\\mathcal{T}(s_{t+1} \\mid s_t, a_t)$.\n",
        "\n",
        "1. Can you learn the optimal policy via Q-learning?\n",
        "\n",
        "2. Can you learn the optimal policy via Message-Passing?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-9pDvZdybRq",
        "colab_type": "text"
      },
      "source": [
        "#### Exercise 3.1.2: Equivalence\n",
        "\n",
        "Is it the case that the optimal message-passing policy can be equivalent to the one discovered by Q-learning? If yes, under which conditions? If no, why not?\n"
      ]
    }
  ]
}