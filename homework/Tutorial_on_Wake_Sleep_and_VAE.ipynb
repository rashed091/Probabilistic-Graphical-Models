{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gm6BDXIdvN3A"
   },
   "source": [
    "# Tutorial on Wake-Sleep and VAE\n",
    "\n",
    "**Author**: [Lisa Lee](https://leelisa.com/)\n",
    "\n",
    "This tutorial introduces the reader to the Wake-Sleep algorithm [1,2] and the Variational Autoencoder (VAE) [3, 4, 5].\n",
    "\n",
    "*Note*: Please be careful not to publish solutions online, as this problem may be reused for future iterations of the [10-708 Probabilistic Graphical Model](https://sailinglab.github.io/pgm-spring-2019/) course.\n",
    "\n",
    "### References\n",
    "\n",
    "[1] Hinton et al., \"The wake-sleep algorithm for\n",
    "unsupervised neural networks\", Science 1995. [[PDF](http://www.cs.toronto.edu/~fritz/absps/ws.pdf)]\n",
    "\n",
    "[2] Dayan, \"Helmholtz machines and wake-sleep learning\", 2000. [[PDF](https://pdfs.semanticscholar.org/dd8c/da00ccb0af1594fbaa5d41ee639d053a9cb2.pdf)]\n",
    "\n",
    "[3] Kingma & Welling, \"Auto-encoding variational bayes\", 2013. [[arXiv](https://arxiv.org/abs/1312.6114)]\n",
    "\n",
    "[4] Doersch, \"Tutorial on variational autoencoders\", 2016. [[arXiv](https://arxiv.org/abs/1606.05908)]\n",
    "\n",
    "[5] Jan Hendrick Metzen, \"Variational Autoencoder in TensorFlow\", 2015. [[Blog](https://jmetzen.github.io/2015-11-27/vae.html)]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZdT92K1Wzk1"
   },
   "source": [
    "# Part 1: Algorithm Derivations\n",
    "\n",
    "In this section, you will derive the Wake-Sleep and VAE algorithms, both of which can be used to train a Helmholtz Machine.\n",
    "\n",
    "This section is organized as follows:\n",
    "\n",
    "1. Definition of Helmholtz Machine, and derivation of the **evidence lower bound objective (ELBO)**, which lowerbounds the data log-likelihood $\\log p_\\theta(\\mathbf{x})$.\n",
    "\n",
    "3. Derivation of the **Wake-Sleep** algorithm, which alternates between the Wake phase and Sleep phase to optimize an estimate of ELBO.\n",
    "\n",
    "4. Derivation of **VAE**, which optimizes a stochastic estimate of ELBO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OvAatxgeZESG"
   },
   "source": [
    "## I. Variational Inference\n",
    "\n",
    "Suppose we want to learn a directed latent variable model\n",
    "\n",
    "<img src=\"https://iaml.it/blog/variational-autoencoders-1/images/iOOXa6P.png\" alt=\"drawing\" width=\"200\"/>\n",
    "\n",
    "that is able represent a complex distribution $p(\\mathbf{x})$  over the data in the following form:\n",
    "\\begin{equation}\n",
    "    p_{\\boldsymbol \\theta}(\\mathbf{x}) = \\int p_{\\boldsymbol \\theta}(\\mathbf{x} \\mid \\mathbf{z}) p(\\mathbf{z}) d\\mathbf{z}\n",
    "\\end{equation}\n",
    "where:\n",
    "* The latent variables $\\mathbf{z}$ are distributed according to a standard Gaussian prior $p(\\mathbf{z}) = N(0, I)$.\n",
    "* The data $\\mathbf{x}$ are binary vectors. In other words, $p_{\\boldsymbol \\theta}(\\mathbf{x} \\mid \\mathbf{z})$ can be modeled with a sigmoid belief net, so the likelihood is of the form $p_\\theta(\\mathbf{x}|\\mathbf{z}) = \\text{Bernoulli}( f_\\theta (\\mathbf{z}) )$.\n",
    "\n",
    "*Note*: In MNIST,  the data points $\\mathbf{x}$ take values in $[0, 1]$ rather than $\\{ 0, 1\\}$, but the loss term $\\mathbb{E}_{q}[p_\\theta(\\mathbf{x} \\mid \\mathbf{z})]$ still uses sigmoid cross-entropy loss, which is a common practice [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZULqchohUNHb"
   },
   "source": [
    "### 1.1: Helmholtz Machine\n",
    "\n",
    "The Helmholtz machine [2] is an architecture that can find hidden structure in data by learning a generative model of the data. \n",
    "It consists of two networks: the **recognition** network $q_\\phi(\\mathbf{z} \\mid \\mathbf{x})$ embeds the input data $\\mathbf{x}$ into latent space, and the **generative** network $p_\\theta(\\mathbf{x} \\mid \\mathbf{z})$ reconstructs the data given a latent code $\\mathbf{z}$.\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2017-08-08/666ab73d8095710b2d377e129f5e7959fcf62a9f/9-Figure1-1.png\" alt=\"drawing\" width=\"300\"/>\n",
    "\n",
    "More specifically, the Helmholtz machine tries to learn the **recognition** parameters $\\phi$ and **generative** parameters $\\theta$ such that $$\n",
    "q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\approx p_\\theta(\\mathbf{z} \\mid \\mathbf{x}) \\propto p_\\theta(\\mathbf{x}, \\mathbf{z})\n",
    "$$\n",
    "where:\n",
    "\n",
    "* $q_\\phi(\\mathbf{z} \\mid \\mathbf{x})$ is the variational distribution approximating the posterior distribution $p_\\theta(\\mathbf{z} \\mid \\mathbf{x})$ over the latents $\\mathbf{z}$ given the data $\\mathbf{x}$. Assume that the recognition network $q_\\phi$ is parameterized by a Gaussian, i.e., $$\n",
    "q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) = N(\\mathbf{z}; \\mu_\\phi(\\mathbf{x}), \\Sigma_\\phi (\\mathbf{x})).\n",
    "$$\n",
    "\n",
    "* $p_\\theta(\\mathbf{x}, \\mathbf{z}) = p(\\mathbf{z}) p_\\theta(\\mathbf{x} \\mid \\mathbf{z})$ is the joint probability of $(\\mathbf{x}, \\mathbf{z})$, where $p(\\mathbf{z})=N(0,I)$ is the prior distribution over latents, and $p_\\theta(\\mathbf{x} \\mid \\mathbf{z})$ is the data likelihood.\n",
    "\n",
    "\n",
    "Helmholtz machines are usually trained using unsupervised learning algorithms such as the classical **Wake-Sleep** algorithm [1, 2] or the modern **Variational Autoencoder (VAE)** [3, 4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pMTH6OKWOMz_"
   },
   "source": [
    "### 1.2: Evidence Lower Bound Objective (ELBO)\n",
    "\n",
    "Suppose we want to approximate the posterior distribution $p_{\\boldsymbol \\theta}(\\mathbf{z} \\mid \\mathbf{x})$ using some variational distribution $q_{\\boldsymbol \\phi}(\\mathbf{z} \\mid \\mathbf{x})$. A tractable way to learn this model is to optimize the **evidence lower bound objective (ELBO)**, also known as the variational lower bound, defined as follows:\n",
    "$$\n",
    "\\mathcal{L}(\\mathbf{x})\n",
    "= \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x})} [\\log p_\\theta(\\mathbf{x}, \\mathbf{z}) - \\log q_\\phi(\\mathbf{z} \\mid \\mathbf{x})]\n",
    "= \\int_z q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\log \\frac{p_\\theta(\\mathbf{x}, \\mathbf{z})}{q_\\phi(\\mathbf{z} \\mid \\mathbf{x})} d \\mathbf{z}\n",
    "\\tag{1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-fL0_OQqOO7G"
   },
   "source": [
    "#### Exercise 1.2.1: ELBO\n",
    "\n",
    "For a single data point $\\mathbf{x}^{(i)}$, prove that\n",
    "$$\n",
    "    \\log p_\\theta(\\mathbf{x}^{(i)})\n",
    "    \\geq \\mathcal{L}(\\mathbf{x}^{(i)}).\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-9u6FK0OQx9"
   },
   "source": [
    "#### Section 1.2.2: Lower Bound on the Data Log-Likelihood\n",
    "The above result shows that, for iid data points $\\mathbf{x} = \\{\\mathbf{x}^{(i)}\\}_{i=1}^N$,\n",
    "$$\n",
    "    \\log p_\\theta(\\mathbf{x}) \\stackrel{\\text{iid}}{=} \\sum_{i=1}^N \\log p_\\theta(\\mathbf{x}^{(i)})\n",
    "    \\geq \\sum_{i=1}^N \\mathcal{L}(\\mathbf{x}^{(i)})\n",
    "    = \\mathcal{L}(\\mathbf{x})\n",
    "$$\n",
    "which gives the ELBO $\\mathcal{L}(\\mathbf{x})$ on the data log-likelihood $\\log p_\\theta(\\mathbf{x})$. Thus, maximizing this lower bound $\\mathcal{L}(\\mathbf{x})$ forces the true objective $p_\\theta(\\mathbf{x})$ to be optimized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HTRttWsTbRrD"
   },
   "source": [
    "### 1.3: A Tighter Lower Bound\n",
    "\n",
    "To compare trained models, we could simply look at the values of the ELBO. However, the bound could be loose and hence the numbers could be misleading. Here, we derive and prove a tighter approximation of the lower bound on the marginal likelihood, defined as follows:\n",
    "$$\n",
    "\\mathcal{L}_k(\\mathbf{x}) = \\mathbb{E}_{\\mathbf{z}^{(1)}, \\dots, \\mathbf{z}^{(k)} \\sim q_{\\mathbf{\\phi}}(\\mathbf{z} \\mid \\mathbf{x})} \\left[ \\log \\frac{1}{k} \\sum_{i=1}^k \\frac{p_{\\mathbf{\\theta}}(\\mathbf{x}, \\mathbf{z}^{(i)})}{q_{\\mathbf{\\phi}}(\\mathbf{z}^{(i)} \\mid \\mathbf{x})} \\right]\n",
    "\\tag{2}\n",
    "$$\n",
    "    \n",
    "In Part 2, you will use this alternate lower bound $\\mathcal{L}_k(\\mathbf{x})$ in Eq. (2) to **evaluate** trained models. For **training** Wake-Sleep and VAE, you will optimize the ELBO $\\mathcal{L}(\\mathbf{x})$ in Eq. (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dPfGvfyUbX6f"
   },
   "source": [
    "#### Exercise 1.3.1\n",
    "\n",
    "Prove that $\\log p(\\mathbf{x}) \\geq \\mathcal{L}_{k}(\\mathbf{x})$ for any $k \\in \\mathbb{N}$. (*Hint*: Use Jensen's inequality.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9Ail4gqbbu5"
   },
   "source": [
    "#### Exercise 1.3.2\n",
    "\n",
    "Prove that $\\mathcal{L}_{k+1}(\\mathbf{x}) \\geq \\mathcal{L}_{k}(\\mathbf{x})$ for any $k \\in \\mathbb{N}$. You can use the following lemma without proof:\n",
    "    \n",
    "*Lemma*: Let $I_k \\subset [k+1] := \\{1, \\ldots, k+1\\}$ with $|I_k| = k$ be a uniformly distributed subset of distinct indices from $[k+1]$. Then for any sequence of numbers $a_1, \\ldots, a_{k+1}$,\n",
    "$$\n",
    "    \\mathbb{E}_{I_k} \\left[ \\sum_{i \\in I_k} a_i \\over k \\right] \n",
    "    = { \\sum_{i=1}^{k+1} a_i \\over k+1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PP-VwVvbbfgf"
   },
   "source": [
    "#### Exercise 1.3.3\n",
    "\n",
    "The above two results show that\n",
    "$$\n",
    "\\log p(\\mathbf{x})\n",
    "\\geq \\mathcal{L}_{k+1}(\\mathbf{x})\n",
    "\\geq \\mathcal{L}_{k}(\\mathbf{x}).\n",
    "$$\n",
    "However, prove that the above inequalities do not guarantee $\\mathcal{L}_k(\\mathbf{x}) \\rightarrow \\log p(\\mathbf{x})$ when $k \\rightarrow \\infty$. (*Hint*: Provide a counterexample.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VvQBhB30OSZG"
   },
   "source": [
    "## II. Wake-Sleep\n",
    "\n",
    "In this section, we will derive the optimization objectives for the Wake-Sleep algorithm, which decomposes the optimization procedure into two phases:\n",
    "\n",
    "* **Wake-phase**: Given recognition weights $\\phi$, we activate the recognition process and update the generative weights $\\theta$ to increase the probability that they would reconstruct the correct activity vector in the layer below.\n",
    "* **Sleep-phase**: Given generative weights $\\theta$, we activate the generative process and update the recognition weights $\\phi$ to increase the probability that they would produce the correct activity vector in the layer above. Since it has generated the instance, it knows the true underlying causes, and therefore has available the target values for the hidden units that are required to train the bottom-up weights $\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bFYg91fFeLU4"
   },
   "source": [
    "### 2.1: Wake-phase\n",
    "\n",
    "The Wake-phase fixes the recognition weights $\\phi$ and optimizes a Monte Carlo estimate of ELBO w.r.t. the generative weights $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w6EMMFyBf9Mi"
   },
   "source": [
    "#### Exercise 2.1.1: Wake objective\n",
    "\n",
    "Given $N$ iid data points $\\mathbf{x} = \\{\\mathbf{x}^{(i)}\\}_{i=1}^N$, show that\n",
    "$$\n",
    "\\theta^* := \\underset{\\theta}{\\text{arg max}}  \\mathcal{L}(\\mathbf{x}) = \\underset{\\theta}{\\text{arg max}} \\sum_{i=1}^N \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x}^{(i)})} \\log p_\\theta(\\mathbf{x}^{(i)} \\mid \\mathbf{z})\n",
    "\\tag{3}\n",
    "$$\n",
    "which gives the Wake-phase objective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGcBy7WteQ_Y"
   },
   "source": [
    "#### Section 2.1.2: Psuedocode\n",
    "\n",
    "Given $N$ iid data points $\\{\\mathbf{x}^{(i)}\\}_{i=1}^N$, do the following for each $i \\in [N]$:\n",
    "1. Feed $\\mathbf{x}^{(i)}$ into the recognition network to get $\\mu_\\phi(\\mathbf{x}^{(i)})$ and $\\Sigma_\\phi(\\mathbf{x}^{(i)})$.\n",
    "2.  Draw $L$ samples {\\color{blue}$\\mathbf{z}^{(i)}_1, \\ldots, \\mathbf{z}^{(i)}_L  \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x}^{(i)}) = N(\\mathbf{z}; \\mu_\\phi(\\mathbf{x}^{(i)}), \\Sigma_\\phi (\\mathbf{x}^{(i)}))$ }.\n",
    "3. For each $l \\in [L]$, feed $\\mathbf{z}^{(i)}_l$ into the generative network to get $f_\\theta(\\mathbf{z}^{(i)}_l)$ for the likelihood $p_\\theta(\\mathbf{x} \\mid \\mathbf{z}^{(i)}_l) = \\text{Bernoulli}( \\mathbf{x} ; f_\\theta( \\mathbf{z}^{(i)}_l ))$.\n",
    "\n",
    "Finally, use SGD to maximize\n",
    "$$\n",
    "\\max_\\theta \\sum_{i=1}^N {1 \\over L} \\sum_{l=1}^L \\log p_\\theta(\\mathbf{x}^{(i)} \\mid \\mathbf{z}^{(i)}_l)\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "This gives a Monte Carlo estimate of the Wake-phase objective in Eq. (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmVsysYYeDNC"
   },
   "source": [
    "### 2.2: Sleep-phase\n",
    "\n",
    "\n",
    "The Sleep phase fixes the generative weights $\\theta$ and updates the recognition weights $\\phi$. It is generally intractable to directly minimize the KL-divergence term in $\\mathcal{L}(\\mathbf{x})$ w.r.t. $\\phi$:\n",
    "$$\n",
    "    \\underset{\\phi}{\\arg \\min} \\; KL \\left[ q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\middle\\| p_\\theta(\\mathbf{z} \\mid \\mathbf{x}) \\right] \n",
    "    = \\underset{\\phi}{\\arg \\min} \\int_{\\mathbf{z}} q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\log { q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\over p_\\theta(\\mathbf{z} \\mid \\mathbf{x}) } \\; d\\mathbf{z}\n",
    "$$\n",
    "    So instead, the Sleep phase minimizes the KL divergence the wrong way round,\n",
    "$$\n",
    "    \\arg \\min_\\phi KL \\left[ p_\\theta(\\mathbf{z} \\mid \\mathbf{x}) \\middle\\| q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\right].\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "97oWfZKUgAdw"
   },
   "source": [
    "#### Exercise 2.2.1: Sleep objective\n",
    "\n",
    "Suppose we sample $\\mathbf{z} \\sim p(\\mathbf{z}) = N(0, I)$, then sample $\\mathbf{x} \\sim p_\\theta(\\mathbf{x} \\mid \\mathbf{z})$. Show that\n",
    "$$\n",
    "    \\phi^* := \\underset{\\phi}{\\arg \\min} KL \\left[ p_\\theta(\\mathbf{z} \\mid \\mathbf{x}) \\middle\\| q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\right]\n",
    "    = \\underset{\\phi}{\\arg \\max} \\mathbb{E}_{\n",
    "    {p_\\theta(\\mathbf{x}, \\mathbf{z})}\n",
    "    } \\left[ \\log q_\\phi(\\mathbf{z} \\mid \\mathbf{x}) \\right]\n",
    "\\tag{5}\n",
    "$$\n",
    "which gives the Sleep-phase objective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTmTjL_7gCLz"
   },
   "source": [
    "#### Section 2.2.2: Pseudocode\n",
    "\n",
    "Let $L \\in \\mathbb{N}$ be a sample size hyperparameter. For each $l \\in [L]$, do the following:\n",
    "1. Draw $\\mathbf{z}^l \\sim N(0,I)$.\n",
    "2. Sample $\\mathbf{x}^l$ from the generative network $p_\\theta(\\mathbf{x} \\mid \\mathbf{z}^l) = \\text{Bernoulli}(f_\\theta(\\mathbf{z}^l))$.\n",
    "3. Feed $\\mathbf{x}^l$ into the recognition network to get $\\mu(\\mathbf{x}^l)$ and $\\Sigma(\\mathbf{x}^l)$.\n",
    "4. Compute $q_\\phi(\\mathbf{z}^l \\mid \\mathbf{x}^l) = N(\\mathbf{z}^l; \\mu(\\mathbf{x}^l), \\Sigma(\\mathbf{x}^l))$.\n",
    "\n",
    "Finally, do SGD to maximize\n",
    "$$\n",
    "\\max_\\phi {1 \\over L} \\sum_{l=1}^L \\log q_\\phi( \\mathbf{z}^l \\mid \\mathbf{x}^l )\n",
    "\\tag{6}\n",
    "$$\n",
    "\n",
    "This gives a Monte Carlo estimate of the Sleep-phase objective in Eq. (5)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "He0a6sQjdL3H"
   },
   "source": [
    "\n",
    "## III. Variational Autoencoders\n",
    "\n",
    "In this section, you will derive the optimization procedure for the VAE. Unlike Wake-Sleep, VAE\n",
    "avoids the two-stage optimization procedure and instead optimizes a stochastic estimate of ELBO directly w.r.t. to parameters $\\theta$ of the generative model (generation network) and parameters $\\phi$ of the variational distribution (recognition network)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtS7fEb3dtVO"
   },
   "source": [
    "### 3.1: Objective Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X7UP6bVjd7mu"
   },
   "source": [
    "#### Exercise 3.1.1: Rewriting ELBO\n",
    "\n",
    "For a given data point $\\mathbf{x}^{(i)}$, show that ELBO can be rewritten as\n",
    "$$\n",
    "    \\mathcal{L}(\\mathbf{x}^{(i)})\n",
    "    = -  KL \\left[ q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\middle\\| p_\\theta(\\mathbf{z} \\mid \\mathbf{x}) \\right]  + \\mathbb{E}_{\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x}^{(i)})} [\\log p_\\theta(\\mathbf{x}^{(i)} \\mid \\mathbf{z}) ].\n",
    "\\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9QMNHtMVZ3RI"
   },
   "source": [
    "#### Section 3.1.2: Stochastic estimator for ELBO\n",
    "\n",
    "Equation (7) gives a stochastic estimator for ELBO:\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}(\\mathbf{x}^{(i)})\n",
    "= -  KL \\left[ q_{\\phi}(\\mathbf{z} \\mid \\mathbf{x}) \\middle\\| p_\\theta(\\mathbf{z} \\mid \\mathbf{x}) \\right] + \\frac{1}{L} \\sum_{l=1}^L [\\log p_\\theta(\\mathbf{x}^{(i)} \\mid \\mathbf{z}^{(i, l)}) ]\n",
    "\\tag{8}\n",
    "$$\n",
    "where $\\{ \\mathbf{z}^{(i,l)} \\}_{l=1}^L$ are sampled from $q_\\phi(\\mathbf{z} \\mid \\mathbf{x}^{(i)})$. The VAE algorithm optimizes this stochastic estimate of ELBO using a Monte Carlo gradient estimate.\n",
    "\n",
    "In order to optimize the VAE objective in Eq. (8) efficiently, we use a **reparameterization trick** to rewrite  $\\mathbb{E}_{q_\\phi(\\mathbf{z} \\mid \\mathbf{x})}[\\cdot]$ such that the Monte Carlo estimate of the expectation is differentiable w.r.t. $\\phi$. More specifically, we reparameterize the latent variable\n",
    "$$\n",
    "\\mathbf{z} \\sim q_\\phi(\\mathbf{z} \\mid \\mathbf{x}^{(i)}) = N(\\mathbf{z} \\mid \\mu_\\phi(\\mathbf{x}^{(i)}),  \\Sigma_\\phi^2(\\mathbf{x}^{(i)}) )\n",
    "$$\n",
    "as a deterministic function of the input $\\mathbf{x}^{(i)}$ and an auxiliary noise variable $\\epsilon$:\n",
    "$$\n",
    "\\mathbf{z} = \\mu_\\phi(\\mathbf{x}^{(i)}) + \\Sigma_\\phi(\\mathbf{x}^{(i)}) \\odot \\epsilon\n",
    "\\qquad\\qquad \\epsilon \\sim N(0, I)\n",
    "$$\n",
    "where $\\odot$ signifies an element-wise product, and $\\Sigma_\\phi(\\mathbf{x}^{(i)})$ is a vector of the same size as $z$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1YEvRMJdZzp"
   },
   "source": [
    "#### Exercise 3.1.3: VAE objective\n",
    "\n",
    "Using this reparameterization, show that the VAE objective in Eq. (8) can be rewritten as\n",
    "$$\n",
    "\\tilde{\\mathcal{L}}(\\mathbf{x}^{(i)})\n",
    "= {1 \\over 2} \\sum_{j=1}^J \\left( 1 + \\log( \\Sigma_{(i),j}^2 ) - \\mu_{(i),j}^2 - \\Sigma_{(i),j}^2 \\right) + {1 \\over L} \\sum_{l=1}^L \\log p_\\theta(\\mathbf{x}^{(i)} \\mid \\mathbf{z}^{(i, l)})\n",
    "\\tag{9}\n",
    "$$\n",
    "where $\\mu_{(i)} := \\mu_\\phi(\\mathbf{x}^{(i)})$ and $\\Sigma_{(i)} := \\Sigma_\\phi(\\mathbf{x}^{(i)})$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bG1JtQdedPgG"
   },
   "source": [
    "### 3.2: Training Algorithm\n",
    "\n",
    "The VAE optimization procedure works as follows:\n",
    "1. For each $l \\in [L]$, draw $\\epsilon^{(l)} \\sim N(0, I)$, and compute $\\mathbf{z}^{(i,l)} := \\mu_{(i)} + \\Sigma_{(i)} \\odot \\epsilon^{(l)}$. \n",
    "2. Optimize the VAE objective in Eq. (9) w.r.t. $\\mu$, $\\Sigma$, and $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "el3T-lkwcr9i"
   },
   "source": [
    "#### Exercise 3.2.1: Derivatives of the VAE objective\n",
    "\n",
    "Derive the gradients of the VAE objective in Eq. (9) w.r.t. $\\mu_{(i),j}$, $\\Sigma_{(i),j}$, and $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aUxQPtDAa9UT"
   },
   "source": [
    "### 3.3: Concluding Remarks\n",
    "\n",
    "VAE is an elegant way to link directed graphical models to neural networks, and is theoretically appeasing because we optimise a (stochastic estimate of the) bound on the likelihood of the data. If the approximations made while performing variational bayes are valid, the training algorithm is guaranteed to increase the likelihood of the generative model. Moreover, there is a clear and recognized way to evaluate the quality of the model using the log-likelihood (either estimated by importance sampling or lower-bounded).\n",
    "\n",
    "For i.i.d. datasets with continuous latent variables per datapoint, posterior inference for VAE can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vcrC9qW5acL3"
   },
   "source": [
    "## IV. Short-answer questions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iA0yCO3Gcbnm"
   },
   "source": [
    "#### Exercise 4.1: Wake-Sleep\n",
    "\n",
    "Wake-Sleep requires a concurrent optimization of two objective functions, which together do not correspond to the optimization of (a bound of) the marginal likelihood. There is no guarantee that optimizing the Wake-Sleep objectives leads to a decrease in the free energy because:\n",
    "\n",
    "(Choose 0-2 of the following choices)\n",
    "\n",
    "1. The sleep phase trains the recognition model to invert the generative model for input vectors that are distributed according to the generative model rather than according to the real data.\n",
    "2. The sleep phase learning does not follow the correct gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pUnDYKXxchhc"
   },
   "source": [
    "#### Exercise 4.2: Discrete Latent Variables\n",
    "\n",
    " Between Wake-Sleep and VAE, which algorithm(s) can be applied to models with discrete latent variables?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cyXxPuW7cjy7"
   },
   "source": [
    "#### Exercise 4.3: Computational Complexity\n",
    " \n",
    "(True or False) Wake-Sleep and VAE have the same computational complexity per datapoint."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A54UTXxyWTM3"
   },
   "source": [
    "# Part 2: Implementation\n",
    "\n",
    "This section contains a working implementation of the Variational Autoencoder (VAE) [5]. All provided code will run as is, but you will need to complete the following TODO's:\n",
    "\n",
    "1. Implement the Wake-Sleep algorithm by modifying the provided AEVB code.\n",
    "2. Implement the alternate lower bound metric from Section 1.3:\n",
    "$$\n",
    "\\mathcal{L}_k(\\mathbf{x}) = \\mathbb{E}_{\\mathbf{z}^{(1)}, \\dots, \\mathbf{z}^{(k)} \\sim q_{\\mathbf{\\phi}}(\\mathbf{z} \\mid \\mathbf{x})} \\left[ \\log \\frac{1}{k} \\sum_{i=1}^k \\frac{p_{\\mathbf{\\theta}}(\\mathbf{x}, \\mathbf{z}^{(i)})}{q_{\\mathbf{\\phi}}(\\mathbf{z}^{(i)} \\mid \\mathbf{x})} \\right]\n",
    "$$\n",
    "3. Run experiments on the MNIST handwritten digits dataset.\n",
    "4. Submit your modified .ipynb notebook (containing your Wake-Sleep and $\\mathcal{L}_k$ implementations) on Gradescope. (In Colab: Select \"File\" > \"Download .ipynb\" to download the notebook, then upload the notebook to Gradescope.)\n",
    "\n",
    "Please be careful not to publish your solutions on the web. By default, your Colab notebooks are set to private."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7NjoHzGZVgsa"
   },
   "source": [
    "## I. Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ho-bHc65mHH6"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "au0-u479vZon"
   },
   "source": [
    "The following code downloads the MNIST dataset. (You can ignore the warnings for this assignment.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iecY8_IlmJLB"
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.learn.python.learn.datasets.mnist import read_data_sets\n",
    "mnist = read_data_sets('mnist', one_hot=True)\n",
    "\n",
    "# Print dataset shapes.\n",
    "# - Images are 28 x 28.\n",
    "# - Labels are one of 10 digits, {0, 1, ..., 9}.\n",
    "print()\n",
    "print('Train images shape:', mnist.train.images.shape)\n",
    "print('Train labels shape:', mnist.train.labels.shape)\n",
    "\n",
    "print('Test images shape:', mnist.test.images.shape)\n",
    "print('Test labels shape:', mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1vSLz8PZvenY"
   },
   "source": [
    "Now you can play around with the MNIST dataset. Below, we provide a function to visualize the 28 x 28 image matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q8DxAgcFvdBl"
   },
   "outputs": [],
   "source": [
    "def plot_mnist(X, nrows=10, ncols=10):\n",
    "    \"\"\"\n",
    "    Plots the given MNIST digits on a grid of size (nrows x ncols).\n",
    "    \"\"\"\n",
    "    N = len(X)\n",
    "    assert N <= nrows * ncols\n",
    "\n",
    "    # Plot each 28 x 28 image.\n",
    "    plt.figure(figsize=(nrows, ncols))\n",
    "    for i in range(N):\n",
    "        x = X[i].reshape(28, 28)\n",
    "\n",
    "        plt.subplot(nrows, ncols, i + 1)\n",
    "        fig = plt.imshow(x, vmin=0, vmax=1, cmap=\"gray\")\n",
    "\n",
    "        # Hide axes. (Both x-axis and y-axis range from 0 to 27.)\n",
    "        fig.axes.get_xaxis().set_visible(False)\n",
    "        fig.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # If you want to save the plot using plt.savefig(fig_path), please\n",
    "    # follow the instructions above (under \"How to save files\").\n",
    "\n",
    "# Plot digits.\n",
    "plot_mnist(mnist.train.images[:9], nrows=3, ncols=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nwjmP2C3vjpi"
   },
   "source": [
    "## II. Implementation\n",
    "\n",
    "We now define a class \"VariationalAutoencoder\" that implements the AEVB learning algorithm. The VAE is trained incrementally with mini-batches using partial fit. The trained model can be used to reconstruct unseen input, generate new samples, and map inputs to the latent space.\n",
    "\n",
    "Model specification:\n",
    "\n",
    "* Both the generative network $p_{\\mathbf{\\theta}}(\\mathbf{x} \\mid \\mathbf{z})$ and the recognition network $q_{\\mathbf{\\phi}}(\\mathbf{z} \\mid \\mathbf{x})$ are parametrized by neural networks with one hidden layer that consists of 512 ReLU neurons and a latent space dimension of 2. The weights are initialized by sampling from $\\mathcal{N}(0, 0.01)$, and biases are initialized to zeros.\n",
    "\n",
    "* Since $\\mathbf{x}$'s take values in [0, 1], the output layer of the generation network that represents $p_{\\mathbf{\\theta}}(\\mathbf{x} \\mid \\mathbf{z})$ should consist of sigmoid neurons.\n",
    "\n",
    "* The variational distribution $q_{\\mathbf{\\phi}}(\\mathbf{z} \\mid \\mathbf{x})$ is represented by a Gaussian of the form $\\mathcal{N}(\\mathbf{z}; \\mathbf{\\mu}_{\\mathbf{\\phi}}(\\mathbf{x}), \\mathbf{\\Sigma}^2_{\\mathbf{\\phi}}(\\mathbf{x})I)$, where $\\mathbf{\\mu}_{\\mathbf{\\phi}}(\\mathbf{x})$ and $\\mathbf{\\Sigma}_{\\mathbf{\\phi}}(\\mathbf{x})$ are linear outputs of the recognition network.\n",
    "\n",
    "### TODO's\n",
    "The code will run as is, but you will later need to implement the following TODO's specified throughout the code:\n",
    "\n",
    "1. Wake-Sleep algorithm.  You just need to fill in the TODO's inside the VariationalAutoencoder class, under the \"if self.algo == 'wake-sleep':\" statements in the class methods _create_network(), _create_loss_optimizer(), and _partial\\_fit(). The remaining parts (e.g., architectures for the recognition and generation networks) should remain the same.\n",
    "  - You may need different learning rates for the Wake-phase and Sleep-phase. In the reference implementation, we used learning rates 1e-3 for the Wake-phase and 1e-5 for the Sleep-phase. We used a sample size of 100 to compute the Monte Carlo estimate of the gradient for the Sleep objective.\n",
    "  - The optional argument \"var_list\" in tf.train.AdamOptimizer() lets you specify a list of tf.Variable to update to minimize the loss.\n",
    "\n",
    "2. Computation of the lower bound $\\mathcal{L}_k$ as defined in Section 1.4:\n",
    "$$\n",
    "\\mathcal{L}_k(\\mathbf{x}) = \\mathbb{E}_{\\mathbf{z}^{(1)}, \\dots, \\mathbf{z}^{(k)} \\sim q_{\\mathbf{\\phi}}(\\mathbf{z} \\mid \\mathbf{x})} \\left[ \\log \\frac{1}{k} \\sum_{i=1}^k \\frac{p_{\\mathbf{\\theta}}(\\mathbf{x}, \\mathbf{z}^{(i)})}{q_{\\mathbf{\\phi}}(\\mathbf{z}^{(i)} \\mid \\mathbf{x})} \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WiP-9Wgva8Z"
   },
   "outputs": [],
   "source": [
    "class VariationalAutoencoder():\n",
    "    def __init__(self, algo='vae', learning_rate=0.001, k=500, ckpt_path=\"\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          algo (str): The algorithm. Value must be 'vae' or 'wake-sleep'.\n",
    "          learning_rate (float): The learning rate.\n",
    "          k (int): Used to compute L_k as defined in Section 1.4.\n",
    "          ckpt_path (str): Model checkpoint path\n",
    "        \"\"\"\n",
    "        assert algo in ['vae', 'wake-sleep']\n",
    "        self.algo = algo\n",
    "        self.learning_rate = learning_rate\n",
    "        self.k = k\n",
    "\n",
    "        self.z_dim = 2         # Latent space dimension\n",
    "        self.input_size = 784  # MNIST image size: 28*28\n",
    "\n",
    "        self._create_network()\n",
    "        self._create_loss_optimizer()\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.sess = tf.InteractiveSession()\n",
    "\n",
    "        if ckpt_path:\n",
    "            self.load_model(ckpt_path)\n",
    "        else:\n",
    "            self.sess.run(init)\n",
    "\n",
    "    def _init_weights(self, in_dim, out_dim):\n",
    "        \"\"\"Return random values of shape (in_dim, out_dim) from a gaussian\n",
    "        N(0, 0.01) distribution.\"\"\"\n",
    "        return tf.random_normal((in_dim, out_dim), mean=0.0, stddev=0.1,\n",
    "                                dtype=tf.float32)\n",
    "\n",
    "    def _initialize_weights(self, n_hidden_r=512, n_hidden_g=512):\n",
    "        network_weights = dict()\n",
    "        network_weights['weights_r'] = {\n",
    "            'h1': tf.Variable(self._init_weights(self.input_size, n_hidden_r)),\n",
    "            'out_mean': tf.Variable(self._init_weights(n_hidden_r, self.z_dim)),\n",
    "            'out_log_sigma': tf.Variable(\n",
    "                self._init_weights(n_hidden_r, self.z_dim))\n",
    "        }\n",
    "        network_weights['biases_r'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_r], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(tf.zeros([self.z_dim], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(\n",
    "                tf.zeros([self.z_dim], dtype=tf.float32))\n",
    "        }\n",
    "        network_weights['weights_g'] = {\n",
    "            'h1': tf.Variable(self._init_weights(self.z_dim, n_hidden_g)),\n",
    "            'out_mean': tf.Variable(\n",
    "                self._init_weights(n_hidden_g, self.input_size)),\n",
    "            'out_log_sigma': tf.Variable(\n",
    "                self._init_weights(n_hidden_g, self.input_size))\n",
    "        }\n",
    "        network_weights['biases_g'] = {\n",
    "            'b1': tf.Variable(tf.zeros([n_hidden_g], dtype=tf.float32)),\n",
    "            'out_mean': tf.Variable(\n",
    "                tf.zeros([self.input_size], dtype=tf.float32)),\n",
    "            'out_log_sigma': tf.Variable(\n",
    "                tf.zeros([self.input_size], dtype=tf.float32))\n",
    "        }\n",
    "        return network_weights\n",
    "\n",
    "    def _create_network(self):\n",
    "        # Input MNIST images\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.input_size])\n",
    "\n",
    "        # Initialize autoencoder network weights and biases.\n",
    "        self.network_weights = self._initialize_weights()\n",
    "\n",
    "        # Use recognition network to determine mean and (log) variance of the\n",
    "        # Gaussian distribution in latent space.\n",
    "        #   z_mean:         (batch_size, 2)\n",
    "        #   z_log_sigma_sq: (batch_size, 2)\n",
    "        self.z_mean, self.z_log_sigma_sq = self._recognition_network(\n",
    "            self.x,\n",
    "            self.network_weights[\"weights_r\"],\n",
    "            self.network_weights[\"biases_r\"])\n",
    "\n",
    "        # Draw one sample z from Gaussian distribution.\n",
    "        batch_size = tf.shape(self.x)[0]\n",
    "        eps = tf.random_normal((batch_size, self.z_dim), 0, 1, \n",
    "                               dtype=tf.float32)\n",
    "        # z = mu + sigma * epsilon\n",
    "        self.z = tf.add(self.z_mean, \n",
    "                        tf.multiply(tf.sqrt(tf.exp(self.z_log_sigma_sq)), eps))\n",
    "\n",
    "        # Use generator to determine the mean of the Bernoulli distribution of\n",
    "        # the reconstructed input.\n",
    "        self.x_reconstr_mean = self._generator_network(\n",
    "            self.z,  # [B * L, 2]\n",
    "            self.network_weights[\"weights_g\"],\n",
    "            self.network_weights[\"biases_g\"])\n",
    "\n",
    "        if self.algo == 'wake-sleep':\n",
    "            # TODO: You may need to add variables for the sleep-phase.\n",
    "            pass\n",
    "\n",
    "        # TODO: You may need to add variables for computing the lower bound L_k.\n",
    "\n",
    "    def _recognition_network(self, x, weights, biases):\n",
    "        # Generate probabilistic encoder (recognition network), which maps\n",
    "        # inputs onto a normal distribution in latent space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']),\n",
    "                                  biases['b1']))\n",
    "        z_mean = tf.add(tf.matmul(layer, weights['out_mean']),\n",
    "                        biases['out_mean'])\n",
    "        z_log_sigma_sq = tf.add(\n",
    "                   tf.matmul(layer, weights['out_log_sigma']), \n",
    "                   biases['out_log_sigma'])\n",
    "        return (z_mean, z_log_sigma_sq)\n",
    "\n",
    "    def _generator_network(self, z, weights, biases):\n",
    "        # Generate probabilistic decoder (decoder network), which maps points in\n",
    "        # latent space onto a Bernoulli distribution in data space.\n",
    "        # The transformation is parametrized and can be learned.\n",
    "        layer = tf.nn.relu(tf.add(tf.matmul(z, weights['h1']), \n",
    "                                           biases['b1'])) \n",
    "        x_reconstr_mean = tf.nn.sigmoid(\n",
    "                    tf.add(tf.matmul(layer, weights['out_mean']), \n",
    "                    biases['out_mean']))\n",
    "        return x_reconstr_mean\n",
    "\n",
    "    def _create_loss_optimizer(self):\n",
    "        # Reconstruction term\n",
    "        reconstr_term = (self.x * tf.log(1e-10 + self.x_reconstr_mean)\n",
    "                + (1-self.x) * tf.log(1e-10 + 1 - self.x_reconstr_mean))\n",
    "        reconstr_loss = -tf.reduce_sum(reconstr_term, 1)\n",
    "        \n",
    "        if self.algo == 'vae':\n",
    "            latent_term = (1 + self.z_log_sigma_sq - tf.square(self.z_mean)\n",
    "                             - tf.exp(self.z_log_sigma_sq))\n",
    "            latent_loss = -0.5 * tf.reduce_sum(latent_term, 1)\n",
    "\n",
    "            # Average loss over batch.\n",
    "            self.cost = tf.reduce_mean(reconstr_loss + latent_loss)   \n",
    "\n",
    "            # Optimize loss.\n",
    "            self.optimizer = tf.train.AdamOptimizer(\n",
    "                    learning_rate=self.learning_rate).minimize(self.cost)\n",
    "        elif self.algo == 'wake-sleep':\n",
    "            # TODO: Implement Wake-Sleep optimizers.\n",
    "            # Hints:\n",
    "            #   - You can use different learning rates for wake-phase and\n",
    "            #     sleep-phase.\n",
    "            #   - The optional argument `var_list` lets you specify a list of\n",
    "            #     tf.Variable to update to minimize loss. Example:\n",
    "            #     tf.train.AdamOptimizer(learning_rate=lr).minimize(cost, var_list=[...])\n",
    "            pass\n",
    " \n",
    "    def partial_fit(self, X):\n",
    "        \"\"\"Train model based on mini-batch of input data.\n",
    "\n",
    "        Return cost of mini-batch.\n",
    "        \"\"\"\n",
    "        if self.algo == 'vae':\n",
    "            opt, cost = self.sess.run((self.optimizer, self.cost), \n",
    "                                      feed_dict={self.x: X})\n",
    "            return [cost]\n",
    "        elif self.algo == 'wake-sleep':\n",
    "            # TODO: Implement wake & sleep costs for Wake-Sleep.\n",
    "            pass\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform data by mapping it into the latent space.\"\"\"\n",
    "        # Note: This maps to mean of distribution. We could alternatively\n",
    "        # sample from Gaussian distribution.\n",
    "        return self.sess.run(self.z_mean, feed_dict={self.x: X})\n",
    "\n",
    "    def generate(self, z_mu=None):\n",
    "        \"\"\" Generate data by sampling from latent space.\n",
    "        \n",
    "        If z_mu is not None, data for this point in latent space is\n",
    "        generated. Otherwise, z_mu is drawn from prior in latent \n",
    "        space.        \n",
    "        \"\"\"\n",
    "        if z_mu is None:\n",
    "            z_mu = np.random.normal(size=self.z_dim)\n",
    "        # Note: This maps to mean of distribution. We could alternatively\n",
    "        # sample from Gaussian distribution.\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.z: z_mu})\n",
    "\n",
    "    def reconstruct(self, X):\n",
    "        \"\"\" Use VAE to reconstruct given data. \"\"\"\n",
    "        return self.sess.run(self.x_reconstr_mean, \n",
    "                             feed_dict={self.x: X})\n",
    "    \n",
    "    # TODO: You may need to add a method that returns values needed to compute\n",
    "    #       L_k loss for the given data.\n",
    "\n",
    "    def save_model(self, ckpt_path):\n",
    "        self.saver.save(self.sess, ckpt_path)\n",
    "        print(\"Saved model to \" + ckpt_path)\n",
    "\n",
    "    def load_model(self, ckpt_path):\n",
    "        self.saver.restore(self.sess, ckpt_path)\n",
    "        print(\"Loaded model from \" + ckpt_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YtrDvmIswHTU"
   },
   "source": [
    "## III. Training\n",
    "\n",
    "The following code trains VAE on the MNIST dataset for 100 epochs using batch size 100.\n",
    "\n",
    "### TODO\n",
    "\n",
    "Train both VAE and Wake-Sleep for 100 epochs using batch size 100.\n",
    "\n",
    "* For each algorithm, plot $\\mathcal{L}_{100}^\\text{test}$ vs. the epoch number.\n",
    "  - To save computation time, you can evaluate $\\mathcal{L}_{100}$ every 10 epochs.\n",
    "  - Use the full test set $\\{\\mathbf{x}^{(i)}\\}_{i=1}^n$ to evaluate $\\mathcal{L}_{100} = \\frac{1}{n} \\sum_{i=1}^n \\mathcal{L}_{100}(\\mathbf{x}^{(i)})$.\n",
    "  - In the reference implementation, each evaluation of $\\mathcal{L}_{100}^\\text{test}$ on the test set takes about 160 seconds.\n",
    " \n",
    "* For Wake-Sleep, also plot the training losses for the wake-phase and sleep-phase vs. the epoch number.\n",
    "\n",
    "If you encounter NaN's, try changing your learning rate(s); use the log-sum-exp trick; or sometimes rerunning the code fixes the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3pnNGvtuv7_0"
   },
   "outputs": [],
   "source": [
    "def train(algo='vae',\n",
    "          learning_rate=0.001,\n",
    "          batch_size=100, training_epochs=100, eval_freq=10):\n",
    "    model = VariationalAutoencoder(algo=algo, learning_rate=learning_rate)\n",
    "    n_samples = mnist.train.num_examples\n",
    "    total_batch = int(n_samples / batch_size)\n",
    "\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        for i in range(total_batch):\n",
    "            # Sample training batch.\n",
    "            batch_xs, _ = mnist.train.next_batch(batch_size)\n",
    "\n",
    "            # Fit training using batch data.\n",
    "            costs = model.partial_fit(batch_xs)\n",
    "    \n",
    "            # Compute average loss.\n",
    "            avg_cost += costs[0] / n_samples * batch_size\n",
    "\n",
    "        if epoch % eval_freq == 0:\n",
    "            print(\"Epoch:\", '%04d' % (epoch+1),\n",
    "                  \"Train cost=\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "            # TODO: Compute and plot L_k.\n",
    "\n",
    "    # If you want to save the model checkpoint, please follow the instructions\n",
    "    # above (under \"How to save files\"), then uncomment the following lines:\n",
    "    #   ckpt_path = 'out/{}-final.ckpt'.format(algo)\n",
    "    #   model.save_model(ckpt_path)\n",
    "    # Then you can later load a saved model by doing:\n",
    "    #   vae = VariationalAutoencoder(algo='vae', ckpt_path='out/vae-final.ckpt')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Train VAE.\n",
    "vae = train(algo='vae', training_epochs=100)\n",
    "\n",
    "# TODO: Train Wake-Sleep. Also plot the training losses for the wake-phase and\n",
    "#       sleep-phase vs. the epoch number.\n",
    "#ws = train(algo='wake-sleep', training_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BUmaJIocwLSX"
   },
   "source": [
    "## IV. Reconstructed Images\n",
    "\n",
    "Next, we provide code that samples 100 MNIST images from the test set, uses the recognition network to map them to latent space, then applies the generator network to reconstruct the images.\n",
    "\n",
    "**TODO**: Run this code to visualize these reconstruction samples on a $10 \\times 10$ tile grid. Also visualize the original MNIST images on a $10 \\times 10$ tile grid. Briefly compare the results for Wake-Sleep vs. AEVB. (You just need to uncomment the last lines to run it for Wake-Sleep.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_loxwz07wJud"
   },
   "outputs": [],
   "source": [
    "# Randomly sample 100 MNIST images from the test set.\n",
    "x_sample = mnist.test.next_batch(100)[0]\n",
    "plot_mnist(x_sample)\n",
    "\n",
    "# Use the trained VAE model to reconstruct the images.\n",
    "reconstruction = vae.reconstruct(x_sample)\n",
    "plot_mnist(reconstruction, nrows=10, ncols=10)\n",
    "\n",
    "# Use the trained Wake-Sleep model to reconstruct the images.\n",
    "#reconstruction = ws.reconstruct(x_sample)\n",
    "#plot_mnist(reconstruction, nrows=10, ncols=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qA-AXIRvwtvx"
   },
   "source": [
    "## V. Latent Space Visualization (Part 1)\n",
    "\n",
    "Since we have specifically chosen the latent space to be 2-dimensional, now we can easily visualize the learned latent manifold of digits. We provide code that samples 5000 MNIST images from the test set, and visualize their latent representations as a scatter plot, where colors of the points correspond to the digit labels.\n",
    "\n",
    "**TODO**: Run this code to visualize the latent space scatterplot. Briefly compare the results for Wake-Sleep vs. AEVB. (You just need to uncomment the last line to run it for Wake-Sleep.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6b1E_niHwNxv"
   },
   "outputs": [],
   "source": [
    "def plot_latent_space_scatterplot(model):\n",
    "    # Scatter plot of the lagent representations of X, colored by labels.\n",
    "    x_sample, y_sample = mnist.test.next_batch(5000)\n",
    "    z_mu = model.transform(x_sample)\n",
    "    plt.figure(figsize=(8, 6)) \n",
    "    plt.scatter(z_mu[:, 0], z_mu[:, 1], c=np.argmax(y_sample, 1))\n",
    "    plt.colorbar()\n",
    "    plt.grid()\n",
    "\n",
    "plot_latent_space_scatterplot(vae)\n",
    "\n",
    "# TODO: Do the same thing for Wake-Sleep.\n",
    "#plot_latent_space_scatterplot(ws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "07LVRxuqwwyK"
   },
   "source": [
    "## VI. Latent Space Visualization (Part 2)\n",
    "\n",
    "Finally, we provide code that uses the generator network to plot reconstructions at the positions in the latent space for which they have been generated.\n",
    "\n",
    "The code below does the following:\n",
    "\n",
    "1. Let $\\{\\mathbf{z}^{(i)} = (\\mathbf{z}_1^{(i)}, \\mathbf{z}_2^{(i)})\\}_{i=1}^{5000}$ be the latent representations of 5000 images from the test set.\n",
    "\n",
    "2. Let $\\mathcal{Z}_1$ consist of 15 evenly spaced points between the interval $\\left[ \\min_i \\textbf{z}_1^{(i)}, \\max_i \\textbf{z}_1^{(i)} \\right]$, and similarly, let $\\mathcal{Z}_2$ consist of 15 evenly spaced points between $\\left[ \\min_i \\textbf{z}_2^{(i)}, \\max_i \\textbf{z}_2^{(i)} \\right]$.\n",
    "\n",
    "3. For each $\\mathbf{z} = (\\mathbf{z}_1, \\mathbf{z}_2) \\in \\mathcal{Z}_1 \\times \\mathcal{Z}_2$, generate and visualize digits using the trained model, and tile the digits into a $15 \\times 15$ grid.\n",
    "\n",
    "**TODO**: Run this code to visualize the latent space reconstructions. Briefly compare the results for Wake-Sleep vs. AEVB.  (You just need to uncomment the last line to run it for Wake-Sleep.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eUUofNCLwvSV"
   },
   "outputs": [],
   "source": [
    "def plot_latent_space(X, model):\n",
    "    # Plot images generated from latent space.\n",
    "    z_mu = model.transform(X)\n",
    "    z0_min, z1_min = np.min(z_mu, axis=0)\n",
    "    z0_max, z1_max = np.max(z_mu, axis=0)\n",
    "    Z = [[z0, z1] for z0 in np.linspace(z0_min, z0_max, 15)\n",
    "                  for z1 in np.linspace(z1_min, z1_max, 15)]\n",
    "    x_reconstr = model.generate(Z)\n",
    "    plot_mnist(x_reconstr, nrows=15, ncols=15)\n",
    "\n",
    "# Sample 100 MNIST images from the test set.\n",
    "x_test, y_test = mnist.test.next_batch(5000)\n",
    "\n",
    "plot_latent_space(x_test, vae)\n",
    "\n",
    "# TODO: Do the same thing for Wake-Sleep.\n",
    "#plot_latent_space(x_test, ws)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Tutorial on Wake-Sleep and VAE",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
