{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "## Stephen Elston"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Policy Gradients\n",
    "\n",
    "In the previous lesson we created parameterized state value and action value functions:\n",
    "\n",
    "$$v(s_t) \\approx  V(s_t,\\mathbf{w}_t) \\\\\n",
    "Q(s_t,a_t) \\approx Q(s_t,a_t,\\mathbf{w}_t)$$\n",
    "\n",
    "Where, $\\mathbf{w}$ is the parameter vector. Using these parameterized functions optimal policies are computed. \n",
    "\n",
    "Now, we will consider **parameterized policy functions** which can be written in the form:   \n",
    "\n",
    "$$\\pi(a\\ |\\ s, \\mathbf{\\theta}) = Pr\\{A_t = a\\ |\\ S_t = s, \\mathbf{\\theta_t} = \\mathbf{\\theta} \\}$$   \n",
    "\n",
    "Where, $\\mathbf{\\theta} \\in R^d$ is the d-dimensional **parameter vector**. The parameterized policy is often referred to as the **actor**.   \n",
    "\n",
    "A **parameterized value function**, $\\hat{v}(s, \\mathbf{w})$, can be used to evaluate a policy. The value function is determined by state, s, and the d-dimensional parameter vector $\\mathbf{w} \\in R^d$. The parameterized value function is often referred to as the **critic**.  \n",
    "\n",
    "\n",
    "**Actor-critic** algorithms use gradient methods to learn both the policy parameters and the value function or critic.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameterized Policy Approximation\n",
    "\n",
    "How can we parameterize a policy?   \n",
    "\n",
    "First, the parameterized policy must be **differentiable** with respect to to the parameter vector, $\\mathbf{\\theta}$, to be amenable to gradient ascent methods. To be learnable the policy gradient with respect to $\\mathbf{\\theta} \\in R^d$, $\\nabla \\pi(a\\ |\\ s, \\mathbf{\\theta})$, must exist and be bounded for $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}(s)$, where $\\mathcal{S}$ is the set of all states, and $\\mathcal{A}(s)$ is the set of all actions given s.  \n",
    "\n",
    "Second, the policy must never become deterministic. In other words, the probability of taking an action must not be simply binary, $\\{ 0,1 \\}$, or $\\pi(a\\ |\\ s, \\mathbf{\\theta}) \\in \\{ 0,1 \\}$. Instead, a viable policy must allow each possible action with some probability for all states,  $0 \\gt \\pi(a\\ |\\ s, \\mathbf{\\theta}) \\gt 1.0,\\ \\forall\\ a,\\ \\forall\\ s$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of parameterized policy \n",
    "\n",
    "You may be wondering what the advantages and disadvantages of parameterized policy might be? The advantages can be summarized as:\n",
    "\n",
    "- **Improved convergence properties**. In some cases, learning a parameterized policy can be more sample efficient than other RL learning methods.   \n",
    "- **Scalable to high dimensional and continuous action spaces**. We have investigated methods to learn policy for continuous state spaces. But, the examples we have examined to now have discrete action spaces. However, many real-world problems have continuous action spaces. Parameterized policy methods work well with continuous action spaces. \n",
    "- **Can learn a stochastic policies**. All of the algorithms we have examined until now create deterministic policies. Whereas, parameterized policy can be stochastic.   \n",
    "\n",
    "The disadvantages of a parameterized policy include:\n",
    "\n",
    "- These algorithms will **often converge to a locally optimal solutions**, rather than a globally optimal solutions.  \n",
    "- Policy evaluation is relatively inefficient and has high variance. We will examine methods to reduce the variance shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy optimization   \n",
    "\n",
    "Learning with parameterized policy involves maximizing the value of the policy. A number of optimization methods have been used for this problem. Here, we will use **gradient ascent** to maximize the loss function.      \n",
    "\n",
    "The goal of policy gradient methods is to learn a parameter vector, $\\mathbf{\\theta}$, which **maximizes a loss function**, $J(\\mathbf{\\theta})$. The commonly used learning method is to apply **gradient ascent** method of the form:  \n",
    "\n",
    "$$\\mathbf{\\theta}_{t+_1} = \\mathbf{\\theta}_t + \\alpha \\widehat{\\nabla J(\\mathbf{\\theta})}$$  \n",
    "\n",
    "Where,\n",
    "\n",
    "$\\alpha = $ the learning rate.  \n",
    "$\\widehat{\\nabla J(\\mathbf{\\theta})} \\in R^d = $ the estimate of the d-dimension **gradient** vector of the loss function:\n",
    "\n",
    "$$\\widehat{\\nabla_{\\theta} J(\\mathbf{\\theta})} = \n",
    "\\begin{bmatrix} \n",
    "\\frac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_1} \\\\\n",
    "\\frac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_2} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J(\\mathbf{\\theta})}{\\partial \\theta_d}\n",
    "\\end{bmatrix}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Gradient Theorem  \n",
    "\n",
    "We can find the **policy gradient** analytically, if $\\pi_{theta}$ is **differentiable and non-zero everywhere**. The gradient is then $\\nabla_{\\theta} \\pi_{\\theta}$. But, how can this gradient be found in practice? The answer is to apply the **policy gradient theorem**.    \n",
    "\n",
    "For an episodic MDP we can define the performance by the loss function:\n",
    "\n",
    "$$J(\\mathbf{\\theta}) = v_{\\pi_{\\mathbf{\\theta}}}(s_0)$$  \n",
    "\n",
    "Where $s_0$ is the starting state of the episode. In this case, there is no discounting, with $\\gamma = 1$.\n",
    "\n",
    "Given the loss function defined above the policy gradient theorem says that the gradient is:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) \\propto \\sum_s \\mu(s) \\sum_a q_\\pi(s,q) \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})\\\\ = \\mathbb{E}_{\\pi_\\theta} \\big[ \\sum_a q_\\pi(s,q) \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) \\big]$$\n",
    "\n",
    "For a **one=step Markov Decision Process** (MDP) we can  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood ratio and score function\n",
    "\n",
    "There is an efficient way to find the gradient of the policy $\\pi(a|S_t,\\mathbf{\\theta})$. An identity of **likelihood ratios** can be used:\n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\pi(a|S_t,\\mathbf{\\theta}) \\frac{ \\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta})}{\\pi(a|S_t,\\mathbf{\\theta})}\\\\\n",
    "= \\pi(a|S_t,\\mathbf{\\theta}) \\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\theta})  $$\n",
    "\n",
    "Where $\\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\theta})$ is the **score function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Policies\n",
    "\n",
    "Algorithms we have examined previously, value iteration and policy iteration, result in **deterministic policies**. A deterministic policy takes an optimal action given the state. \n",
    "\n",
    "But, what happens if there is uncertainty as to the best action? In this case, a **stochastic policy** is required. As you likely intuit, the action taken by a stochastic policy is probabilistic.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete stochastic actions    \n",
    "\n",
    "The deterministic policies we have examined previously all take discrete actions. A policy with deterministic discrete actions can be represented, $\\pi(a|s) \\in {0,1}$. In other words, a binary response, an action is either taken or not. \n",
    "\n",
    "Alternatively, the actions taken by a stochastic policy are determined probabilistically. If there are a limited number of possible actions, the probability of taking an action can be computed as **softmax action preferences**:\n",
    "\n",
    "$$\\pi(a|s, \\mathbf{\\theta}) = \\frac{e^{h(s,a,\\mathbf{\\theta})}}{\\sum_b e^{h(s,a,\\mathbf{\\theta})}}$$   \n",
    "\n",
    "The action preferences with the largest probabilities are the most likely to be taken.    \n",
    "\n",
    "For the case of policy parameterization using linear function approximation, $\\phi(s,a)\\ \\mathbf{\\theta}$:    \n",
    "\n",
    "$$\\pi(a|s, \\mathbf{\\theta}) \\propto  e^{\\phi(s,a)^T\\ \\mathbf{\\theta}}$$\n",
    "\n",
    "The score function then becomes:  \n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\phi(s,a) - \\mathbb{E}_{\\pi_\\theta} \\big[ \\phi(s,\\cdot) \\big]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuous actions and Gaussian distributions   \n",
    "\n",
    "Many real world problems have continuous action spaces. Parameterized policies are ideal for continuous action spaces. A stochastic policy for a continuous action space can be parameterized using a Gaussian distribution:    \n",
    "\n",
    "$$a \\sim \\mathcal{N} \\big( \\mu(s),\\sigma^2 \\big)$$  \n",
    "\n",
    "where, the mean action is parameterized, $u(s) = \\pi(s)^T\\ \\mathbf{\\theta}$. It is also possible to parameterize $\\sigma^2$.\n",
    "\n",
    "The score function is then:   \n",
    "\n",
    "$$\\nabla_\\theta \\pi(a|S_t,\\mathbf{\\theta}) = \\frac{\\big(a - \\mu(s) \\big) \\pi(s)}{\\sigma^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo Policy Gradient - Reinforce  \n",
    "\n",
    "By direct application of the policy gradient theorem the **reinforce algorithm** can be developed. For each episode, the steps of the reinforce algorithm are: \n",
    "\n",
    "1. Using Monte Carlo policy evaluation, the state value, $v(s)$, is computed.  \n",
    "2. Update the policy parameters using the policy gradient theorem:\n",
    "\n",
    "$$\\mathbf{\\theta}_{t+1} = \\mathbf{\\theta}_t + \\alpha\\ \\nabla_\\theta log\\pi(a|S_t,\\mathbf{\\mathbf{\\theta}})\\  v_t(s)$$\n",
    "\n",
    "Where $\\alpha$ is the learning rate.  \n",
    "\n",
    "While the reinforce algorithm converges, the variance of the Monte Carlo policy gradient can be large.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reducing Variance with a Critic\n",
    "\n",
    "How can the variance of the policy gradient be reduced? Can use a **critic** to estimate the action-value function:\n",
    "\n",
    "$$Q_{\\pi_{\\mathbf{\\theta}}}(s,a) \\approx Q_{w}(s,a)$$\n",
    "\n",
    "The steps of the actor-critic algorithm alternates between these steps:\n",
    "- Critic updates the action-value function parameters, $w$.\n",
    "- Actor updated the policy parameters, $\\theta$, using the critic update. \n",
    "\n",
    "### Actor-Critic with approximate policy gradient\n",
    "\n",
    "The actor-critic algorithm uses an **approximate policy gradient**.\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) \\approx  = \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ Q_w(s,a) \\big]$$  \n",
    "\n",
    "Which makes the parameter update:   \n",
    "\n",
    "$$\\Delta \\theta = \\alpha\\ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ Q_w(s,a)$$\n",
    "\n",
    "How to estimate the action-value, $Q_w(s,a)$? Can use the policy evaluation of $\\pi_\\theta$, for the parameters $\\theta$. We have examine several methods for policy evaluation:   \n",
    " \n",
    "- Monte Carlo policy evaluation.\n",
    "- Temporal difference (TD) policy evaluation. \n",
    "- Least squares fitting of policy evaluation function by least squares. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias in Actor-Critic methods\n",
    "\n",
    "Using an approximate policy gradient introduces **bias**, which can lead to poor convergence of the solution. How can one choose a value function approximation which minimizes this bias. There are two criteria which must be met:\n",
    "\n",
    "First, the value function must be **compatible** with the policy. By this we mean the following relationship should be true:\n",
    "\n",
    "$$\\nabla_w\\ Q_w(s,a) = \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)$$\n",
    "\n",
    "Second, the value function must have parameters, $\\mathbf{w}$ which minimizes the mean squared error:   \n",
    "\n",
    "$$\\epsilon = \\mathbb{E}_{\\pi_\\theta} \\big[ \\big( Q_{\\pi_\\theta}(s,a) - Q_w(s,a) \\big)^2 \\big]$$\n",
    "\n",
    "The above criteria leads to the following exact policy gradient that meets both:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) \\approx  = \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ Q_w(s,a) \\big]$$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from math import cos, log\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sim_car(x, x_dot, acceleration, x_lims = (-1.2,0.5), x_dot_lims = (-0.07,0.07)):\n",
    "    ## Compute velocity within limits\n",
    "    x_dot_prime = x_dot + 0.001 * acceleration - 0.0025 * cos(3.0 * x)\n",
    "    if(x_dot_prime < x_dot_lims[0]): x_dot_prime = x_dot_lims[0]\n",
    "    if(x_dot_prime > x_dot_lims[1]): x_dot_prime = x_dot_lims[1]\n",
    "        \n",
    "    ## Now update position\n",
    "    x_prime = x + x_dot\n",
    "    if(x_prime < x_lims[0]): x_prime = x_lims[0]\n",
    "    if(x_prime > x_lims[1]): x_prime = x_lims[1]\n",
    "      \n",
    "    ## At the terminal state or not and set reward\n",
    "    if(x_prime >= x_lims[1]): \n",
    "        done = True\n",
    "        reward = 100.0\n",
    "    else: \n",
    "        done = False\n",
    "        reward = -1.0\n",
    "        \n",
    "    return(x_prime, x_dot_prime, done, reward)    \n",
    "        \n",
    "def initalize_car(x_lims = (-0.6,-0.4)):\n",
    "    ## Find random start for car\n",
    "    return(nr.uniform(x_lims[0],x_lims[1]))\n",
    "\n",
    "## Test the function\n",
    "a = -0.0\n",
    "x_dot = [0.0]\n",
    "x = [initalize_car()]\n",
    "for i in range(100):\n",
    "    x_temp, x_dot_temp, done, reward = sim_car(x[i], x_dot[i], a)\n",
    "    x.append(x_temp)\n",
    "    x_dot.append(x_dot_temp)\n",
    "    \n",
    "def plot_car(x, x_dot):    \n",
    "    ## Plot car position\n",
    "    fig = plt.figure()\n",
    "    ax1 = fig.add_subplot(211)    \n",
    "    ax1.plot(x)\n",
    "    ax1.set_ylabel('Positon of car')\n",
    "    \n",
    "    ## PLot car velocity\n",
    "    ax2 = fig.add_subplot(212)  \n",
    "    ax2.plot(x_dot)\n",
    "    ax2.set_ylabel('Velocity of car')\n",
    "    ax2.set_xlabel('Time')\n",
    "    \n",
    "plot_car(x,x_dot)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = -1.2 state = 0\n",
      "x = -1.1105263157894736 state = 1\n",
      "x = -1.0210526315789474 state = 2\n",
      "x = -0.9315789473684211 state = 3\n",
      "x = -0.8421052631578947 state = 4\n",
      "x = -0.7526315789473683 state = 5\n",
      "x = -0.6631578947368421 state = 6\n",
      "x = -0.5736842105263158 state = 7\n",
      "x = -0.4842105263157894 state = 8\n",
      "x = -0.39473684210526305 state = 9\n",
      "x = -0.3052631578947368 state = 10\n",
      "x = -0.21578947368421053 state = 11\n",
      "x = -0.12631578947368416 state = 12\n",
      "x = -0.03684210526315779 state = 13\n",
      "x = 0.05263157894736836 state = 14\n",
      "x = 0.14210526315789473 state = 15\n",
      "x = 0.2315789473684211 state = 16\n",
      "x = 0.3210526315789475 state = 17\n",
      "x = 0.41052631578947385 state = 18\n",
      "x = 0.5 state = 19\n"
     ]
    }
   ],
   "source": [
    "def x_state(x, x_lims = (-1.2,0.5), n_tiles = 20):\n",
    "    \"\"\"Function to compute tile state given positon\"\"\"\n",
    "    state = int((x - x_lims[0])/(x_lims[1] - x_lims[0]) * float(n_tiles))\n",
    "    if(state > n_tiles - 1): state = n_tiles - 1\n",
    "    return(state)\n",
    "\n",
    "for x in list(np.linspace(-1.2,0.5,20)):\n",
    "    print('x = ' + str(x) + ' state = ' + str(x_state(x)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_dot = -0.07 state = 0\n",
      "x_dot = -0.06263157894736843 state = 1\n",
      "x_dot = -0.05526315789473685 state = 2\n",
      "x_dot = -0.04789473684210527 state = 3\n",
      "x_dot = -0.04052631578947369 state = 4\n",
      "x_dot = -0.03315789473684211 state = 5\n",
      "x_dot = -0.02578947368421053 state = 6\n",
      "x_dot = -0.01842105263157895 state = 7\n",
      "x_dot = -0.01105263157894737 state = 8\n",
      "x_dot = -0.00368421052631579 state = 9\n",
      "x_dot = 0.00368421052631579 state = 10\n",
      "x_dot = 0.01105263157894737 state = 11\n",
      "x_dot = 0.01842105263157895 state = 12\n",
      "x_dot = 0.02578947368421053 state = 13\n",
      "x_dot = 0.03315789473684211 state = 14\n",
      "x_dot = 0.04052631578947369 state = 15\n",
      "x_dot = 0.04789473684210527 state = 16\n",
      "x_dot = 0.05526315789473685 state = 17\n",
      "x_dot = 0.06263157894736843 state = 18\n",
      "x_dot = 0.07 state = 19\n"
     ]
    }
   ],
   "source": [
    "def x_dot_state(x_dot, x_dot_lims = (-0.07,0.07), n_tiles = 20):\n",
    "    \"\"\"Function to compute tile state given velocity\"\"\"\n",
    "    state = int((x_dot - x_dot_lims[0])/(x_dot_lims[1] - x_dot_lims[0]) * float(n_tiles))\n",
    "    if(state > n_tiles - 1): state = n_tiles - 1\n",
    "    return(state)\n",
    "\n",
    "for x in list(np.linspace(-0.07,0.07,20)):\n",
    "    print('x_dot = ' + str(x) + ' state = ' + str(x_dot_state(x)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Q(x, x_dot, w):\n",
    "    '''Function to compute action value, Q, given state and parameter vector w'''\n",
    "    return w[1, x_state(x), x_dot_state(x_dot)] * x + w[2, x_state(x), x_dot_state(x_dot)] * x_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_log_pi(a, x, x_dot, theta, sigma):\n",
    "    '''This function computes the gradients of the log probability\n",
    "    for the policy given the state and action. The function assumes\n",
    "    a stochastic Gaussina distributed action space'''\n",
    "    delta_pi = (a - theta[x_state(x), x_dot_state(x_dot)])/sigma\n",
    "    return log(delta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_theta(theta, x, x_dot, a, sigma):\n",
    "    '''This function updates the policy parameter, theta for the state and action specified'''\n",
    "    Q = compute_Q(x, x_dot, w)\n",
    "    delta_pi = delta_log_pi(a, x, x_dot, theta, sigma)\n",
    "    theta[x_state(x), x_dot_state(x_dot)] =  theta[x_state(x), x_dot_state(x_dot)] + alpha *  delta_pi * Q\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_action(x, x_dot, sigma, theta): \n",
    "    '''Function computes a next action, state, reward, and done flag given a state'''\n",
    "    a = nr.normal(loc=theta[x_state(x), x_dot_state(x_dot)], scale=sigma, size=1)\n",
    "    x_prime, x_dot_prime, done, reward = sim_car(x, x_dot, a)\n",
    "    return a, a_prime, x_prime, x_dot_prime, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tile_AC(episodes = 10, gamma = 0.9, epsilon = 0.05, alpha = 0.02, \n",
    "               n = 4, goal = 0.5, x_dot_knot = 0.0):\n",
    "    \n",
    "    ## Initialize the parameter arrays for w, and theta\n",
    "    ## indexed by position, velocity\n",
    "    w = np.zeros((2,20,20))\n",
    "    theta = np.zeros((20,20))\n",
    "\n",
    "    I = 1.0\n",
    "    \n",
    "    ## Loop over the episodes\n",
    "    for _ in range(episodes):\n",
    "        ## Initialize the car state\n",
    "        x_dot = [x_dot_knot]\n",
    "        x_dot_index = x_dot_state(x_dot[0])\n",
    "        x = [initalize_car()]\n",
    "        x_index = x_state(x[0])\n",
    "        \n",
    "        done = False\n",
    "        i = 0\n",
    "        while(not done): # Loop until end of episode\n",
    "            ## find the next action and state\n",
    "            a, a_prime, x_prime, x_dot_prime, reward, done = next_action(x[0], x_dot[0], sigma, theta)\n",
    "            ## Compute the TD error\n",
    "            delta = reward + gamma * compute_Q(x_prime, x_dot_prime, w) - compute_Q(x, x_dot, w)\n",
    "            ## Update the policy parameter theta\n",
    "            theta = update_theta(theta, x, x_dot, a, sigma)\n",
    "            \n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advantage Actor-Critic Methods\n",
    "\n",
    "As previously mentioned, approximate policy gradients can have high variance. The use of a baseline function reduces the variance. A good choice of a **baseline function**. The ideal baseline function must not change the expectation of the policy gradient. A good choice is the **advantage function**:  \n",
    "\n",
    "$$A_{\\pi_\\theta}(s,a) = Q_{\\pi_\\theta}(s,a) - V_{\\pi_\\theta}(s)$$\n",
    "\n",
    "The advantage function is the difference between the action-value function and value function. At convergence the difference is 0. \n",
    "\n",
    "The policy gradient for the advantage function is:  \n",
    "\n",
    "$$\\nabla_{\\theta} J(\\mathbf{\\theta}) \\approx  = \\mathbb{E}_{\\pi_\\theta} \\big[ \\nabla_\\theta\\ log\\ \\pi_\\theta(s,a)\\ A_{\\pi_\\theta}(s,a) \\big]$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2019, Stephen F. Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
