{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To Monte Carlo Reinforcement Learning\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "## Stephen Elston\n",
    "\n",
    "Starting with this lesson, we will turn our attention to a **reinforcement learning**. Reinforcement learning is a distinctive type of machine learning, differing from supervised learning and unsupervised learning. \n",
    "\n",
    "Reinforcement learning has several characteristics, which differentiate this method from other machine learning and from dynamic programming. The table below outlines key differences between the model types:\n",
    "\n",
    "\n",
    "| Model Type | Environment Model | State | Labeled Data | Loss Function |\n",
    "| :--- | :---: | :---: | :---: | :----- |\n",
    "|Supervised Learning | Yes | No | Yes| Error metric |\n",
    "|Unsupervised Learning | Yes | No | No | Error metric |\n",
    "| Bandit Agent | No | No | No | Reward |\n",
    "| Dynamic Programming | Yes | Yes | No | Reward |\n",
    "| Reinforcement Learning | No | Yes | No | Reward | \n",
    "\n",
    "Some highlights of these differences include:  \n",
    "\n",
    "\n",
    "- Like dynamic programming, reinforcement learning **optimizes a reward function**. This is in contrast to supervised and unsupervised learning which attempt to minimize error or loss function.  \n",
    "- **No Markov model** needs to be specified for reinforcement learning, in contrast to dynamic programming.\n",
    "- Reinforcement learning algorithms learn by **experience**. Over time, the algorithm learns a model of the environment and these results are used to optimize the expected reward. Learning from experience is in contrast to supervised learning which uses marked cases (labels).  \n",
    "- Reinforcement learning agents take **actions** and only receive **state** and **rewards** from the environment. These are the only interaction between the RL agent and the environment.   \n",
    "\n",
    "The interaction between a reinforcement learning agent and the environment are illustrated in the figure below. Notice that the only feedback the agent receives from the environment is the reward and and state information. The agent receives no other evidence.   \n",
    "\n",
    "<img src=\"img/RL_AgentModel.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Reinforcement Learning Agent and Environment** </center>  \n",
    "\n",
    "\n",
    "**Suggested readings** for Monte Carlo reinforcement learning Chapter 5 of Sutton and Barto, second edition, provides a good introduction, including many alternative algorithms not discussed here.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Monte Carlo Reinforcement Learning\n",
    "\n",
    "A wide variety of reinforcement learning algorithms have been developed over the past few decades. In this lesson we will explore the basics of the Monte Carlo method. Monte Carlo algorithms have been known for most of the history of reinforcement learning. However, they are generally considered inefficient for several reasons that will become apparent as we proceed:\n",
    "1. Monte Carlo methods rely large numbers of **random samples** to produce estimates. Thus, Monte Carlo algorithms are inherently computationally intensive. \n",
    "2. Monte Carlo reinforcement learning algorithms must **complete an entire episode** before any reward estimate can be produce and the policy improved. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basics of Monte Carlo Simulation\n",
    "\n",
    "Monte Carlo sampling was developed in the 1940s. Originally, Monte Carlo methods were used to compute estimates of complex functions which where analytically intractable. The basic idea is to **compute an estimate** of a complex function by **averaging a large number of samples**. \n",
    "\n",
    "Monte Carlo methods rely on the [**weak law of large numbers**](https://en.wikipedia.org/wiki/Law_of_large_numbers). The law of large numbers is a theorem that states that statistics of independent samples converge to the population values as more unbiased experiments are performed. We can write this mathematically for the **expected value** pr mean as:\n",
    "\n",
    "$$Let\\ \\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n} X_i\\\\\n",
    "then\\ by\\ the\\ law\\ of\\ Large\\ Numbers\\\\\n",
    "\\bar{X} \\rightarrow E(X) = \\mu\\\\\n",
    "as\\\\\n",
    "n \\rightarrow \\infty$$\n",
    "\n",
    "Thus, if we sample some process $X$ enough times (possibly infinite), we can compute the expected value from these samples. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo Reinforcement Learning\n",
    "\n",
    "But, how do we apply Monte Carlo sampling to reinforcement learning? More specifically, how do we apply Monte Carlo sampling to **episodic** reinforcement learning tasks. \n",
    "\n",
    "To understand this algorithm, it helps to examine the backup diagram shown below. This diagram shows Monte Carlo sampling of a single episode.    \n",
    "\n",
    "<img src=\"img/MC_Backup.JPG\" alt=\"Drawing\" style=\"width:75px; height:400px\"/>\n",
    "<center> **Backup Diagram for Monte Carlo Reinforcement Learning** </center>  \n",
    "\n",
    "Starting at the top of the diagram the system is in a state, s. An action, a, causes a transition to a new state. The sampling of the episode proceeds until the terminal state, t, is reached. The return for the initial state can only be computed once the Monte Carlo backup **ends at the terminal state**. In other words, **Monte Carlo algorithms do not bootstrap**.  \n",
    "\n",
    "In reinforcement learning we do not know the model. But, the agent can take a series of actions and find the rewards for these actions. For each episode the agent will accumulate the history of rewards for each action, given the state. \n",
    "\n",
    "Recall that for a finite or episodic Markov reward processes we define the **return** for state transitions starting with the current state. The return is the sum of the rewards for the $T$ future states transitions of the episodic process, and can be expressed as:\n",
    "\n",
    "$$G_t = R_{t+1} + R_{t+2} + \\ldots = R_{T}= \\sum_{k = 0}^{T} R_{t+k+1}$$ \n",
    "\n",
    "Thus, for any episode the Monte Carlo algorithm will sample the return for the states visited. Over a large (actually infinite) number of episodes the Monte Carlo algorithm will sample each action value several times. The sampled return values are then averaged for each state action. This process will converge to the actual action values, which are **unobservable** directly. \n",
    "\n",
    "For sampling a single episode of a Markov process, the Monte Carlo algorithm may or may not visit a state one or more times. The question then becomes, How should returns be computed if a state is visited more than once in an episode? There are two options each of which has different statical convergence properties:\n",
    "1. **First visit** Monte Carlo estimates returns from rewards from the first visit to a state in an episode. We will use first visit Monte Carlo in this lesson.\n",
    "2. **Every visit** Monte Carlo accumulates the rewards for any visit to a state in an episode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of First Visit Monte Carlo RL\n",
    "\n",
    "With this short introduction MC RL learning in mind, tet's try an example. We will sample the action value function using a simple MC algorithm here. \n",
    "\n",
    "**Navigation** to a goal is a significant problem in robotics. Real-world navigation is rather complex. Therefore, in this example we will use a simple analog called a **grid world**. The grid world for this problem is shown below. \n",
    "\n",
    "<img src=\"img/GridWorld.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **A 4x4 Grid World with Terminal State** </center>\n",
    "\n",
    "The grid world consists of a 4x4 set of positions the robot can occupy. Each position is considered a state. The goal is to navigate to state 0, the goal, in the minimum steps. We will explore methods to find policies which reach this goal and achieve maximum reward. \n",
    "\n",
    "Grid position 0 is the goal and a **terminal state**. There are no possible state transitions out of this position. The presence of a terminal state makes this an **episodic Markov random process**. For each episode sampled the robot can start in any other random position, $\\{ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \\}$. This random selection process makes this a **random start** Monte Carlo algorithm. The episode terminates when the robot enters the terminal position (state 0).  \n",
    "\n",
    "\n",
    "\n",
    "In reality, an RL agent may need to explore to find the possible actions when it is in some particular state. To simplify our example, we encode, or represent, these possibilities in a dictionary as shown in the code block below. We use a dictionary of dictionaries to perform the lookup. The keys of the outer dictionary are the identifiers (numbers) of the states. The keys of the inner dictionary are the possible actions and the values are the **successor state**, $s'$, for that transition.  \n",
    "\n",
    "In each state, there are four possible actions the robot can take:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "The MC RL agent has no model. Therefore, beyond these allowed actions, all other information is encapsulated in the environment and is unobservable by the agent. This is the key difference between reinforcement learning and dynamic programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "## Define the transition dictonary of dictionaries:\n",
    "neighbors = {0:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "          1:{'u':1, 'd':5, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':6, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':7, 'l':2, 'r':3},\n",
    "          4:{'u':0, 'd':8, 'l':4, 'r':5},\n",
    "          5:{'u':1, 'd':9, 'l':4, 'r':6},\n",
    "          6:{'u':2, 'd':10, 'l':5, 'r':7},\n",
    "          7:{'u':3, 'd':11, 'l':6, 'r':7},\n",
    "          8:{'u':4, 'd':12, 'l':8, 'r':9},\n",
    "          9:{'u':5, 'd':13, 'l':8, 'r':10},\n",
    "          10:{'u':6, 'd':14, 'l':9, 'r':11},\n",
    "          11:{'u':7, 'd':15, 'l':10, 'r':11},\n",
    "          12:{'u':8, 'd':12, 'l':12, 'r':13},\n",
    "          13:{'u':9, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':10, 'd':14, 'l':13, 'r':15},\n",
    "          15:{'u':11, 'd':15, 'l':14, 'r':15}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To simulate the environment, we need a reward structure. In this case, the robot receives the following rewards:   \n",
    "\n",
    "- 10 for entering position 0. \n",
    "- -1 for attempting to leave the grid. In other words, we penalize the robot for hitting the edges of the grid.  \n",
    "- -0.1 for all other state transitions, which is the cost for the robot to move from one state to another. If we did not have this penalty, the robot could follow any random plan to the goal which did not hit the edges. \n",
    "\n",
    "This **reward structure is unknown to the MC RL agent**. The agent must **learn** the rewards by sampling the environment. Here the rewards are in the form of action values.    \n",
    "\n",
    "We encode these rewards in the same type of dictionary structure used for the foregoing structures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = {0:{'u':10.0, 'd':10.0, 'l':10.0, 'r':10.0},\n",
    "          1:{'u':-1, 'd':-0.1, 'l':10.0, 'r':-0.1},\n",
    "          2:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          4:{'u':10.0, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          6:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          7:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          8:{'u':-0.1, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          11:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          12:{'u':-0.1, 'd':-1.0, 'l':-1.0, 'r':-0.1},\n",
    "          13:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          15:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-1.0}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the properties of the environment defined, it is time to create an environment simulator. The code in the cell below simulates the environment. The function is called with a state and action. It returns the next state, 's_prime' and 'reward'. \n",
    "\n",
    "To simplify the rest of the code in this notebook we are treating the dictionaries as global. In general, this would be considered poor programming practice. \n",
    "\n",
    "Execute the code in the cell below and observe the results from the test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, -1, False)\n",
      "(5, -0.1, False)\n",
      "(2, -0.1, False)\n",
      "(0, 10.0, True)\n"
     ]
    }
   ],
   "source": [
    "def simulate_environment(s, action, neighbors = neighbors, rewards = rewards, terminal = 0):\n",
    "    \"\"\"\n",
    "    Function simulates the environment\n",
    "    returns s_prime and reward given s and action\n",
    "    \"\"\"\n",
    "    s_prime = neighbors[s][action]\n",
    "    reward = rewards[s][action]\n",
    "    return (s_prime, reward, is_terminal(s_prime, terminal))\n",
    "\n",
    "def is_terminal(state, terminal = 0):\n",
    "    return state == terminal\n",
    "\n",
    "## Test the function\n",
    "for a in ['u', 'd', 'r', 'l']:\n",
    "    print(simulate_environment(1, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results look correct. It appears the simulator works correctly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MC Policy Evaluation\n",
    "\n",
    "**Policy evaluation** is an essential part of reinforcement learning. Policy evaluation is required to compare the performance of different reinforcement learning methods. Policy evaluation is performed using state-value estimation methods. In this case, we will use a Monte Carlo state-value estimation algorithm.   \n",
    "\n",
    "To develop and test a policy evaluation algorithm, we need to define the transition probabilities for an initial policy. We set the probabilities for each transition as a **uniform distribution** leading to random action by the robot. As there are 4 possible transitions from each state, this means all transition probabilities are 0.25. In other words, this is a random policy which does not favor any particular plan. \n",
    "\n",
    "The initial uniform transition probabilities are encoded using a dictionary of dictionaries. The organization of this data structure is identical to the foregoing data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_policy = {0:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                        2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For policy evaluation We are using random start Monte Carlo starts. The code in the cell below generates a random state to start the Monte Carlo episode, which is not the terminal state. Execute this code and examine the results pf the test cases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 10, 12, 2, 3, 11, 7, 1, 8]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def start_episode(n_states):\n",
    "    '''Function to find a random starting value for the episode\n",
    "    that is not the terminal state'''\n",
    "    state = nr.choice(range(n_states))\n",
    "    while(is_terminal(state)):\n",
    "         state = nr.choice(range(n_states))\n",
    "    return state\n",
    "\n",
    "## test the function to make sure never starting in terminal state\n",
    "[start_episode(15) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in the cell below finds an action given the state and the policy by the following steps:  \n",
    "1. The probability of taking an action and is determined by the transition probabilities specified by the policy. \n",
    "2. The next state and the reward are received by making queries to the environment using the `simulate_environment` function. \n",
    "\n",
    "Notice that the agent receives no specific information about the environment.   \n",
    "\n",
    "Execute this code and examine the results of the test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('u', 0, 10.0, True)\n",
      "('r', 2, -0.1, False)\n",
      "('d', 6, -0.1, False)\n",
      "('r', 3, -1.0, False)\n",
      "('u', 0, 10.0, True)\n",
      "('d', 9, -0.1, False)\n",
      "('d', 10, -0.1, False)\n",
      "('d', 11, -0.1, False)\n",
      "('u', 4, -0.1, False)\n",
      "('l', 8, -0.1, False)\n",
      "('l', 9, -0.1, False)\n",
      "('l', 10, -0.1, False)\n",
      "('d', 12, -1.0, False)\n",
      "('r', 14, -0.1, False)\n",
      "('l', 13, -0.1, False)\n",
      "('r', 15, -1.0, False)\n"
     ]
    }
   ],
   "source": [
    "def take_action(state, policy, actions = {1:'u', 2:'d', 3:'l', 4:'r'}):\n",
    "    '''Function takes action given state using the transition probabilities \n",
    "    of the policy'''\n",
    "    ## Find the action given the transistion probabilities defined by the policy.\n",
    "    action = actions[nr.choice(range(len(actions)), p = list(policy[state].values())) + 1]\n",
    "    s_prime, reward, is_terminal = simulate_environment(state, action)\n",
    "    return (action, s_prime, reward, is_terminal)\n",
    "\n",
    "## Test function for several states\n",
    "for s in range(16):\n",
    "    print(take_action(s, initial_policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now have all the prerequisites for the functions which do most of the work for policy evaluation. \n",
    "\n",
    "The first function is called `MC_episode`, which performs first visit Monte Carlo for one episode by the following steps:\n",
    "1. A loop is executed until episode ends when a terminal state is reached. \n",
    "2. The state transitions are determined using the policy with the `take_action` function.  \n",
    "3. The function accumulates the reward and and number of times a state has been visited.  \n",
    "\n",
    "The `MC_state_values` function computes the state values for a number of episodes by the following steps:\n",
    "\n",
    "1.  Iterates over the specified number of episodes. For each episode the `MC_episode` function is called and the reward accumulated. \n",
    "2. Once the episodes have concluded, the state values are normalized by dividing the accumulated rewards by the number of visits.\n",
    "\n",
    "Execute the code and examine the resulting state-values for the random walk policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         5.57434357 2.76479076 2.04389007]\n",
      " [5.65890318 2.78748107 2.01472932 1.59934829]\n",
      " [2.22452534 1.88735317 1.52328405 1.24284272]\n",
      " [1.81315597 1.47838912 1.2324538  1.06050072]]\n"
     ]
    }
   ],
   "source": [
    "def MC_episode(policy, G, n_visits, episode, n_states): \n",
    "    '''Function creates the Monte Carlo samples of one episode.\n",
    "    This function does most of the real work'''\n",
    "    ## For each episode we use a list to keep track of states we have visited.\n",
    "    ## Once we visit a state we need to accumulate values to get the returns\n",
    "    states_visited = []\n",
    "    states = list(policy.keys())\n",
    "        \n",
    "    ## Find the starting state\n",
    "    current_state = start_episode(n_states)\n",
    "    terminal = False\n",
    "    g = 0.0\n",
    "        \n",
    "    while(not terminal):\n",
    "        ## Find the next action and reward\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "            \n",
    "        ## Add the reward to the states visited if this is a first visit  \n",
    "        if(current_state not in states_visited):\n",
    "            ## Mark that the current state has been visited \n",
    "            states_visited.append(current_state) \n",
    "            ## Add the reward to states visited \n",
    "            for state in states_visited:\n",
    "                n_visits[state] = n_visits[state] + 1.0\n",
    "                G[state] = G[state] + (reward - G[state])/n_visits[state]\n",
    "        \n",
    "        ## Update the current state for next transition\n",
    "        current_state = s_prime \n",
    "    return (G, n_visits) \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def MC_state_values(policy, n_episodes):\n",
    "    '''Function that evaluates the state value of \n",
    "    a policy using the Monte Carlo method.'''\n",
    "    ## Create list of states \n",
    "    states = list(initial_policy.keys())\n",
    "    n_states = len(states)\n",
    "    \n",
    "    ## An array to hold the accumulated returns as we visit states\n",
    "    G = np.zeros((n_states))\n",
    "    \n",
    "    ## An array to keep track of how many times we visit each state so we can \n",
    "    ## compute the mean\n",
    "    n_visits = np.zeros((n_states))\n",
    "    \n",
    "    ## Iterate over the episodes\n",
    "    for i in range(n_episodes):\n",
    "        G, n_visits = MC_episode(policy, G, n_visits, i, n_states) # neighbors, i, n_states)\n",
    "    return(state_values) \n",
    "        \n",
    "\n",
    "## Test the functions\n",
    "nr.seed(335)\n",
    "state_values = MC_state_values(initial_policy, n_episodes = 2000)\n",
    "print(state_values.reshape((4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results look promising. The returns become smaller the further the state is from the goal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement\n",
    "\n",
    "Now that we have a way to evaluate a policy using a first visit Monte Carlo algorithm we need a method to improve policy. Monte Carlo policy improvement employs the following steps:\n",
    "1. Sample rewards given state, s, and action a, following policy $\\pi$. \n",
    "2. The action values are computed from returns. \n",
    "2. The policy is updated using an $\\epsilon$-greedy algorithm. \n",
    "\n",
    "The first function, `MC_action_value_episode`, required computes the action-values given the current policy for a single episode. This function uses a first visit Monte Carlo algorithm to find the action-values for a single episode with the following steps:\n",
    "\n",
    "1. The `take_action` function is used to determine the next action given the state following the policy. \n",
    "2. The visit is recoded in the `state_actions_visited` array. \n",
    "3. The core of the function are a pair of nested loops over states and actions. For states that have been visited, the reward is summed and the number of visits is incremented.   \n",
    "\n",
    "The numpy array, `Q`, holds the accumulated reward. The row index is for state and the column index is for action, $\\{ up,\\ down,\\ left,\\ right \\}$. Th|e numpy array `state_actions_visited` is indexed in the same manner.\n",
    "\n",
    "Execute this code and examine the results of the basic test cases. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [10.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 4.95        0.          0.          0.        ]\n",
      " [ 0.          0.          3.26666667  0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          1.92      ]\n",
      " [ 2.425       0.          1.58333333  0.        ]\n",
      " [ 0.          1.05        1.34285714  0.        ]\n",
      " [ 0.          0.          0.92222222  0.        ]]\n",
      "\n",
      "n_visits\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [2. 0. 0. 0.]\n",
      " [0. 0. 3. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 5.]\n",
      " [4. 0. 6. 0.]\n",
      " [0. 8. 7. 0.]\n",
      " [0. 0. 9. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "def MC_action_value_episode(policy, Q, n_visits, inital_state, n_states, n_actions, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    '''Function creates the Monte Carlo samples of action values for one episode.\n",
    "    This function does most of the real work'''\n",
    "    ## For each episode we use a list to keep track of states we have visited.\n",
    "    ## Once we visit a state we need to accumulate values to get the returns\n",
    "    state_actions_visited = np.zeros((n_states, n_actions))\n",
    "    \n",
    "    current_state = initial_state\n",
    "    terminal = False  \n",
    "    while(not terminal):\n",
    "        ## Find the next action and reward\n",
    "        action, s_prime, reward, terminal = take_action(current_state, policy)\n",
    "\n",
    "        action_idx = action_index[action]         \n",
    "        \n",
    "        ## Check if this state-action has been visited.\n",
    "        if(state_actions_visited[current_state, action_idx] != 1.0):\n",
    "            ## Mark that the current state-action has been visited \n",
    "            state_actions_visited[current_state, action_idx] = 1.0  \n",
    "            ## This is first vist MS, so must loop over all state-action pairs and \n",
    "            ## add the reward and increment the count for the ones visited.\n",
    "            for s,a in list(itertools.product(range(n_states), range(n_actions))):\n",
    "                ## Add reward to if these has been a visit to the state\n",
    "                if(state_actions_visited[s,a] == 1.0):\n",
    "                    n_visits[s,a] = n_visits[s,a] + 1.0\n",
    "                    Q[s,a] = Q[s,a] + (reward - Q[s,a])/n_visits[s,a]    \n",
    "        ## Update the current state for next transition\n",
    "        current_state = s_prime\n",
    "    return (Q, n_visits) \n",
    "\n",
    "## Basic test of the function\n",
    "n_actions = 4\n",
    "n_states = 16\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "n_visits = np.zeros((n_states, n_actions))\n",
    "initial_state = 15\n",
    "Q, n_visits = MC_action_value_episode(initial_policy, Q, n_visits, initial_state, n_states, n_actions)\n",
    "print('Q')\n",
    "print(Q)\n",
    "print('\\nn_visits')\n",
    "print(n_visits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function in the cell below computes the action values using the specified number of episodes, using first visit Monte Carlo. The steps are:\n",
    "\n",
    "1. For each episode, the `MC_action_value_episode` function is called. \n",
    "2. The accumulated rewards are normalized by the number of visits to compute the action values. \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.        ]\n",
      " [ 3.57052632  2.43333333 10.          3.13918919]\n",
      " [ 2.10337079  1.64219653  3.48073555  1.14419643]\n",
      " [ 0.91688312  1.38        1.55333333  1.39047619]\n",
      " [10.          2.65591398  3.27257525  2.41083521]\n",
      " [ 3.70247934  1.84874372  3.60513196  1.58318584]\n",
      " [ 1.9759799   1.4252      2.02717391  1.11276596]\n",
      " [ 1.14324324  1.125       1.58410959  1.34324324]\n",
      " [ 3.45892857  1.77863071  2.1031401   1.52708333]\n",
      " [ 2.22318841  1.35928144  2.08611111  1.16832579]\n",
      " [ 1.47220903  1.10419847  1.64965986  1.17649573]\n",
      " [ 1.17349398  0.92406015  1.22119725  0.89594595]\n",
      " [ 2.06716229  1.48298279  1.47825465  1.48818898]\n",
      " [ 1.62674591  1.15032258  1.50510708  1.22557143]\n",
      " [ 1.15139073  1.00937099  1.21923211  0.98679962]\n",
      " [ 1.00664093  0.80046512  0.99974737  0.82717557]]\n"
     ]
    }
   ],
   "source": [
    "def print_Q(Q):\n",
    "    Q = pd.DataFrame(Q, columns = ['up', 'down', 'left', 'right'])\n",
    "    print(Q)\n",
    "\n",
    "def MC_action_values(policy, Q, n_episodes, inital_state):\n",
    "    '''Function evaluates the action-values given a policy for the specified number of episodes and \n",
    "    initial state'''\n",
    "    n_states = len(policy)\n",
    "    n_actions = len(policy[0])\n",
    "    ## Array to count visits to action-value pairs\n",
    "    n_visits = np.zeros((n_states, n_actions))\n",
    "    ## Dictionary to hold neighbor states\n",
    "    neighbors = {}\n",
    "    \n",
    "    ## Loop over number of episodes\n",
    "    for _ in range(n_episodes):\n",
    "        ## One episode of MC\n",
    "        Q, n_visits = MC_action_value_episode(policy, Q, n_visits, initial_state, n_states, n_actions)\n",
    "    return(Q)\n",
    "    \n",
    "## Basic test of the function\n",
    "n_episodes = 1000\n",
    "initial_state = 15\n",
    "Q = np.zeros((n_states, n_actions))\n",
    "initial_state = 15\n",
    "Q = MC_action_values(initial_policy, Q, n_episodes, initial_state)\n",
    "print_Q(Q)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the code required to perform first visit Monte Carlo action value estimation, its time to perform policy improvement. \n",
    "\n",
    "The `update_policy` function in the cell below performs $\\epsilon$-greedy policy improvement given the action values. The steps are:\n",
    "\n",
    "1. Loop over all states to update the policy for each state. \n",
    "2. For each state find the actions with the maximum action value. The best action may not be unique. \n",
    "3. The transition probability is computed for the actions with the largest action value. All other actions will be assigned a transition probability of $\\epsilon$.\n",
    "4. The policy is updated with the new action values. \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 1: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 2: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 3: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 4: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 5: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 6: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 7: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 8: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 9: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 10: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 11: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 12: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 13: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 14: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 15: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_policy(policy, Q, epsilon, action_index = {'u':0, 'd':1, 'l':2, 'r':3}):\n",
    "    '''Updates the policy based on estiamtes of Q using \n",
    "    an epslion greedy algorithm. The action with the highest\n",
    "    action value is used.'''\n",
    "    \n",
    "    ## Find the keys for the actions in the policy\n",
    "    keys = list(policy[0].keys())\n",
    "    \n",
    "    ## Iterate over the states and find the maximm action value.\n",
    "    for state in range(len(policy)):\n",
    "        ## First find the index of the max Q values  \n",
    "        q = Q[state,:]\n",
    "        max_action_index = np.where(q == max(q))[0]\n",
    "\n",
    "        ## Find the probabilities for the transitions\n",
    "        n_transitions = float(len(q))\n",
    "        n_max_transitions = float(len(max_action_index))\n",
    "        p_max_transitions = (1.0 - epsilon *(n_transitions - n_max_transitions))/(n_max_transitions)\n",
    "  \n",
    "        ## Now assign the probabilities to the policy as epsilon greedy.\n",
    "        for key in keys:\n",
    "            if(action_index[key] in max_action_index): policy[state][key] = p_max_transitions\n",
    "            else: policy[state][key] = epsilon\n",
    "    return(policy)                \n",
    "\n",
    "update_policy(initial_policy, Q, 0.1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalized Policy Improvement\n",
    "\n",
    "When performing MC RL, we can start with an arbitrary initial policy. The MC RL agent will then improve this policy and in the process will **learn the Markov process model**. Again, this is a key difference with dynamic programming where this model is specified. \n",
    "\n",
    "RL MC uses the idea of **generalized policy improvement**. Recall that GPI divides the policy improvement and evaluation steps into opposing processes and iterates between them. This process can be done in a quite granular way, even on a single episode or even state at a time. The figure below illustrates the concept of GPI.  \n",
    "\n",
    "<img src=\"img/GPI.JPG\" alt=\"Drawing\" style=\"width:250px; height:250px\"/>\n",
    "<center> **Concept of Generalized Policy Improvement** </center>\n",
    "\n",
    "The code in the cell below uses the GPI method to improve the policy for the grid world using GPI. The outer loop or cycle performs one iteration of GPI which has two steps: \n",
    "1. At the start of the loop the returns for the current policy are evaluated. In this case we use the average of several MC episodes. Keep in mind that the agent's walk is determined by the policy, but the returns accumulated are from the environment. \n",
    "2. Next, the policy is updated using the new return values. The policy is improved by increasing the transition probabilities for actions (transitions) with higher reward. To ensure that the algorithm **continues to explore**, transition probabilities are never set to 0 but rather to a small minimum value $\\epsilon$. \n",
    " \n",
    "Addition details on the operation of this algorithm can be obtained by reading the comments. Execute this code and examine the results.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n"
     ]
    }
   ],
   "source": [
    "def MC_policy_improvement(policy, n_episodes, n_cycles, inital_state = 15, epsilon = 0.1, n_actions = 4):\n",
    "    '''Function perfoms GPI using Monte Carlo value estimation.\n",
    "    Updates to policy are epsilon greedy to prevent the algorithm\n",
    "    from being trapped at some point.'''\n",
    "    Q = np.zeros((len(policy), n_actions))\n",
    "    ## Iterate over the required number of cycles\n",
    "    for _ in range(n_cycles):\n",
    "        Q = MC_action_values(policy, Q, n_episodes, inital_state)\n",
    "        policy = update_policy(policy, Q, epsilon = epsilon)\n",
    "    return(policy)\n",
    "\n",
    "improved_policy = MC_policy_improvement(initial_policy, 5000, 5, epsilon = 0.1)  \n",
    "for state in range(16):\n",
    "    print(improved_policy[state])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The improved policy makes sense. Transitions that move the robot closer to the goal are favored. \n",
    "\n",
    "Finally, execute the code in the cell below to compute the returns for the improved policy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         5.57434357 2.76479076 2.04389007]\n",
      " [5.65890318 2.78748107 2.01472932 1.59934829]\n",
      " [2.22452534 1.88735317 1.52328405 1.24284272]\n",
      " [1.81315597 1.47838912 1.2324538  1.06050072]]\n"
     ]
    }
   ],
   "source": [
    "nr.seed(369)\n",
    "state_values = MC_state_values(improved_policy, n_episodes = 10000)\n",
    "print(state_values.reshape((4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These returns are significantly higher than for the random policy, indicating the policy is indeed an improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'u': 0.25, 'd': 0.25, 'l': 0.25, 'r': 0.25}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.7, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.7, 'd': 0.1, 'l': 0.1, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n",
      "{'u': 0.1, 'd': 0.1, 'l': 0.7, 'r': 0.1}\n"
     ]
    }
   ],
   "source": [
    "improved_policy_2 = MC_policy_improvement(initial_policy, 100, 5000, epsilon = 0.1)  \n",
    "for state in range(16):\n",
    "    print(improved_policy_2[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         5.57434357 2.76479076 2.04389007]\n",
      " [5.65890318 2.78748107 2.01472932 1.59934829]\n",
      " [2.22452534 1.88735317 1.52328405 1.24284272]\n",
      " [1.81315597 1.47838912 1.2324538  1.06050072]]\n"
     ]
    }
   ],
   "source": [
    "nr.seed(369)\n",
    "state_values = MC_state_values(improved_policy_2, n_episodes = 10000)\n",
    "print(state_values.reshape((4,4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, 2019, Stephen F Elston. All rights reserved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
