{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Bayesian Networks (BNs)\n",
    "\n",
    "\n",
    "\n",
    "## CSCI E-83\n",
    "## Stephen Elston\n",
    "\n",
    "**Bayesian Networks** or **BNs** are one of the basic representations of probabilistic graphical models. BNs are **directed acyclic graphs** or **DAGs** which form a compact **representation** of a joint distribution. Schematically we can view the role of representation in an intelligent agent as shown in the figure below.\n",
    "\n",
    "<img src=\"img/Representation.JPG\" alt=\"Drawing\" style=\"width:400px; height:200px\"/>\n",
    "<center> **Representation in an intelligent agent** </center>\n",
    "\n",
    "The representation captures the **conditional independence structure** of the joint distribution. Taking advantage of the independency structure of the distribution can greatly reduce the computational complexity of inference on the graphic model. \n",
    "\n",
    "The directed nature of the BN can capture **causality**. **Influence** travels along the directed edges of the graph. However, keep in mind, information on independence can flow both directions along the graph.\n",
    "\n",
    "The **conditional marginal distribution** of each variable is only dependent on its **parents**. In other words, **belief** is propagated along the DAG. In this sense, the DAG is a representation of a **generative sampling process**. \n",
    "\n",
    "Graphical models allow us to add **evidence** as data is observed for certain variables. Further, we can perform **queries** on **hidden variables**, variables with no evidence. We will address queries and inference in another lesson.\n",
    "\n",
    "**Suggested readings:** The following reading is an optional supplement to the material presented here:\n",
    "- Barber, Sections 3.1, 3.2, 3.3, or\n",
    "- Murphy, Section 10.1, 10.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some graph terminology\n",
    "\n",
    "Before we dive into the probabilistic graphical models let's examine some basic graph terminology. Graphs are are comprised of two types of elements:\n",
    "1. **Nodes** or **vertices** in probabilistic models contain the distribution information.\n",
    "2. **Edges** define the relationships between the nodes. Edges can be **directed** or **undirected**. Information flows in the direction specified along a directed edge. Undirected edges have no preferred direction of information flow. \n",
    "3. The set of edges of a graph (less nodes) are called the **skeleton**. \n",
    "4. Nodes connected by an edge are **neighbors**. \n",
    "5. The **degree** of a node is the number of neighbors. \n",
    "6. **Parents** are nodes with a directed edge to another node, know as the **child**. Parents are said to have **influence** on their children. \n",
    "7. **Ancestors** are nodes which are grandparents, great grandparents, etc, of a with a path of directed edges to a child. \n",
    "8. **Descendants** are nodes which receive information from parents or ancestors along a path of directed edges. For example, a child node is a descendant of its parents and grand parents. \n",
    "9. **Leaf** nodes have no directed edges to ancestors. \n",
    "\n",
    "We can classify graphs by their topology. The figure below shows some basic graph types.   \n",
    "\n",
    "<img src=\"img/GraphTypes.JPG\" alt=\"Drawing\" style=\"width:600px; height:200px\"/>\n",
    "<center> **Examples of basic graph types** </center>\n",
    "\n",
    "Four graph types are illustrated in the figure above:   \n",
    "1. **Directed acyclic graph** is a graph where all edges are **directed** and there are no **cycles**. Information or **influence** flows in one direction along a directed edge. In the case illustrated, influence flows  $A \\rightarrow B$ and $C \\rightarrow B$, but not the other way. This makes $A$ and $C$ **ancestors** or **parents** of $B$. An **acyclic graph** has no cycle.\n",
    "2. **Directed cyclic graph** has directed edges but contains a cycle. A **cycle** is a directed path that starts and returns to the same node. In this case the illustrated case the cycle is $A \\rightarrow B \\rightarrow C \\rightarrow A$.\n",
    "3. **Undirected graph** has no directed edges. \n",
    "4. **Partially directed graph** has both directed and undirected edges. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple example\n",
    "\n",
    "We have already worked on the student GRE score and letter problem. As a reminder, a student would like to make inferences or **queries** on the joint distribution $P(D,I,S,G,L)$. She can make the following assertions about the independencies for this problem: \n",
    "\n",
    "1. The degree of difficulty, D, of the machine learning course is **unconditionally independent** of all other variable, $\\{D \\bot I,S,G.L \\}$.\n",
    "2. The intelligence of the student is, I, **unconditionally independent** of all other variable, $\\{I \\bot D,S,G.L \\}$.\n",
    "3. The quality of the student's recommendation letter, L, is **conditionally independent**  of intelligence, her GRE score, and her letter, given her grade, $\\{L \\bot I,S,L\\ |\\ G\\}$.\n",
    "4. The student's grade in the machine learning course, G, is **conditionally independent** of her GRE score and her letter, give the difficulty of the course and her intelligence, $\\{G  \\bot S,L\\ |\\ I,D\\}$. \n",
    "5. The students GRE scores, S, are **conditionally independent** of her grade, difficulty of the course and letter, given her intelligence, $\\{S \\bot G,D,L\\ |\\ I\\}$.\n",
    "\n",
    "Given these independecies, the distribution can be factored as follows:\n",
    "\n",
    "$$P(D,I,S,G,L) = p(D)\\ p(I)\\ p(S|I)\\ p(G|I,D)\\ P(L|G)$$\n",
    "\n",
    "It is relatively easy to construct a BN that represents the independencies of this distribution as shown in the figure below.  \n",
    "\n",
    "<img src=\"img/LetterDAG.JPG\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center> **DAG for the student score and letter distribution** </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independency and uniqueness of BNs\n",
    "\n",
    "We have just defined a BN with the Independency structure of the distribution defined by the assertions. The independency structure is quite important since by imposing independencies can greatly reduce the computational complexity of the graph.   \n",
    "\n",
    "However, there is no guarantee that a given BN uniquely defines the independency structure of a distribution. A simple example of four BNs representing the same independency structure is shown in the figure below. In each case, the following **conditional independence** assertions are true:\n",
    "\n",
    "$$A\\ \\bot\\ C\\ |\\ B\\\\\n",
    "Or,\\\\\n",
    "C\\ \\bot\\ A\\ |\\ B$$\n",
    "\n",
    "\n",
    "<img src=\"img/Dependency.JPG\" alt=\"Drawing\" style=\"width:600px; height:300px\"/>\n",
    "<center> **Multiple BNs with same dependence structure** </center>\n",
    "\n",
    "Each of these cases supports that   \n",
    "\n",
    "1. **Causal or Cascade:** In this case A causes B which causes C. This information is represented in the directed edges. If B is an evidence variable then A and C are separated and cannot dependent. Once a value is assigned to B, A depends on B and C depends on B, but A and C are **decoupled**. \n",
    "2. **Evidential:** Once again, the evidence variable B separates A and C, just as they do in the causal case. In this case, **evidence** is added to the DAG which allows inference of A.   \n",
    "3. **Common Evidence, V-Structure, or Collider:** In this case, having evidence on B **blocks** the path or **separates** A from C. In other words, knowing B explains away A and C. \n",
    "4. **Common Cause or Common Parent:** In this case, B is causal to both A and C, making A and C independent. \n",
    "\n",
    "A key point here is that any of these DAGs has the same aforementioned independence properties. In other words, multiple DAGs can exhibit the same independencies. We say that these DAGs exhibit **I-equivalence**, since they have the same independencies. \n",
    "\n",
    "We can write a generalization of the independence properties of a BN:\n",
    "\n",
    "> **Definition:** On a graph $G$, the variable $X_i$ is independent of its **nondecendents** given its **parents**, $Pa_{X_i}$. We can say that $G$ includes as set of **local conditional independence assumptions**:\n",
    "\n",
    "$$I_{\\ell}(G): \\{X_i\\ \\bot\\ Nondecendants_{X_i}\\ |\\ Pa_{X_i}:\\ \\forall i\\ \\}$$\n",
    "\n",
    "Let's look at some more examples of applying this definition The Figure below illustrates some key cases.\n",
    "\n",
    "<img src=\"img/Independencies2.JPG\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center> **Multiple BNs with same dependence structure** </center>\n",
    "\n",
    "Following the numbering of each DAG, we can make make the following assertions (statements) about independencies in each of these DAGs:\n",
    "\n",
    "1. **$P(A,B,C) = P(B\\ |\\ A,C)\\ P(A)\\ P(C)$:** In this case, A and C are causes of B, and are therefore independent.\n",
    "2. **$P(A,B,C) = P(A\\ |\\ B)\\ P(C\\ |\\ B)\\ P(B)$:** Here, B is the cause of independent effects A and C.\n",
    "3. **$P(A,C\\ |\\ B) = P(A\\ |\\ B)\\ P(C\\ |\\ B)$:** A and C are dependent given B. While the causes of A and C are a-priori independent, having evidence for B tells us about both causes A and C. \n",
    "4. **$P(A,C\\ |\\ B) = P(A\\ |\\ B)\\ P(C\\ |\\ B)$:** A and C are independent when conditioned on B. Given the cause B we know everything about each effect A and C. \n",
    "5. **$P(A,C\\ |\\ B) = P(A\\ |\\ B)\\ P(C\\ |\\ B)$:** Since D is a descendant of B, this case is the same as 3 above.\n",
    "6. **$P(A,C) = P(A)\\ P(C)$:** A and C are made independent by marginalizing over B. In other words, in the absence of specific information on B, A and C are unconditionally independent.\n",
    "7. **$P(A,C) \\ne P(A)\\ P(C)$:** In this case, A and C are dependent when marginalizing over B. In other words the effects, A and C are dependent on B. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pgmpy for directed graphical model\n",
    "\n",
    "We can use the Python pgmpy package to create and manipulate Bayesian DAGs. You can find complete documentation on this package [here](http://pgmpy.org/).\n",
    "\n",
    "If you have not installed pgmpy, un-comment the code in the cell below and execute it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pgmpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To import the parts of pgmpy we will use for the examples in this notebook, execute the code in the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with the example, we need to define the graph. The first step is to define the **skeleton** of the graph. This is done by defining the edges. Each edge is define as a tuple; (start node, end node). Execute the code in the cell below to create the skeleton of the graph      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define the network structure.\n",
    "## The first value of the tuple defines the origin\n",
    "## of the connector and the second the terminal point\n",
    "## of the directed edge. \n",
    "student_model = BayesianModel([('D', 'G'), ('I', 'G'), ('G', 'L'), ('I', 'S')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the graph skeleton defined, we need to specify the probability distributions of the nodes. In this case, we are using discrete distributions.   \n",
    "\n",
    "We will start with the two independent variables, $D$ and $I$, which are binomially distributed. As there are only two possible states, the **variable cardinality** is set to 2. Notice that the sum of the probabilities must add to 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════╤═════╕\n",
      "│ D_0 │ 0.7 │\n",
      "├─────┼─────┤\n",
      "│ D_1 │ 0.3 │\n",
      "╘═════╧═════╛\n",
      "╒═════╤═════╕\n",
      "│ I_0 │ 0.8 │\n",
      "├─────┼─────┤\n",
      "│ I_1 │ 0.2 │\n",
      "╘═════╧═════╛\n"
     ]
    }
   ],
   "source": [
    "## Define the independent variables\n",
    "CDP_D = TabularCPD(variable='D', variable_card=2, values=[[0.7, 0.3]])\n",
    "print(CDP_D)\n",
    "CDP_I = TabularCPD(variable='I', variable_card=2, values=[[0.8, 0.2]])\n",
    "print(CDP_I)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will deal with the children of the two independent variables. Now we need to define the variables associated with **evidence variables**. An evidence variable is an **observable** variable for which data can be acquired. We will explore observable variables and evidence later. \n",
    "\n",
    "The letter variable, $L$, is dependent on the grade $G$. Therefore we say that $G$ is the **evidence** variable. The variable $L$ has two possible states, so we set the variable cardinality to 2, which creates two lists. The evidence variable, $G$, has cardinality of 3, so there are three values for each of the states of the two states $L$. \n",
    "\n",
    "In a similar manner we can model the GRE score, $S$, using the evidence variable, Intelligence, $I$. Execute the code in the cell below and examine the tables. The variable $S$ has two possible states, so we set the variable cardinality to 2, which creates two lists. The evidence variable has cardinality of 2, so there are two values for each of the states of the two states $L$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════╤═════╤═════╤══════╕\n",
      "│ G   │ G_0 │ G_1 │ G_2  │\n",
      "├─────┼─────┼─────┼──────┤\n",
      "│ L_0 │ 0.1 │ 0.4 │ 0.99 │\n",
      "├─────┼─────┼─────┼──────┤\n",
      "│ L_1 │ 0.9 │ 0.6 │ 0.01 │\n",
      "╘═════╧═════╧═════╧══════╛\n",
      "╒═════╤══════╤═════╕\n",
      "│ I   │ I_0  │ I_1 │\n",
      "├─────┼──────┼─────┤\n",
      "│ S_0 │ 0.95 │ 0.2 │\n",
      "├─────┼──────┼─────┤\n",
      "│ S_1 │ 0.05 │ 0.8 │\n",
      "╘═════╧══════╧═════╛\n"
     ]
    }
   ],
   "source": [
    "# Define the distributions with a single conditional variable \n",
    "# or evidence. \n",
    "CDP_L = TabularCPD(variable='L', variable_card=2, \n",
    "                   values=[[0.1, 0.4, 0.99],\n",
    "                           [0.9, 0.6, 0.01]],\n",
    "                   evidence=['G'], # Leter depends on the grade\n",
    "                   evidence_card=[3])\n",
    "print(CDP_L)\n",
    "\n",
    "CDP_S = TabularCPD(variable='S', variable_card=2,\n",
    "                   values=[[0.95, 0.2],\n",
    "                           [0.05, 0.8]],\n",
    "                   evidence=['I'], # GRE score depneds on intelligence\n",
    "                   evidence_card=[2])\n",
    "print(CDP_S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last variable, Grade, $G$. There are two evidence variables in this case, Intelligence, $I$ and Difficulty, $D$. The $G$ variable has three possible states, or cardinality of 3. Each of the evidence variables has two possible states resulting in 4 values; $4 = 2 * 2$. This relationship leads to a **three dimensional table**, defined by three lists with four values in each list. We can represent this relationship using multiple indicies on one axis. Execute the code in the cell below and examine the resulting table.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═════╤═════╤══════╤══════╤═════╕\n",
      "│ I   │ I_0 │ I_0  │ I_1  │ I_1 │\n",
      "├─────┼─────┼──────┼──────┼─────┤\n",
      "│ D   │ D_0 │ D_1  │ D_0  │ D_1 │\n",
      "├─────┼─────┼──────┼──────┼─────┤\n",
      "│ G_0 │ 0.3 │ 0.05 │ 0.9  │ 0.5 │\n",
      "├─────┼─────┼──────┼──────┼─────┤\n",
      "│ G_1 │ 0.4 │ 0.25 │ 0.08 │ 0.3 │\n",
      "├─────┼─────┼──────┼──────┼─────┤\n",
      "│ G_2 │ 0.3 │ 0.7  │ 0.02 │ 0.2 │\n",
      "╘═════╧═════╧══════╧══════╧═════╛\n"
     ]
    }
   ],
   "source": [
    "# Define the distributions with a multiple conditional variables \n",
    "# or evidence. \n",
    "CDP_G = TabularCPD(variable='G', variable_card=3, \n",
    "                   values=[[0.3, 0.05, 0.9,  0.5],\n",
    "                           [0.4, 0.25, 0.08, 0.3],\n",
    "                           [0.3, 0.7,  0.02, 0.2]],\n",
    "                  evidence=['I', 'D'],\n",
    "                  evidence_card=[2, 2])\n",
    "print(CDP_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one last step. We need to add the CDPs to the model. Execute the code in the cell below to add the CDPs to the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<TabularCPD representing P(D:2) at 0x212e0e12940>,\n",
       " <TabularCPD representing P(I:2) at 0x212e0e12978>,\n",
       " <TabularCPD representing P(S:2 | I:2) at 0x212e0e12f28>,\n",
       " <TabularCPD representing P(G:3 | I:2, D:2) at 0x212e0e12c50>,\n",
       " <TabularCPD representing P(L:2 | G:3) at 0x212e0e12ef0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "student_model.add_cpds(CDP_D, CDP_I, CDP_S, CDP_G, CDP_L)\n",
    "student_model.get_cpds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(student_model.get_factorized_product())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now defined the directed graphical model using three steps:\n",
    "1. Skeleton of the graph with the directed edges,\n",
    "2. **conditional probability distributions** (**CPD**) for each node, and\n",
    "3. the evidence variables for some nodes. \n",
    "\n",
    "With the graph defined we can find the independencies of the DAG representing the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(D _|_ I, S)\n",
       "(I _|_ D)\n",
       "(S _|_ G, D, L | I)\n",
       "(G _|_ S | I, D)\n",
       "(L _|_ I, D, S | G)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Find the local independencies in the BN\n",
    "# Getting all the local independencies in the network.\n",
    "student_model.local_independencies(['D', 'I', 'S', 'G', 'L'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These independencies are exactly as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGs and independence maps\n",
    "\n",
    "We can generalize beyond the above example we can say that a DAG represents a probability distribution **consistent** with the DAG. We can factor the the graph:\n",
    "$$P(X) = \\prod_{i=1:d}P(x_i|X_{\\pi_i})$$\n",
    "\n",
    "Where, $P(X)$ is probability distribution, $X_{\\pi_i}$ is the set of **parents** of $x_i$, a node on the graph, and $d$. In other words, in a DAG a node $x_i$ is independent of all other nodes give the set of the node's parents, $X_{\\pi_i}$.\n",
    "\n",
    "We can also generalize the idea of independencies with the **local Markov assumption** applied to a DAG G by writing:\n",
    "\n",
    "$$I(G) = \\{ X \\bot Z\\ |\\ Y:\\ dsep_G(X:Z|Y) \\} $$ \n",
    "\n",
    "This leads us to an important definition of the properties of DAGs:\n",
    "\n",
    "> **Definition:** A DAG, G, is an **independence map** or **I-map** of a distribution P if $I_l(G) \\subseteq I(P)$, where $I(P)$ is the set of independencies of the distribution P and $I_l(G)$ is the set of independencies of the DAG. We can express this relationship as:\n",
    "\n",
    "$$(X\\ \\bot\\ Y\\ |\\ Z_G) \\Rightarrow (X\\ \\bot\\ Y\\ |\\ Z_P)$$\n",
    "\n",
    "We can see that there are multiple DAGs for which $I_l(G) \\subseteq I(P)$ can be true.  But, what are the practical implications of this definition? \n",
    "\n",
    "From the above definition, you can see that it can be the case that a DAG can be an I-map, but not a complete representation of the independencies of $P$. This leads us to another definition:\n",
    "\n",
    ">**Definition:** A DAG, G, is a **minimal I-map** for a distribution P if removal of even a single edge renders $G$ not an I-map. \n",
    "\n",
    "The concept of an I-map is quite useful. In fact, no graph $G$ can be considered a useful representation of a distribution $P$ if it is not an I-map of $P$. A minimal I-map will have the minimum possible complexity, but may be hard to find in practice. \n",
    "\n",
    "Note that a minimal I-map may not be unique for a distribution $P$. If two DAGs have the same I-map, we say they are **I-equivalent**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D-separation\n",
    "\n",
    "This leads to the following definition:\n",
    "\n",
    "> **Definition:** Given subsets $X$, $Y$ and $Z$, $X$ and $Y$ are conditionally independent or **D-separated** conditioned on the subset $Z$ if they are separated on the moralized graph. \n",
    "\n",
    "The definition leads to another related definition:\n",
    "\n",
    "> **Definition:** A graph $G$ is a **dependency map** or **D-map** of a distribution $P$ if the graph contains every conditional independence in $P$. We can represent this relationship as:\n",
    "\n",
    "$$(X\\ \\bot\\ Y\\ |\\ Z_G) \\Leftarrow (X\\ \\bot\\ Y\\ |\\ Z_P)$$\n",
    "\n",
    "Recalling the definition of an I-map we can create another definition.\n",
    "\n",
    "> **Definition:** If a graph $G$ is both an I-map and a D-map of a distribution $P$ we say that $G$ is a **prefect map** of $P$. We can write this relationships as:\n",
    "\n",
    "$$(X\\ \\bot\\ Y\\ |\\ Z_G) \\Leftrightarrow (X\\ \\bot\\ Y\\ |\\ Z_P)$$\n",
    "\n",
    "> **Note:** While it would be nice if a graph were a perfect map of a distribution, this will rarely be the case in real world problems. Thus, a perfect map is mostly useful as a reference point in developing probabilistic graphical models.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trails in DAGs\n",
    "\n",
    "\n",
    "The concept of **trails** provides another way to think about separation in DAGs. You can think of a trail as a path between one subset of a DAG and another subset of the DAG. If the trail is **active** there is no independency between the  If a trail between subsets is **blocked** the subsets are independent. Trails are blocked by evidence or observed. Following on the previous figure we can define four cases:\n",
    "\n",
    "1. **Causal trail;** is active if and only if B is not observed. \n",
    "2. **Evidential trail;** is active if and only if B is not observed.\n",
    "3. **Common cause;** is active if and only if B is not observed.\n",
    "4. **Common effect;** is active if and only if B or one of B's decendants is observed.  \n",
    "\n",
    "The independencies resulting from trails can be generalized s **d-separation**:\n",
    "\n",
    "> **Definition:** Let $X$, $Y$ and $Z$ be three subsets of graph G. We say $X$ and $Y$ are **d-separated** given Z if there is no active trail between and node $x \\in X$ and any othee node $y \\in Y$ given Z. We can express d-separation as $d-sep_G(X;Y\\ |\\ Z)$.   \n",
    "\n",
    "The intuition is that when there is evidence on the trail between subsets, the subset with evidence effectively blocks the trail creating the separation.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes ball algorithm\n",
    "\n",
    "Another way to understand independencies on trails is with the concept of **Bayes' Ball**, also know as the **Bayes Ball algorithm**. The idea is to think about how a ball can move along the trails in the graph. If the ball encounters a blocked trail it bounces back and cannot continue. Following these rules, the independencies in a DAG can be found. \n",
    "\n",
    "The rules for the Bayes' ball for basic DAG structures are illustrated below. Evidence variables are shown as shaded. The path of the ball are shown with arrows. Notice that the ball can travel in either direction along the directed edges. Blocked trails are shown with a bar blocking the path of the ball. \n",
    "\n",
    "<img src=\"img/BayesBall.JPG\" alt=\"Drawing\" style=\"width:600px; height:350px\"/>\n",
    "<center> **Illustration of using Bayes' ball to find indepenencies** </center>\n",
    "\n",
    "Using the 6 rules shown graphically above, we can find all possible independencies of a DAG. With a bit of thought, you should be able to see that these 6 simple rules are the same as the 4 simple cases stated at the beginning of these section. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the active trails in our example graph by executing the code in the cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Active trail between D and G -> True\n",
      "Active trail between G and S -> True\n",
      "Active trail between D and I -> False\n",
      "Active trail between L and I -> True\n",
      "Active trail between L and S -> True\n"
     ]
    }
   ],
   "source": [
    "def test_active(start,end):\n",
    "    print('Active trail between ' + start + ' and ' + end + ' -> ' + str(student_model.is_active_trail(start,end)))\n",
    "starts = ['D', 'G', 'D', 'L', 'L']\n",
    "ends = ['G', 'S', 'I', 'I', 'S']\n",
    "for s,e in zip(starts,ends): test_active(s,e)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evidence, hidden variables and queries\n",
    "\n",
    "Up to now we have only dealt with distributions of the variables in our graphical models. \n",
    "\n",
    "It is possible that we will **observe** data values for some variables, which is typically referred to as **evidence**. Evidence provides specific values for **observable variables**. Once we have evidence, the size of the tables we must work with can be reduced. Only entries consistent with the evidence need be retained. Thus, computational complexity can be reduced, sometimes greatly. \n",
    "\n",
    "An example of evidence might be a specific score $S$ on the GRE test for a student. If the score is high, $S^1$, then we need not consider table entries with a low GRE score $S^0$.\n",
    "\n",
    "Not all variables are observable however. We must estimate the marginal distribution of these **unobservable variables** given the evidence. We say that we perform a **query** on the unobservable variables. The query returns the marginal distribution of the variable. \n",
    "\n",
    "For example, human intelligence cannot be directly observed. In our student model, we might observe a student's GRE score as $S^1$ and course grade as $G^0$. Given this evidence, we can now query to compute the marginal posterior distribution of the student's intelligence. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertain and unreliable evidence\n",
    "\n",
    "In the real world, evidence can be **uncertain** or **unreliable**. Both of these situations effect the marginal distribution returned by a query on any unobservable variable. \n",
    "\n",
    "### Uncertain evidence\n",
    "\n",
    "First, let's consider the effect of **uncertain evidence**. There are many situations in which uncertain evidence can arise. For example, an observer might only have a certain confidence in an observation. An ornithologist sees a bird, but may only be 70% certain of the species. The quality of the habitat (an unobservable) is a parent of species present. How can we include the uncertainty of the ornithologist's observation into the model? \n",
    "\n",
    "A model of habitat quality, $q$, given the species, $s$ can be written:\n",
    "\n",
    "$$p(q\\ |\\ s) = \\sum_s p(q,s\\ |\\ \\tilde{s}) = \\sum_s p(q\\ |\\ s, \\tilde{s})\\ p(s\\ |\\ \\tilde{s})$$ \n",
    "\n",
    "Now, $q$ is dependent on $s$ independent of the uncertainty of the observation $\\tilde{s}$, leading to:\n",
    "\n",
    "$$\\sum_s p(q\\ |\\ s)\\ p(s\\ |\\ \\tilde{s})$$\n",
    "\n",
    "### Unreliable evidence\n",
    "\n",
    "Let's say our ornithologist decides her observations are going too slowly and decides to hire an assistant. Unfortunately, the person she hires has exaggerated his sill at bird species identification skills. Thus, this observer is providing **unreliable evidence**. Unfortunately, the ornithologist must now apply a rather abstract and subjective treatment of this unreliable evidence. \n",
    "\n",
    "The ornithologist must replace the unreliable evidence with **virtual evidence** or **likelihood evidence**. Mathematically we can express this process as:\n",
    "\n",
    "$$p(q\\ |\\ s) = \\frac{1}{Z} p(s\\ |\\ q)\\ p(q) \\rightarrow \\frac{1}{Z} p(H\\ |\\ q)\\ p(q)$$    \n",
    "\n",
    "where, $Z$ is a normalization or partition function, and $H$ is the fixed virtual evidence selected by the ornithologist. Since $p(H\\ |\\ q)$ is selected subjectively this cannot be treated as a proper probability distribution. \n",
    "\n",
    "> **Note:** The process of using virtual evidence is purely Bayesian. A frequentist would take one of two possible approaches. The simplest approach is simply to remove or **censor** the suspect values from the data set. Alternatively, the suspect values can be **confidence weighted** to account for the suspect values.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge engineering with DAGs\n",
    "\n",
    "We have seen how DAGs are a powerful and compact representation of joint distributions. But, how do we specify the DAG? There are two distinct aspects of the specification process:\n",
    "\n",
    "1. The **qualitative specification** which defines the skeleton of the DAG. The skeleton specifies the influences and independencies in the belief network. In many practical situations, creating the qualitative specification is difficult and problematic. There are a number of ways the qualitative specification of a DAG including: \n",
    "  - **Prior knowledge** of influence or causal relationships based on knowledge of the probability distributions in the domain. \n",
    "  - **Eliciting assessments of experts** as to which variables are influential or causal to other variables. \n",
    "  - **Learning** the independence structure of the probability distribution from data. This is a computationally difficult problem involving .      \n",
    "      \n",
    "      \n",
    "2. The **quantitative specification** which defines the details of the conditional distributions. There are a number of ways to create the quantitative specification of a DAG: \n",
    "  - **Prior knowledge** of the conditional distributions.\n",
    "  - **Empirically determining the conditional distributions** from data. \n",
    "  - **Eliciting opinions of experts** as to the likelihood of events. \n",
    "  - **Fitting** parameters of a distribution model.    \n",
    "     \n",
    "  \n",
    "It should be noted that multiple methods are often used to create these specifications.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Bayesian network semantics  \n",
    "\n",
    "We have seen how distributions can be factorized on Bayesian networks. We can make the following statements:\n",
    "\n",
    "1. Factorization on BNs is based on conditional distributions.  \n",
    "2. Factorization on a BN implies conditional independencies.   \n",
    "\n",
    "These facts lead to the following definition:\n",
    "\n",
    "> **Definition:** Given a distribution $P$ and a Bayesian network $G$, $P$ factorizes as a set of CDPs specified as the nodes of $G$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F Elston. All rights reserved."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
