{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Q-Learning \n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    "## Stephen Elston\n",
    "\n",
    "\n",
    "In the previous lesson we explored Monte Carlo **reinforcement learning**. MC RL required that the returns for an entire episode be computed before any values are available for use. The disadvantage of this approach is the the full set of returns are required for state value or action value estimates. But, how can we get state values or action-values in fewer time steps? It turns out there are algorithms which compute estimates in as few as one step known as **time difference learning** or **TD-learning** and **Q-learning**. \n",
    "\n",
    "Recall that reinforcement learning has several distinctive characteristics, which differentiate this method from other machine learning and dynamic programming:\n",
    "- **No Markov model** needs to be specified for reinforcement learning, in contrast to dynamic programming.\n",
    "- Like dynamic programming, reinforcement learning **optimizes a reward function**. This is in contrast to supervised and unsupervised learning which use an error or objective function.  \n",
    "- Reinforcement learning algorithms learn by **experience**. Over time, the algorithm learns a model of the environment and these results are used to optimize the expected reward. Learning from experience is in contrast to supervised learning which uses known marked cases. \n",
    "- Reinforcement learning agents take **actions** and only receive **state** and **rewards** from the environment. These are the only interaction between the RL agent and the environment.    \n",
    "\n",
    "The interaction between a reinforcement learning agent and the environment are illustrated in the figure below. Notice that the only feedback the agent receives from the environment is reward and state.   \n",
    "\n",
    "<img src=\"img/RL_AgentModel.JPG\" alt=\"Drawing\" style=\"width:500px; height:300px\"/>\n",
    "<center> **Reinforcement Learning Agent and Environment** </center>  \n",
    "\n",
    "The ability to learn from experience is an attractive concept. This method of learning seems to mimic human learning. However, reinforcement learning has proven difficult to use in real-world applications. For a review of successes and problems arising when applying RL to robotics see [Kobler et. al.](https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Kober_IJRR_2013.pdf). At the present time, RL has mostly succeeded in cases where simulations can be used to gain experience. \n",
    "\n",
    "**Suggested readings** for TD and Q reinforcement learning, Chapters 6 and 7 of Sutton and Barto, second edition, provides a good introductions, including many alternative algorithms and details not discussed here.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On Policy vs. Off Policy Algorithms\n",
    "\n",
    "In this lesson we will explore examples of two broad categories of RL algorithms known as **on policy** and **off policy** methods. \n",
    "\n",
    "On policy methods evaluate and improve a single policy. On policy methods converge quickly and often to good solution. In general, **exploration** is performed using $\\epsilon$-greedy methods. The TD(0) and MC algorithms we have examined are examples of on policy methods. On policy algorithms are known to have good convergence properties. \n",
    "\n",
    "In contrast, off policy methods use two policies. The policy the agent is following is called the **behavior policy**, denoted $b(A_t | S_t)$. The policy being improved is known as the **target policy**, denoted $\\pi (A_t | S_t)$. The agent obtains samples of the environment while following the behavior policy. These samples are used to improve the target policy. An advantage of off policy methods is that a deterministic behavior policy can be used while a better target policy is developed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Time Difference RL\n",
    "\n",
    "With this short introduction TD RL in mind, let's try an example. We will sample the value function using a basic TD(0) algorithm here. \n",
    "\n",
    "As discussed in other labs, **Navigation** to a goal is a significant problem in robotics. Real-world navigation is rather complex. Therefore, in this example we will use a simple analog called a **grid world**. The grid world for this problem is shown below. \n",
    "\n",
    "<img src=\"img/GridWorld.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **A 4x4 Grid World with Terminal State** </center>\n",
    "\n",
    "The grid world consists of a 4x4 set of positions the robot can occupy. Each position is considered a state. The goal is to navigate to state 0, the goal, in the minimum steps. We will explore methods to find policies which reach this goal and achieve maximum reward. \n",
    "\n",
    "Grid position 0 is the goal and a **terminal state**. There are no possible state transitions out of this position. The presence of a terminal state makes this an **episodic Markov random process**. For each episode sampled the robot can start in any other random position, $\\{ 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15 \\}$. This random selection process makes this a **random start** TD algorithm. The episode terminates when the robot enters the terminal position (state 0).  \n",
    "\n",
    "In reality, an RL agent may need to explore to find the possible actions when it is in some particular state. To simplify our example, we encode, or represent, these possibilities in a dictionary as shown in the code block below. We use a dictionary of dictionaries to perform the lookup. The keys of the outer dictionary are the identifiers (numbers) of the states. The keys of the inner dictionary are the possible actions and the values are the **successor state**, $s'$, for that transition.  \n",
    "\n",
    "In each state, there are four possible actions the robot can take:\n",
    "- up, u\n",
    "- down, d,\n",
    "- left, l\n",
    "- right, r\n",
    "\n",
    "The TD RL agent has no model for the environment. Therefore, beyond these allowed actions, all other information is encapsulated in the environment and is unobservable by the agent. This is the key difference between reinforcement learning and dynamic programming. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, array([-1. , -0.1, 10. , -0.1]), False)\n",
      "(5, array([-0.1, -0.1, -0.1, -0.1]), False)\n",
      "(2, array([-1. , -0.1, -0.1, -0.1]), False)\n",
      "(0, array([10., 10., 10., 10.]), True)\n"
     ]
    }
   ],
   "source": [
    "## import numpy for latter\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "import pandas as pd\n",
    "\n",
    "## Define the transition dictonary of dictionaries:\n",
    "neighbors = {0:{'u':0, 'd':0, 'l':0, 'r':0},\n",
    "          1:{'u':1, 'd':5, 'l':0, 'r':2},\n",
    "          2:{'u':2, 'd':6, 'l':1, 'r':3},\n",
    "          3:{'u':3, 'd':7, 'l':2, 'r':3},\n",
    "          4:{'u':0, 'd':8, 'l':4, 'r':5},\n",
    "          5:{'u':1, 'd':9, 'l':4, 'r':6},\n",
    "          6:{'u':2, 'd':10, 'l':5, 'r':7},\n",
    "          7:{'u':3, 'd':11, 'l':6, 'r':7},\n",
    "          8:{'u':4, 'd':12, 'l':8, 'r':9},\n",
    "          9:{'u':5, 'd':13, 'l':8, 'r':10},\n",
    "          10:{'u':6, 'd':14, 'l':9, 'r':11},\n",
    "          11:{'u':7, 'd':15, 'l':10, 'r':11},\n",
    "          12:{'u':8, 'd':12, 'l':12, 'r':13},\n",
    "          13:{'u':9, 'd':13, 'l':12, 'r':14},\n",
    "          14:{'u':10, 'd':14, 'l':13, 'r':15},\n",
    "          15:{'u':11, 'd':15, 'l':14, 'r':15}}\n",
    "\n",
    "rewards = {0:{'u':10.0, 'd':10.0, 'l':10.0, 'r':10.0},\n",
    "          1:{'u':-1, 'd':-0.1, 'l':10.0, 'r':-0.1},\n",
    "          2:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          3:{'u':-1.0, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          4:{'u':10.0, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          5:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          6:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          7:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          8:{'u':-0.1, 'd':-0.1, 'l':-1.0, 'r':-0.1},\n",
    "          9:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          10:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-0.1},\n",
    "          11:{'u':-0.1, 'd':-0.1, 'l':-0.1, 'r':-1.0},\n",
    "          12:{'u':-0.1, 'd':-1.0, 'l':-1.0, 'r':-0.1},\n",
    "          13:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          14:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-0.1},\n",
    "          15:{'u':-0.1, 'd':-1.0, 'l':-0.1, 'r':-1.0}}\n",
    "\n",
    "\n",
    "def action_lookup(index):\n",
    "    \"\"\"Helper function returns action given an index\"\"\"\n",
    "    action_dic = {0:'u', 1:'d', 2:'l', 3:'r'}\n",
    "    return action_dic[index]\n",
    "\n",
    "def index_lookup(action):\n",
    "    \"\"\"Helper function returns index given action\"\"\"\n",
    "    index_dic = {'u':0, 'd':1, 'l':2, 'r':3}\n",
    "    return index_dic[action]\n",
    "\n",
    "\n",
    "def next_state(state, action_index, neighbors = neighbors, action_lookup = action_lookup):\n",
    "    return(neighbors[state][action_lookup[action_index]])\n",
    "\n",
    "def simulate_environment(s, action, neighbors = neighbors, rewards = rewards, terminal = 0):\n",
    "    \"\"\"\n",
    "    Function simulates the environment for Q-learning.\n",
    "    returns s_prime and reward given s and action\n",
    "    \"\"\"\n",
    "    s_prime = neighbors[s][action]\n",
    "    reward_prime = np.array([rewards[s_prime][a] for a in rewards[0].keys()])\n",
    "    return (s_prime, reward_prime, is_terminal(s_prime, terminal))\n",
    "    \n",
    "\n",
    "def is_terminal(state, terminal = 0):\n",
    "    return state == terminal\n",
    "\n",
    "## Test the function\n",
    "for a in ['u', 'd', 'r', 'l']:\n",
    "    print(simulate_environment(1, a))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "As we have just seen, the SARSA algorithm is an on policy action value TD estimation method. The **Q-learning** algorithm is a **off policy** TD action value estimation method. \n",
    "\n",
    "The update formula for single step Q-learning or **Q-learning(0)** is:\n",
    "\n",
    "$$Q(S_t,A_t) = Q(S_t,A_t) + \\alpha \\big[ R_{t+1} + \\gamma\\ max_a Q(S_{t+1},a) - Q(S_t,A_t) \\big]$$  \n",
    "\n",
    "Where,   \n",
    "$\\delta_t = R_{t+1} + \\gamma max_a Q(S_{t+1},a) - Q(S_t,A_t) = $ the TD error,   \n",
    "$max_a = $ the maximum operator applied to all possible actions in state $S_{t+1}$,   \n",
    "$Q(S_t,A_t) = $ is the action value in state S given action A,  \n",
    "$R_{t+1} = $ is the reward for the next time step,   \n",
    "$\\alpha = $ the learning rate,   \n",
    "$\\gamma = $ discount factor.  \n",
    "\n",
    "The use of the operator $max_a$ makes Q-learning greedy. But, why does using this operator result in an off-policy algorithm? To answer this question, examine the backup diagram shown below. \n",
    "\n",
    "<img src=\"img/Q-Learning.JPG\" alt=\"Drawing\" style=\"width:200px; height:150px\"/>\n",
    "<center> **Backup Diagram for one-step Q-Learning** </center>\n",
    "\n",
    "The $max_a$ greedily picks the action with the greatest value, regardless of policy. Therefore, Q-learning is an off-policy algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why is Q-learning off-policy\n",
    "\n",
    "$$\\pi(S_t,A_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Example\n",
    "\n",
    "The code in the cell below implements the one step Q-learning(0) algorithm. The code is nearly identical to the SARSA(0) code shown previously. The main difference is the addition of the $max_a$ operation when computing the TD error, $\\delta_t$.  Additional details on this algorithm can be seen by reading the code comments.  \n",
    " \n",
    "Execute this code for the random walk policy on the grid world and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13, 2, -1.0),\n",
       " (12, 3, -0.1),\n",
       " (7, 3, -1.0),\n",
       " (11, 3, -1.0),\n",
       " (12, 3, -0.1),\n",
       " (6, 1, -0.1),\n",
       " (9, 0, -0.1),\n",
       " (13, 2, -1.0),\n",
       " (7, 3, -1.0),\n",
       " (1, 3, -0.1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_policy = {0:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        1:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}, \n",
    "                        2:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        3:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        4:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        5:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        6:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        7:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        8:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        9:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        10:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        11:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        12:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        13:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        14:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25},\n",
    "                        15:{'u':0.25, 'd':0.25, 'l':0.25, 'r':0.25}}\n",
    "\n",
    "def start_episode(n_states, n_actions):\n",
    "    '''Function to find a random starting values for the episode\n",
    "    that is not the terminal state'''\n",
    "    state = nr.choice(range(n_states))\n",
    "    while(is_terminal(state)):  ## Make sure not starting at the terminal state\n",
    "         state = nr.choice(range(n_states))\n",
    "    ## Now find a random starting action index\n",
    "    a_index = nr.choice(range(4), size = 1)[0]\n",
    "    s_prime, reward, terminal = simulate_environment(state, action_lookup(a_index))   \n",
    "    return state, a_index, reward[a_index] ## action_lookup(a_index), reward[a_index]\n",
    "\n",
    "## test the function to make sure never starting in terminal state\n",
    "[start_episode(15,4) for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('u', 0, array([10., 10., 10., 10.]), True)\n",
      "('d', 5, array([-0.1, -0.1, -0.1, -0.1]), False)\n",
      "('r', 3, array([-1. , -0.1, -0.1, -1. ]), False)\n",
      "('r', 3, array([-1. , -0.1, -0.1, -1. ]), False)\n",
      "('d', 8, array([-0.1, -0.1, -1. , -0.1]), False)\n",
      "('l', 4, array([10. , -0.1, -1. , -0.1]), False)\n",
      "('l', 5, array([-0.1, -0.1, -0.1, -0.1]), False)\n",
      "('u', 3, array([-1. , -0.1, -0.1, -1. ]), False)\n",
      "('l', 8, array([-0.1, -0.1, -1. , -0.1]), False)\n",
      "('r', 10, array([-0.1, -0.1, -0.1, -0.1]), False)\n",
      "('r', 11, array([-0.1, -0.1, -0.1, -1. ]), False)\n",
      "('u', 7, array([-0.1, -0.1, -0.1, -1. ]), False)\n",
      "('l', 12, array([-0.1, -1. , -1. , -0.1]), False)\n",
      "('d', 13, array([-0.1, -1. , -0.1, -0.1]), False)\n",
      "('r', 15, array([-0.1, -1. , -0.1, -1. ]), False)\n",
      "('r', 15, array([-0.1, -1. , -0.1, -1. ]), False)\n"
     ]
    }
   ],
   "source": [
    "def take_action(state, policy):\n",
    "    '''Function takes action given state using the transition probabilities \n",
    "    of the policy'''\n",
    "    ## Find the action given the transistion probabilities defined by the policy.\n",
    "    action = action_lookup(nr.choice(range(len(policy[0].keys())), p = list(policy[state].values()))) \n",
    "    s_prime, reward, terminal = simulate_environment(state, action)\n",
    "    return (action, s_prime, reward, terminal)\n",
    "\n",
    "## Test function for several states\n",
    "for s in range(16):\n",
    "    print(take_action(s, initial_policy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           up      down       left     right\n",
      "0    0.000000  0.000000   0.000000  0.000000\n",
      "1    9.378568  8.825915  11.111111  9.638781\n",
      "2    8.257252  9.129247  12.677774  9.353216\n",
      "3    7.579711  8.831401  10.566764  7.324834\n",
      "4   11.111111  7.751036   9.705699  9.090306\n",
      "5   10.963034  9.440596  10.820000  9.334741\n",
      "6    9.238181  8.792638  10.496105  8.953630\n",
      "7    9.293451  8.843077   9.056586  6.268314\n",
      "8   12.337982  8.706439   8.156734  9.047736\n",
      "9   10.363309  9.148866  10.726134  8.561491\n",
      "10   9.069376  8.683755   9.999179  8.740681\n",
      "11   9.061688  8.381832   8.897572  6.185006\n",
      "12  10.817929  7.395941   7.966654  9.011821\n",
      "13  10.343257  7.316011   9.337529  8.752100\n",
      "14   8.984951  4.974708   9.148080  8.538702\n",
      "15   8.731064  6.030507   8.722266  6.081399\n"
     ]
    }
   ],
   "source": [
    "def print_Q(Q):\n",
    "    Q = pd.DataFrame(Q, columns = ['up', 'down', 'left', 'right'])\n",
    "    print(Q)\n",
    "\n",
    "def update_Q(Q, current_state, a_index, reward, alpha, gamma):\n",
    "    \"\"\"Function to update the actions values in the Q matrix\"\"\"\n",
    "    ## Get s_prime given s and a\n",
    "    s_prime, reward_prime, terminal = simulate_environment(current_state, action_lookup(a_index))\n",
    "    a_prime_index = nr.choice(np.where(reward_prime == max(reward_prime))[0], size = 1)[0]\n",
    "    ## Update the action values \n",
    "    Q[current_state,a_index] = Q[current_state,a_index] + alpha * (reward + gamma * (Q[s_prime,a_prime_index] - Q[current_state,a_index]))\n",
    "    return Q, s_prime, reward_prime, terminal, a_prime_index\n",
    "\n",
    "def Q_learning_0(policy, episodes, alpha = 0.2, gamma = 0.9):\n",
    "    \"\"\"\n",
    "    Function to perform Q-learning(0) control policy improvement.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    n_actions = len(policy[0].keys())\n",
    "    \n",
    "    ## Initialize Q matrix\n",
    "    Q = np.zeros((n_states,n_actions))\n",
    "    \n",
    "    for _ in range(episodes): # Loop over the episodes\n",
    "        terminal = False\n",
    "        ## Find the inital state, action index and reward\n",
    "        current_state, a_index, reward = start_episode(n_states,n_actions)\n",
    "        \n",
    "        while(not terminal): # Episode ends where get to terminal state   \n",
    "            ## Update the action values in Q\n",
    "            Q, s_prime, reward_prime, terminal, a_prime_index = update_Q(Q, current_state, a_index, reward, alpha, gamma)\n",
    "            ## Set action, reward and state for next iteration\n",
    "            a_index = a_prime_index\n",
    "            current_state = s_prime\n",
    "            reward = reward_prime[a_prime_index]\n",
    "    return(Q)\n",
    "\n",
    "Q = Q_learning_0(initial_policy, 1000)\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 1: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 2: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 3: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 4: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 5: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 6: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 7: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 8: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 9: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 10: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 11: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 12: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 13: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 14: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 15: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_policy(policy, Q, epsilon):\n",
    "    '''Updates the policy based on estiamtes of Q using \n",
    "    an epslion greedy algorithm. The action with the highest\n",
    "    action value is used.'''\n",
    "    \n",
    "    ## Find the keys for the actions in the policy\n",
    "    keys = list(policy[0].keys())\n",
    "    \n",
    "    ## Iterate over the states and find the maximm action value.\n",
    "    for state in range(len(policy)):\n",
    "        ## First find the index of the max Q values  \n",
    "        q = Q[state,:]\n",
    "        max_action_index = np.where(q == max(q))[0]\n",
    "\n",
    "        ## Find the probabilities for the transitions\n",
    "        n_transitions = float(len(q))\n",
    "        n_max_transitions = float(len(max_action_index))\n",
    "        p_max_transitions = (1.0 - epsilon *(n_transitions - n_max_transitions))/(n_max_transitions)\n",
    "  \n",
    "        ## Now assign the probabilities to the policy as epsilon greedy.\n",
    "        for key in keys:\n",
    "            if(index_lookup(key) in max_action_index): policy[state][key] = p_max_transitions\n",
    "            else: policy[state][key] = epsilon\n",
    "    return(policy)                \n",
    "\n",
    "update_policy(initial_policy, Q, 0.1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-Learning\n",
    "\n",
    "The Q-learning algorithm just presented has a significant **bias**. To understand why this might be consider the following thought experiment. In most cases the sampled action values are inaccurate. Some action values will have a positive error and some will have a negative error. In the error is on the order of the values themselves, the $max_a$ operator has a reasonable chance of selecting an action value that is the largest because of this error. However, the $max_a$ operator will never select an action with a low value simply because of the errors. The net result is a bias toward action values with the largest positive error. \n",
    "\n",
    "What can be done to correct this situation? One relatively simple and effective algorithm is known as **double Q-learning**. Double Q-learning maintains two tables of action values. The values from one table are used to perform the bootstrap updates of the other table and vice versa. This approach averages out the bias. For two tables, $Q_1$ and $Q_2$ we can express double Q-learning as follows:\n",
    "\n",
    "$$Q_1(S_t,A_t) = Q_1(S_t,A_t) + \\alpha \\big[ R_{t+1} + \\gamma Q_2 \\big( S_{t+1},argmax_a Q_1(S_{t+1} , a) \\big) - Q_1(S_t,A_t) \\big] \\\\\n",
    "Q_2(S_t,A_t) = Q_2(S_t,A_t) + \\alpha \\big[ R_{t+1} + \\gamma Q_1 \\big( S_{t+1},argmax_a Q_2(S_{t+1} , a) \\big) - Q_2(S_t,A_t) \\big]$$  \n",
    "\n",
    "With a 0.5 probability one or the other of these expressions is used for the TD update at each time step. While double Q-learning requires twice as much memory, to maintain the two tables, the computational complexity is the same when compared to Q-learning. \n",
    "\n",
    "> **Note:** Another **unbiased** one step off policy TD algorithm is known as **expected SARSA**. See Section 6.6 of Sutton and Barto, second edition, for details.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Double Q-Learning\n",
    "\n",
    "The code in the cell below implements the double Q-learning algorithm. The steps are essentially the same as the foregoing code, except for the updates of the two Q tables. Additional details on this algorithm can be seen by reading the code comments.  \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           up      down       left     right\n",
      "0    0.000000  0.000000   0.000000  0.000000\n",
      "1    7.507145  5.265139  11.111111  6.370485\n",
      "2    7.196761  8.066859  11.060283  8.515178\n",
      "3    2.818573  7.706406   8.661550  4.470108\n",
      "4   11.111111  7.553649   6.328946  3.288597\n",
      "5   10.841107  8.534530  10.822172  8.296378\n",
      "6    9.861807  7.659583   9.604808  7.774826\n",
      "7    8.154928  7.106454   8.318308  3.819241\n",
      "8   11.000008  8.237891   5.902324  8.056589\n",
      "9    9.335032  7.106689  10.046072  7.656490\n",
      "10   8.305749  7.223802   8.147776  7.159293\n",
      "11   7.703266  6.927750   7.877955  3.615568\n",
      "12  10.105615  3.583539   3.703075  7.697137\n",
      "13   8.199108  2.103443   7.928437  7.078143\n",
      "14   7.996960  4.008990   7.915629  6.802648\n",
      "15   7.073849  2.164160   7.270687  2.495695\n"
     ]
    }
   ],
   "source": [
    "def update_double_Q(q1, q2, current_state, a_index, reward, alpha, gamma):\n",
    "    \"\"\"Function to update the actions values in the Q matrix\"\"\"\n",
    "    ## Get s_prime given s and a\n",
    "    s_prime, reward_prime, terminal = simulate_environment(current_state, action_lookup(a_index))\n",
    "    a_prime_index = nr.choice(np.where(reward_prime == max(reward_prime))[0], size = 1)[0]\n",
    "    ## Update the action values \n",
    "    q1[current_state,a_index] = q1[current_state,a_index] + alpha * (reward + gamma * (q2[s_prime,a_prime_index] - q1[current_state,a_index]))\n",
    "    return q1, s_prime, reward_prime, terminal, a_prime_index\n",
    "\n",
    "\n",
    "def double_Q_learning_0(policy, episodes, alpha = 0.2, gamma = 0.9):\n",
    "    \"\"\"\n",
    "    Function to perform Q-learning(0) control policy improvement.\n",
    "    \"\"\"\n",
    "    ## Initialize the state list and action values\n",
    "    states = list(policy.keys())\n",
    "    n_states = len(states)\n",
    "    n_actions = len(policy[0].keys())\n",
    "    \n",
    "    ## Initialize both Q matricies\n",
    "    Q1 = np.zeros((n_states,n_actions))\n",
    "    Q2 = np.zeros((n_states,n_actions))\n",
    "    \n",
    "    for _ in range(episodes): # Loop over the episodes\n",
    "        terminal = False\n",
    "        ## Find the inital state, action index and reward\n",
    "        current_state, a_index, reward = start_episode(n_states,n_actions)\n",
    "        \n",
    "        while(not terminal): # Episode ends where get to terminal state   \n",
    "            ## Update the action values in Q1 or Q2 based on random choice\n",
    "            if(nr.uniform() <= 0.5):\n",
    "                Q1, s_prime, reward_prime, terminal, a_prime_index = update_double_Q(Q1, Q2, current_state, a_index, reward, alpha, gamma)\n",
    "            else:\n",
    "                Q2, s_prime, reward_prime, terminal, a_prime_index = update_double_Q(Q2, Q1, current_state, a_index, reward, alpha, gamma)\n",
    "            ## Set action, reward and state for next iteration\n",
    "            a_index = a_prime_index\n",
    "            current_state = s_prime\n",
    "            reward = reward_prime[a_prime_index]\n",
    "    return(Q1)\n",
    "\n",
    "Q = double_Q_learning_0(initial_policy, 1000)\n",
    "print_Q(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {'d': 0.25, 'l': 0.25, 'r': 0.25, 'u': 0.25},\n",
       " 1: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 2: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 3: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 4: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 5: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 6: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 7: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 8: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 9: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 10: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 11: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1},\n",
       " 12: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 13: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 14: {'d': 0.1, 'l': 0.1, 'r': 0.1, 'u': 0.7},\n",
       " 15: {'d': 0.1, 'l': 0.7, 'r': 0.1, 'u': 0.1}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_policy(initial_policy, Q, 0.1)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPI with Double Q Learning\n",
    "\n",
    "The code in the cell below applies general policy iteration using double Q-learning(0) to estimate the action values.  Additional details on this algorithm can be seen by reading the code comments.  \n",
    "\n",
    "Execute this code and examine the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'policy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-98b0ebd3ab98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mDouble_Q_0_Policy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdouble_Q_learning_0_GPI\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcycles\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgoal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.01\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mDouble_Q_0_Policy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'policy' is not defined"
     ]
    }
   ],
   "source": [
    "def double_Q_learning_0_GPI(policy, neighbors, reward, cycles, episodes, goal, alpha = 0.2, gamma = 0.9, epsilon = 0.1):\n",
    "    ## iterate over GPI cycles\n",
    "    current_policy = copy.deepcopy(policy)\n",
    "    for _ in range(cycles):\n",
    "        ## Evaluate policy with SARSA\n",
    "        Q = double_Q_learning_0(policy, neighbors, rewards, episodes = episodes, goal = goal)\n",
    "        \n",
    "        for s in list(current_policy.keys()): # iterate over all states\n",
    "            ## Find the index action with the largest Q values \n",
    "            ## May be more than one. \n",
    "            max_index = np.where(Q[:,s] == max(Q[:,s]))[0]\n",
    "            \n",
    "            ## Probabilities of transition\n",
    "            ## Need to allow for further exploration so don't let any \n",
    "            ## transition probability be 0.\n",
    "            ## Some gymnastics are required to ensure that the probabilities \n",
    "            ## over the transistions actual add to exactly 1.0\n",
    "            neighbors_len = float(Q.shape[0])\n",
    "            max_len = float(len(max_index))\n",
    "            diff = round(neighbors_len - max_len,3)\n",
    "            prob_for_policy = round(1.0/max_len,3)\n",
    "            adjust = round((epsilon * (diff)), 3)\n",
    "            prob_for_policy = prob_for_policy - adjust\n",
    "            if(diff != 0.0):\n",
    "                remainder = (1.0 - max_len * prob_for_policy)/diff\n",
    "            else:\n",
    "                remainder = epsilon\n",
    "                                                 \n",
    "            for i, key in enumerate(current_policy[s]): ## Update policy\n",
    "                if(i in max_index): current_policy[s][key] = prob_for_policy\n",
    "                else: current_policy[s][key] = remainder   \n",
    "                    \n",
    "    return(current_policy)                    \n",
    " \n",
    "\n",
    "Double_Q_0_Policy = double_Q_learning_0_GPI(policy, neighbors, rewards, cycles = 10, episodes = 500, goal = 0, alpha = 0.2, epsilon = 0.01)\n",
    "Double_Q_0_Policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(np.array(td_0_state_values(Double_Q_0_Policy, n_samps = 10000, goal = 0)).reshape((4,4)), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Step Off-Policy Learning with Importance Sampling\n",
    "\n",
    "For n-step off-policy learning we update a target policy $\\pi(A_t|S_t)$ using samples from a behavior policy $b(A_t|S_t)$. Since the two policies differ, the probabilities of an action given the state will undoubtedly differ. For example, the behavior policy can be exploratory whereas, the target policy is greedy. \n",
    "\n",
    "To account for the different probabilities of sampling we reweight by the **importance sampling ratio**. For an n-step algorithm at time step $t$ the importance sampling ratio can be expressed as:\n",
    "\n",
    "$$\\rho_{t:t + n -1} = \\prod_{k=\\tau}^{min(t + n -1,T-1)} \\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}$$\n",
    "\n",
    "The n-step TD update then becomes:\n",
    "\n",
    "$$V_{t+n}(S_t) = V_{t+n-1}(S_t) + \\alpha\\ \\rho_{t:t+n-1} \\big[ G_{t:t+n} - V_{t+n-1}(S_t) \\big],\\ 0 \\leq t < T]$$\n",
    "\n",
    "And the SARSA update becomes:\n",
    "\n",
    "$$Q_{t+n}(S_t, A_t) = Q_{t+n-1}(S_t, A_t) + \\alpha\\ \\rho_{t:t+n-1} \\big[ G_{t:t+n} - Q_{t+n-1}(S_t,A_t) \\big],\\ 0 \\leq t < T]$$\n",
    "\n",
    "For both of the above update equations consider the effect of importance sampling ratio. If the action given state is more likely under the target policy that the behavior policy, more weight is given to updating with the error term. However, If the action given state is less likely under the target policy that the behavior policy, less weight is given to updating with the error term. In this way, the weighting by the importance sampling ratio gives the correct updates for the target policy regardless of the transition probabilities of the behavior policy. \n",
    "\n",
    "> **NOte:** Considerably more detail on n-step off-policy RL algorithms can be found in Sutton and Barto, second edition, Sections 7.3, 7.4 and 7.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copyright 2018, Stephen F Elston. All rights reserved. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
