{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge Assignment\n",
    "## Cliff Walking with Reinforcement Learning\n",
    "\n",
    "## CSCI E-82A\n",
    "\n",
    ">**Make sure** you include your name along with the name of your team and team members in the notebook you submit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this challenge you will apply several reinforcement learning algorithms to a classic problem in reinforcement learning, known as the cliff walking problem. The cliff walking problem is basically a game. The goal is for the agent to find the highest reward (lowest cost) path from a starting state to the goal. \n",
    "\n",
    "There are a number of versions of the cliff walking problems which have been used as research benchmarks over the years. A typical cliff walking problem might use a grid of 4x12. For this challenge you will work with a reduced size grid world of 4x4 illustrated below to reduce training time for your models.   \n",
    "\n",
    "<img src=\"CliffWalking.JPG\" alt=\"Drawing\" style=\"width:200px; height:200px\"/>\n",
    "<center> **Grid World for similified cliff walking problem** </center>\n",
    "\n",
    "The goal is to find the highest reward path from the **starting state**, 12, to the **terminal state**, 15, making this an **episodic task**. The rewards for this task are:\n",
    "1. A reward of -1 for most state transitions. The -1 reward apples to state to state transitions and to transitions toward the boundary of the grid transitioning to the same state.    \n",
    "2. A reward of -100 for 'falling off the cliff'. Falling off the cliff occurs when entering states 13 or 14. The only possible transition out of the cliff states is back to the origin state, 12. There are no possible transitions toward the boundary from the cliff state. \n",
    "\n",
    "Intuitively, we can see that the optimal solution follows the dotted line path shown in the diagram above. The challenge is to find a path that is as close to this optimal as possible.   \n",
    "\n",
    "You can find a short discussion of the cliff walking problem on page 132 of Sutton and Barto, second edition. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge\n",
    "\n",
    "For this challenge you will do the following:\n",
    "\n",
    "1. Create a simulator for the grid world environment. All interactions between your agents and the environment must be though calls to the function you create.  \n",
    "2. Create and apply state value estimation agents using the RL algorithms.\n",
    "3. Use the general policy improvement (GPI) algorithm with the appropriate control algorithm to improve policy. \n",
    "4. Use the state value estimation agent to evaluate the improved policy. \n",
    "5. Compare the results for the various control algorithms you try. \n",
    "\n",
    "Methods to use to solve this problem:\n",
    "\n",
    "1. The Monte Carlo method for value estimation and (action value) control. The action value method for Monte Carlo has not been explicitly addressed in this course. You can find the pseudo code for Monte Carlo control on page 101 of Sutton and Barto, second edition.   \n",
    "2. Create and execute agents using the n-step TD method for value estimation and n-step SARSA (action value) for control.\n",
    "3. Create and execute agents using TD(0) for value estimation and SARSA(0) or Double Q-Learning (action value) control. You are welcome to try both algorithms if you have the time. \n",
    "4. For additional, but optional, challenge you may wish to try a dynamic programming algorithm. Does DP work for this problem or not, and why? \n",
    "\n",
    "> **Hints**\n",
    "> - For TD(0), n-step TD, n-step SARSA, SARSA(0) and Double Q-learning, you may need to change the reward to -10 for state transitions toward the boundary of the grid world.  \n",
    "> - For the n-step algorithms keep in mind that the grid world is rather small. \n",
    "> - Make sure you are not accidentally using two epsilon greedy steps in your GPI process.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
