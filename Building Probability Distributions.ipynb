{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 id=\"example-banana-shaped-distribution\">Example: Banana-shaped distribution</h2>\n",
    "\n",
    "<p>Consider the <em>banana-shaped distribution</em>, a commonly-used testbed for adaptive\n",
    "MCMC methods<sup class=\"footnote-ref\" id=\"fnref:haario1999adaptive\"><a href=\"#fn:haario1999adaptive\">2</a></sup>.\n",
    "Denote the density of this distribution as $p_{Y}(\\mathbf{y})$.\n",
    "To illustrate, 1k samples randomly drawn from this distribution are shown below:</p>\n",
    "\n",
    "<p><img src=\"images/banana_samples.svg\" alt=\"Banana distribution samples\" /></p>\n",
    "\n",
    "<p>The underlying process that generates samples\n",
    "$\\tilde{\\mathbf{y}} \\sim p_{Y}(\\mathbf{y})$ is simple to describe,\n",
    "and is of the general form,</p>\n",
    "\n",
    "<p>$$\n",
    "\\tilde{\\mathbf{y}} \\sim p_{Y}(\\mathbf{y}) \\quad\n",
    "\\Leftrightarrow \\quad\n",
    "\\tilde{\\mathbf{y}} = G(\\tilde{\\mathbf{x}}),\n",
    "\\quad \\tilde{\\mathbf{x}} \\sim p_{X}(\\mathbf{x}).\n",
    "$$</p>\n",
    "\n",
    "<p>In other words, a sample $\\tilde{\\mathbf{y}}$ is the output of a transformation\n",
    "$G$, given a sample $\\tilde{\\mathbf{x}}$ drawn from some underlying\n",
    "base distribution $p_{X}(\\mathbf{x})$.</p>\n",
    "\n",
    "<p>However, it is not as straightforward to compute an analytical expression for\n",
    "density $p_{Y}(\\mathbf{y})$.\n",
    "In fact, this is only possible if $G$ is a <em>differentiable</em> and <em>invertible</em>\n",
    "transformation (a <em>diffeomorphism</em><sup class=\"footnote-ref\" id=\"fnref:1\"><a href=\"#fn:1\">3</a></sup>), and if there is an analytical\n",
    "expression for $p_{X}(\\mathbf{x})$.</p>\n",
    "\n",
    "<p>Transformations that fail to satisfy these conditions (which includes something\n",
    "as simple as a multi-layer perceptron with non-linear activations) give rise to\n",
    "<em>implicit distributions</em>, and will be the subject of many posts to come.\n",
    "But for now, we will restrict our attention to diffeomorphisms.</p>\n",
    "\n",
    "<h3 id=\"base-distribution\">Base distribution</h3>\n",
    "\n",
    "<p>Following on with our example, the base distribution $p_{X}(\\mathbf{x})$ is\n",
    "given by a two-dimensional Gaussian with unit variances and covariance\n",
    "$\\rho = 0.95$:</p>\n",
    "\n",
    "<p>$$\n",
    "p_{X}(\\mathbf{x}) = \\mathcal{N}(\\mathbf{x} | \\mathbf{0}, \\mathbf{\\Sigma}),\n",
    "\\qquad\n",
    "\\mathbf{\\Sigma} =\n",
    "\\begin{bmatrix}\n",
    "  1    & 0.95 \\newline\n",
    "  0.95 & 1\n",
    "\\end{bmatrix}\n",
    "$$</p>\n",
    "\n",
    "<p>This can be encapsulated by an instance of\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/MultivariateNormalTriL\" target=\"_blank\">MultivariateNormalTriL</a>,\n",
    "which is parameterized by a lower-triangular matrix.\n",
    "First let&rsquo;s import TensorFlow Distributions:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import tensorflow.distributions as tfd\n",
    "import torch.distributions as dist\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Then we create the lower-triangular matrix and the instantiate the distribution:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.9500],\n",
       "        [0.9500, 1.0000]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rho = 0.95\n",
    "Sigma = torch.tensor([[1, rho],[rho, 1]])\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.0000],\n",
       "        [0.9500, 0.3122]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# p_x = tfd.MultivariateNormalTriL(scale_tril=tf.cholesky(Sigma))\n",
    "p_x = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>As with all subclasses of <code>tfd.Distribution</code>, we can evaluated the probability\n",
    "density function of this distribution by calling the <code>p_x.prob</code> method.\n",
    "Evaluating this on an uniformly-spaced grid yields the equiprobability contour\n",
    "plot below:</p>\n",
    "\n",
    "<p><img src=\"images/banana_base_density.svg\" alt=\"Base density\" /></p>\n",
    "\n",
    "<h3 id=\"forward-transformation\">Forward Transformation</h3>\n",
    "\n",
    "<p>The required transformation $G$ is defined as:</p>\n",
    "\n",
    "<p>$$\n",
    "G(\\mathbf{x}) =\n",
    "\\begin{bmatrix}\n",
    "  x_1 \\newline\n",
    "  x_2 - x_1^2 - 1 \\newline\n",
    "\\end{bmatrix}\n",
    "$$</p>\n",
    "\n",
    "<p>We implement this in the <code>_forward</code> function below<sup class=\"footnote-ref\" id=\"fnref:2\"><a href=\"#fn:2\">4</a></sup>:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _forward(x):\n",
    "    y_0 = x[..., 0:1]\n",
    "    y_1 = x[..., 1:2] - y_0**2 - 1\n",
    "    y_tail = x[..., 2:-1]\n",
    "    return tf.concat([y_0, y_1, y_tail], axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We can now use this to generate samples from $p_{Y}(\\mathbf{y})$.\n",
    "To do this we first sample from the base distribution $p_{X}(\\mathbf{x})$ by\n",
    "calling <code>p_x.sample</code>. For this illustration, we generate 1k samples, which is\n",
    "specified through the <code>sample_shape</code> argument. We then transform these samples\n",
    "through $G$ by calling <code>_forward</code> on them.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'sample'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-1faf7d5e231a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'sample'"
     ]
    }
   ],
   "source": [
    "x_samples = p_x.sample(1000)\n",
    "y_samples = _forward(x_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The figure below contains scatterplots of the 1k samples <code>x_samples</code> (left)\n",
    "and the transformed <code>y_samples</code> (right):</p>\n",
    "\n",
    "<p><img src=\"images/banana_base_samples.svg\" alt=\"Banana and base samples\" /></p>\n",
    "\n",
    "<h3 id=\"instantiating-a-transformeddistribution-with-a-bijector\">Instantiating a <code>TransformedDistribution</code> with a <code>Bijector</code></h3>\n",
    "\n",
    "<p>Having specified the forward transformation and the underlying distribution, we\n",
    "have now fully described the sample generation process, which is the bare\n",
    "minimum necessary to define a probability distribution.</p>\n",
    "\n",
    "<p>The forward transformation is also the <em>first</em> of <strong>three</strong> operations needed to\n",
    "fully specify a <code>Bijector</code>, which can be used to instantiate a\n",
    "<code>TransformedDistribution</code> that encapsulates the banana-shaped distribution.</p>\n",
    "\n",
    "<h4 id=\"creating-a-bijector\">Creating a <code>Bijector</code></h4>\n",
    "\n",
    "<p>First, let&rsquo;s subclass <code>Bijector</code> to define the <code>Banana</code> bijector and implement\n",
    "the forward transformation as an instance method:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"probability-density-function\">Probability Density Function</h3>\n",
    "\n",
    "<p>Although we can now sample from this distribution, we have yet to define the\n",
    "operations necessary to evaluate its probability density function&mdash;the\n",
    "remaining <em>two</em> of <strong>three</strong> operations needed to fully specify a <code>Bijector</code></p>\n",
    "\n",
    "<p>Indeed, calling <code>p_y.prob</code> at this stage would simply raise a\n",
    "<code>NotImplementedError</code> exception. So what else do we need to define?</p>\n",
    "\n",
    "<p>Recall the probability density of $p_{Y}(\\mathbf{y})$ is given by:</p>\n",
    "\n",
    "<p>$$\n",
    "p_{Y}(\\mathbf{y}) = p_{X}(G^{-1}(\\mathbf{y})) \\mathrm{det}\n",
    "\\left ( \\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y}) \\right )\n",
    "$$</p>\n",
    "\n",
    "<p>Hence we need to specify the inverse transformation $G^{-1}(\\mathbf{y})$ and its\n",
    "Jacobian determinant\n",
    "$\\mathrm{det} \\left ( \\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y}) \\right )$.</p>\n",
    "\n",
    "<p>For numerical stability, the <code>Bijector</code> API requires that this be defined in\n",
    "log-space. Hence, it is useful to recall that the forward and inverse log\n",
    "determinant Jacobians differ only in their signs<sup class=\"footnote-ref\" id=\"fnref:3\"><a href=\"#fn:3\">5</a></sup>,</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  \\log \\mathrm{det} \\left ( \\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y}) \\right )\n",
    "  &amp; = - \\log \\mathrm{det} \\left ( \\frac{\\partial}{\\partial\\mathbf{x}} G(\\mathbf{x}) \\right ),\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>which gives us the option of implementing either (or both).\n",
    "However, do note the following from the official\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/contrib/distributions/bijectors/Bijector\" target=\"_blank\">tf.contrib.distributions.bijectors.Bijector</a> API docs:</p>\n",
    "\n",
    "<blockquote>\n",
    "<p>Generally its preferable to directly implement the inverse Jacobian\n",
    "determinant. This should have superior numerical stability and will often share\n",
    "subgraphs with the <code>_inverse</code> implementation.</p>\n",
    "</blockquote>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"inverse-transformation\">Inverse Transformation</h3>\n",
    "\n",
    "<p>So let&rsquo;s implement the inverse transform $G^{-1}$, which is given by:</p>\n",
    "\n",
    "<p>$$\n",
    "G^{-1}(\\mathbf{y}) =\n",
    "\\begin{bmatrix}\n",
    "  y_1 \\newline\n",
    "  y_2 + y_1^2 + 1 \\newline\n",
    "\\end{bmatrix}\n",
    "$$</p>\n",
    "\n",
    "<p>We define this in the <code>_inverse</code> function below:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">def _inverse(y):\n",
    "\n",
    "    x_0 = y[..., 0:1]\n",
    "    x_1 = y[..., 1:2] + x_0**2 + 1\n",
    "    x_tail = y[..., 2:-1]\n",
    "\n",
    "    return tf.concat([x_0, x_1, x_tail], axis=-1)\n",
    "</code></pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"jacobian-determinant\">Jacobian determinant</h3>\n",
    "\n",
    "<p>Now we compute the log determinant of the Jacobian of the <em>inverse</em>\n",
    "transformation.\n",
    "In this simple example, the transformation is <em>volume-preserving</em>, meaning its\n",
    "Jacobian determinant is equal to 1.</p>\n",
    "\n",
    "<p>This is easy to verify:</p>\n",
    "\n",
    "<p>$$\n",
    "\\begin{align}\n",
    "  \\mathrm{det} \\left ( \\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y}) \\right )\n",
    "  &amp; = \\mathrm{det}\n",
    "  \\begin{pmatrix}\n",
    "    \\frac{\\partial}{\\partial y_1} y_1             &amp; \\frac{\\partial}{\\partial y_2} y_1 \\newline\n",
    "    \\frac{\\partial}{\\partial y_1} y_2 + y_1^2 + 1 &amp; \\frac{\\partial}{\\partial y_2} y_2 + y_1^2 + 1 \\newline\n",
    "  \\end{pmatrix} \\newline\n",
    "  &amp; = \\mathrm{det}\n",
    "  \\begin{pmatrix}\n",
    "    1     &amp; 0 \\newline\n",
    "    2 y_1 &amp; 1 \\newline\n",
    "  \\end{pmatrix}\n",
    "  = 1\n",
    "\\end{align}\n",
    "$$</p>\n",
    "\n",
    "<p>Hence, the log determinant Jacobian is given by zeros shaped like input <code>y</code>, up\n",
    "to the last <code>inverse_min_event_ndims=1</code> dimensions:</p>\n",
    "\n",
    "<pre><code class=\"language-python\">def _inverse_log_det_jacobian(y):\n",
    "\n",
    "    return tf.zeros(shape=y.shape[:-1])\n",
    "</code></pre>\n",
    "\n",
    "<p>Since the log determinant Jacobian is constant, i.e. independent of the input,\n",
    "we can just specify it for one input by setting the flag <code>is_constant_jacobian=True</code><sup class=\"footnote-ref\" id=\"fnref:4\"><a href=\"#fn:4\">6</a></sup>,\n",
    "and the <code>Bijector</code> class will handle the necessary shape inference for us.</p>\n",
    "\n",
    "<p>Putting it all together in the <code>Banana</code> bijector subclass, we have:</p>\n",
    "\n",
    "\n",
    "<p>Finally, we can instantiate distribution <code>p_y</code> by calling\n",
    "<code>tfd.TransformedDistribution</code> as we did before <em>et voil√†</em>,\n",
    "we can now simply call <code>p_y.prob</code> to evaluate the probability density function.</p>\n",
    "\n",
    "<p>Evaluating this on the same uniformly-spaced grid as before yields the following\n",
    "equiprobability contour plot:</p>\n",
    "\n",
    "<p><img src=\"images/banana_density.svg\" alt=\"Banana density\" /></p>\n",
    "\n",
    "<h1 id=\"summary\">Summary</h1>\n",
    "\n",
    "<p>In this post, we showed that using diffeomorphisms&mdash;mappings that are\n",
    "differentiable and invertible, it is possible transform standard distributions\n",
    "into interesting and complicated distributions, while still being able to\n",
    "compute their densities analytically.</p>\n",
    "\n",
    "<p>The <code>Bijector</code> API provides an interface that encapsulates the basic properties\n",
    "of a diffeomorphism needed to transform a distribution. These are: the\n",
    "forward transform itself, its inverse and the determinant of their Jacobians.</p>\n",
    "\n",
    "<p>Using this, <code>TransformedDistribution</code> <em>automatically</em> implements perhaps the two\n",
    "most important methods of a probability distribution: sampling (<code>sample</code>), and\n",
    "density evaluation (<code>prob</code>).</p>\n",
    "\n",
    "<p>Needless to say, this is a very powerful combination.\n",
    "Through the <code>Bijector</code> API, the number of possible distributions that can be\n",
    "implemented and used directly with other functionalities in the TensorFlow\n",
    "Probability ecosystem effectively becomes <em>endless</em>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
