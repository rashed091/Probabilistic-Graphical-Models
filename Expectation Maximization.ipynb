{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>However, we may not know the real labels for each data point in reality. This, naturally, introduces a latent (a.k.a. hiddent/unobserved) variable called $\\mathbf{z} = (z^{(1)}, …, z^{(m)})$, which is multinomial: $z^{(i)}$ indicates which distribution a specific point $x^{(i)}$ belongs to. In the example above, $z^{(i)}$ actually follows a Bernoulli, since there are only two possible outcomes.</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.47203487,  0.33580853,  0.44779122,  1.21412305, -0.6821565 ,\n",
       "       -1.17189709,  0.91835292,  0.66627024,  1.17646205,  1.9994951 ])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = 100\n",
    "mu_arr = np.array([1, 10])\n",
    "sigma_arr = np.array([1, 1])\n",
    "x = np.append(np.random.normal(mu_arr[0], sigma_arr[0], N), \n",
    "              np.random.normal(mu_arr[1], sigma_arr[1], N))\n",
    "x[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x11ef37390>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "ax.scatter(x[:N], np.zeros(N), c='green', marker=2, s=150)\n",
    "ax.scatter(x[N:], np.zeros(N), c='orange', marker='x', s=150)\n",
    "ax.set_ylim([-1e-4, 1e-4])\n",
    "_ = ax.set_yticks([])\n",
    "sns.distplot(x[:N], color='green')\n",
    "sns.distplot(x[N:], color='orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this case, we have a model: $p(\\mathbf{x},\\mathbf{z}; \\Theta)$, where only $\\mathbf{x}$ is observed. Our goal is to maximize $L(\\Theta)=\\prod_{i}p(x^{(i)};\\Theta)$. It is common to instead maximize the <strong><em>log likelihood</em></strong>: $\\ell(\\Theta)=\\sum_{i}ln~p(x^{(i)}; \\Theta)$. This is also called the <strong><em>incompelte data log likelihood</em></strong> because we do not know the latent variable $\\mathbf{z}$ that indicate each data point’s memebrship (to density which a data point belongs).</p> <p>To gain more insights on why this problem is diffcult, we decompose the log likelihood:</p> \n",
    "\n",
    "<div>\\begin{align} \\ell(\\Theta) &= \\sum_{i}ln~p(x^{(i)}; \\Theta) \\\\ &= \\sum_{i} ln \\sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \\Theta) \\end{align}</div> \n",
    "\n",
    "<p>Note that there is summation over $z^{(i)}$ inside logarithm. Even if individual joint probability distributions are in the exponential families, the summation still makes the derivative intractable.</p> <hr /> \n",
    "\n",
    "<p>An example of a latent variable model is the Latent Dirichlet Allocation<sup id=\"fnref-1\"><a class=\"footnote-ref\" href=\"#fn-1\">1</a></sup> (LDA) model for uncovering latent topics in documents of text. Once finished deriving the general EM equations, we'll (begin to) apply them to this model.</p>\n",
    "<h2>Why not maximum likelihood estimation?</h2>\n",
    "<p>As the adage goes, computing the MLE with respect to this marginal is \"hard.\" I loosely understand why. In any case, Bishop<sup id=\"fnref-2\"><a class=\"footnote-ref\" href=\"#fn-2\">2</a></sup> states:</p>\n",
    "<blockquote>\n",
    "<p>A key observation is that the summation over the latent variables appears inside the logarithm. Even if the joint distribution <span class=\"math\">\\(p(\\mathbf{X, Z}\\vert\\theta)\\)</span> belongs to the exponential family, the marginal distribution <span class=\"math\">\\(p(\\mathbf{X}\\vert\\theta)\\)</span> typically does not as a result of this summation. The presence of the sum prevents the logarithm from acting directly on the joint distribution, resulting in complicated expressions for the maximum likelihood solution.</p>\n",
    "</blockquote>\n",
    "<p><strong>We'll want something else to maximize instead.</strong></p> <hr />\n",
    "\n",
    "<h3 id=\"jensen-inequality\">Jensen Inequality</h3> <p>Before we talk about how EM algorithm can help us solve the intractability, we need to introduce Jensen inequality. This will be used later to construct a (tight) lower bound of the log likelihood.</p> <p>Definition:</p> \n",
    "\n",
    "<blockquote> <p>Let function $f$ be a convex function (e.g., $f’’ \\geq 0$). Let $x$ be a random variable. Then we have $f(E[x]) \\leq E[f(x)]$.</p> <p>Further, $f$ is <strong><em>strictly convex</em></strong> if $f’’ \\gt; 0$. In such case, $f(E[x]) = E[f(x)]$ i.f.f. $x=E[x]$</p> <p>On the other hand, $f$ is concave if $f’’ \\leq 0$, and we have $f(E[x]) \\geq E[f(x)]$.</p> </blockquote> \n",
    "\n",
    "<p>A useful example (that will be applied in EM algorithm) is $f(x) = ln~x$ is <strong><em>strictly concave</em></strong> for $x \\gt 0$. Proof:</p> <div>\\begin{align} f''(x) = \\frac{d~}{dx} f'(x) = \\frac{d~\\frac{1}{x}}{dx} = -\\frac{1}{x^2} \\lt 0 \\end{align}</div> <p>Therefore, we have $ln~E[x] \\geq E[ln~x]$</p> <p>Let’s go with a concrete example by plotting $f(x) = ln~x$</p> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.LineCollection at 0x11f0fd400>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(.1, 5, 100)\n",
    "y = np.log(x)\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "ax.scatter(x, y, color='green', marker='+')\n",
    "## 1 (p=0.5)\n",
    "ax.vlines(x=1, ymin=-2.5, ymax=0, linestyle='--', color='green')\n",
    "ax.hlines(y=0, xmin=-.1, xmax=1, linestyle='--', color='green')\n",
    "ax.text(x=1.1, y=np.log(.7), s='$f(1)=0$',\n",
    "        fontdict={'size': 13})\n",
    "## 4 (p=0.5)\n",
    "ax.vlines(x=4, ymin=-2.5, ymax=np.log(4), linestyle='--', color='green')\n",
    "ax.hlines(y=np.log(4), xmin=-.1, xmax=4, linestyle='--', color='green')\n",
    "ax.text(x=4.1, y=np.log(3.1), \n",
    "        s='$f(4)=%.2f$'%(np.log(4)),\n",
    "        fontdict={'size': 13}\n",
    "       )\n",
    "ax.plot([1, 4], np.log([1, 4]), color='green', linestyle='dotted')\n",
    "# E(x) = (1+4)/2 = 2.5\n",
    "ax.vlines(x=2.5, ymin=-2.5, ymax=np.log(2.5), linestyle='--', color='green')\n",
    "ax.hlines(y=np.log(2.5), xmin=-.1, xmax=2.5, linestyle='--', color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 id=\"em-algorithm-formalization\">EM Algorithm Formalization</h3> \n",
    "\n",
    "<h4 id=\"derivation\">Derivation</h4> <p>Now we can formarlize the problem, starting from the log likelihood from Section 1:</p> \n",
    "\n",
    "<div>\\begin{align} \\ell(\\Theta) & = \\sum_i ln~\\sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \\Theta) \\\\ & = \\sum_i ln~\\sum_{z^{(i)}} q_i(z^{(i)}) \\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{q_i(z^{(i)})} \\end{align}</div> <p>where $q_i(z^{(i)})$ is any arbitrary probability distribution for $z^{(i)}$ and thus:$q_i(z^{(i)}) \\geq 0; \\sum_{z^{(i)}}~q_i(z^{(i)}) = 1$.</p> <p>It is interesting (actually important) to note that the summation inside the logarithm, $\\sum_{z^{(i)}} q_i(z^{(i)}) \\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{q_i(z^{(i)})}$, takes the form of expectation of $\\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{q_i(z^{(i)})}$, given $x^{(i)}$. Recall that $E[f(x)] = \\sum_i p(x^{(i)})f(x^{(i)})$, where $p(x)$ is the probability mass function for $x^{(i)}$. Therefore, we can write the above into:</p> <div>\\begin{equation} \\ell(\\Theta) = \\sum_i ln~E_{z^{(i)} \\sim q_i(z^{(i)})}\\Big[ \\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{q_i(z^{(i)})} \\Big] \\end{equation}</div> <p>Now, it is not difficult to see that we can apply Jensen Inequality on the term within the summation over $i$:</p> <div> \\begin{align} \\ell(\\Theta) & = \\sum_i ln~E_{z^{(i)} \\sim q_i(z^{(i)})}\\Big[ \\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{q_i(z^{(i)})} \\Big] \\\\ & \\geq \\sum_i E_{z^{(i)} \\sim q_i(z^{(i)})} \\Big[ ln~\\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{q_i(z^{(i)})} \\Big] \\\\ & = \\sum_i \\sum_{z^{(i)}} q_i(z^{(i)})ln~\\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{q_i(z^{(i)})} \\end{align} </div> <p>In this way, we successfully construct the lower bound for $\\ell(\\Theta)$. However, this is not enough. Note that we want to squeeze the lower bound as much as we can to obtain the <strong><em>equality</em></strong>. Based on Section 2, we need</p> \n",
    "\n",
    "$$\\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{q_i(z^{(i)})} = c$$ \n",
    "\n",
    "<p>where $c$ is some constant. And this leads to</p> \n",
    "\n",
    "\n",
    "$$q_i(z^{(i)}) \\propto p(x^{(i)}, z^{(i)}; \\Theta)~~ s.t. \\sum_{z^{(i)}} q_i(z^{(i)}) = 1$$\n",
    "\n",
    "\n",
    "<p>From the formula above, we can determine the choice of $q$</p> <div> \\begin{align} q_i(z^{(i)}) & = \\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{\\sum_z p(x^{(i)}, z; \\Theta)} \\\\ & = \\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{p(x^{(i)}; \\Theta)} \\\\ & = p(z^{(i)}\\vert x^{(i)}; \\Theta) \\end{align} </div> <p>The last term is actually the posterior distribution of $z^{(i)}$ (soft membership) given the observation $x^{(i)}$ and the paramter $\\Theta$</p> <h4 id=\"algorithm-operationalization\">Algorithm Operationalization</h4> <p>EM is an iterative algorithm that consists of two steps:</p> <ul> <li>E step: Let $q_i(z^{(i)}) = p(z^{(i)}\\vert x^{(i)}; \\Theta)$. The gives a tight lower bound for $\\ell(\\Theta)$. This is actually maximizing the expectation shown above.</li> <li>M step: Update parameters to maximize the lower bound $\\Theta := argmax_{\\Theta} \\sum_i \\sum_{z^{(i)}} q_i(z^{(i)})ln~\\frac{p(x^{(i)}, z^{(i)}; \\Theta)}{q_i(z^{(i)})}$</li> </ul> \n",
    "\n",
    "\n",
    "<h4 id=\"convergence\">Convergence</h4> \n",
    "\n",
    "<p>To prove the convergence of EM algorithm, we can prove the fact that $\\ell(\\Theta^{(t+1)}) \\geq \\ell(\\Theta^{(t)})$ for any $t$, where $\\Theta^{(t)}$ is the parameter estimatations at iteration $t$.</p> <p>Recall that we choose $q_i(z^{(i)}) = p(z^{(i)}\\vert x^{(i)}; \\Theta)$ so that Jensen inequality will become <strong><em>equality</em></strong> such that:</p> <div> $$\\ell(\\Theta^{(t)})= \\sum_i \\sum_{z^{(i)}} q_i^{(t)}(z^{(i)})ln~\\frac{p(x^{(i)}, z^{(i)}; \\Theta^{(t)})}{q_i^{(t)}(z^{(i)})}$$ </div> <p>In the M step, we then update our parameter to be $\\Theta^{(t+1)}$ that maximizes the R.H.S. of the equation above:</p> <div>\\begin{align} \\ell(\\Theta^{(t+1)}) & = \\sum_i \\sum_{z^{(i)}} q_i^{(t+1)}(z^{(i)})ln~\\frac{p(x^{(i)}, z^{(i)}; \\Theta^{(t+1)})}{q_i^{(t+1)}(z^{(i)})} \\\\ & \\geq \\sum_i \\sum_{z^{(i)}} q_i^{(t)}(z^{(i)})ln~\\frac{p(x^{(i)}, z^{(i)}; \\Theta^{(t+1)})}{q_i^{(t)}(z^{(i)})} \\\\ & \\geq \\sum_i \\sum_{z^{(i)}} q_i^{(t)}(z^{(i)})ln~\\frac{p(x^{(i)}, z^{(i)}; \\Theta^{(t)})}{q_i^{(t)}(z^{(i)})} \\\\ & = \\ell(\\Theta^{(t)}) \\end{align}</div> <p>Therefore, EM algorithm will make the log likelihood change monotonically. A common practice to monitor the convergence is to test if the difference of log likelihoods at two success iterations is less than some predefined tolerance parameter.</p> <hr /> \n",
    "\n",
    "\n",
    "<h3 id=\"towards-deeper-understanding-of-em-evidence-lower-bound-elbo\">Towards deeper understanding of EM: Evidence Lower Bound (ELBO)</h3> \n",
    "\n",
    "<p>We just derived the formulation of EM algorithm in details. However, one thing we did not do is to explicitly write out the decomposition of $\\ell(\\Theta)$. In fact, we can mathematically write out the log likelihood into the sum of two terms:</p> <ul> <li>$KL(q\\vert \\vert p) = \\sum_{z} q(z) ln~ \\frac{q(z)}{p(z \\vert \\mathbf{x}; \\Theta)}$</li> <li>$\\mathcal{L}(\\mathbf{x}; \\Theta) = \\sum_{z} q(z) ln~ \\frac{p(\\mathbf{x}, z; \\Theta)}{q(z)}$</li> </ul> \n",
    "\n",
    "\n",
    "<h4 id=\"derivation-1\">Derivation</h4> <div>\\begin{align} ln~p(\\mathbf{x}; \\Theta) & = \\sum_{z} q(z) ln p(\\mathbf{x}; \\Theta) \\\\ & [\\text{Recall that } \\sum_z q(z) = 1] \\\\ & = \\sum_{z} q(z) ln~ \\frac{p(\\mathbf{x}, z; \\Theta)}{p(z \\vert \\mathbf{x}; \\Theta)} \\\\ & = \\sum_{z} q(z) ln~ \\frac{p(\\mathbf{x}, z; \\Theta)~q(z)}{p(z \\vert \\mathbf{x}; \\Theta) ~q(z)} \\\\ & = \\sum_{z} q(z) ln~ \\frac{p(\\mathbf{x}, z; \\Theta)}{q(z)} + \\sum_{z} q(z) ln~ \\frac{q(z)}{p(z \\vert \\mathbf{x}; \\Theta)}\\\\ & = \\mathcal{L}(\\mathbf{x}; \\Theta) + KL(q\\vert \\vert p) \\end{align}</div> <h4 id=\"elbo\">ELBO</h4> <p>Here, we define evidence lower bound to be $ELBO = \\mathcal{L}(\\mathbf{x}; \\Theta)$, because KL-divergence is non-negative. Specifically, the equality only holds when $q(z) = p(z \\vert \\mathbf{x}; \\Theta)$. Therefore, $ELBO$ can be seen as a <strong><em>tight</em></strong> lower bound of the evidence (incomplete data likelihood):</p> \n",
    "\n",
    "\n",
    "$$ln~p(\\mathbf{x}; \\Theta) \\geq ELBO$$\n",
    "\n",
    "\n",
    "<p>Such result is actually consistent with the previous derivation where we directly apply Jensen’s inequality.</p> <hr /> <h3 id=\"applying-em-on-gaussian-mixtures\">Applying EM on Gaussian Mixtures</h3> <p>In this section, we will use an example of Gaussian Mixture to demonstrate the application of EM algorithm.</p> <p>Suppose we have some data $\\mathbf{x}={x^{(1)}, …, x^{(m)}}$, which some from $K$ different Gaussian distributions ($K$ mixtures). We will use the following notations:</p> <ul> <li>$\\mu_k$: the mean of the $k^{th}$ Gaussian component</li> <li>$\\Sigma_k$: the covariance matrix of the $k^{th}$ Gaussian component</li> <li>$\\phi_k$: the multinomial parameter of a specific datapoint belonging to the $k^{th}$ componenet.</li> <li>$z^{(i)}$: the latent variable (multinomial) for each $x^{(i)}$</li> </ul> <p>We also assume that the dimension of each $x^{(i)}$ is $n$.</p> <p>The goal is: $max_{\\mu, \\Sigma, \\phi}~ln~p(\\mathbf{x};\\mu, \\Sigma, \\phi)$. Therefore this follows exactly the EM framework.</p> \n",
    "\n",
    "\n",
    "<h4 id=\"e-step\">E step</h4> <p>We set $w_j^{(i)} = q_i(z^{(i)}=j) = p(z^{(i)}=j \\vert x^{(i)}; \\mu, \\Sigma, \\phi)$.</p> \n",
    "\n",
    "\n",
    "<h4 id=\"m-step\">M step</h4> <p>We will write down the lower bound and get derivatives for each of the three parameters.</p> \n",
    "\n",
    "\n",
    "<div>\\begin{align} \\sum_{i}^{m} \\sum_{j}^{K} & q_i(z^{(i)}=j) ln~\\frac{p(x^{(i)}, z^{(i)}=j; \\mu, \\Sigma, \\phi)}{q_i(z^{(i)}=j)} \\\\ &= \\sum_{i}^{m} \\sum_{j}^{K} q_i(z^{(i)}=j) ln~\\frac{p(x^{(i)}\\vert z^{(i)}=j; \\mu, \\Sigma) p(z^{(i)}=j; \\phi)}{q_i(z^{(i)}=j)} \\end{align}</div> \n",
    "\n",
    "<p>Note that:</p> <ul> <li>$x^{(i)} \\vert z^{(i)}=j; \\mu, \\Sigma \\sim \\mathcal{N}(\\mu_j, \\Sigma_j)$</li> <li>$z^{(i)}=j; \\phi \\sim Multi(\\phi)$</li> </ul> \n",
    "\n",
    "\n",
    "<p>We can then leverage these probability distributions and continue</p> \n",
    "\n",
    "<div>\\begin{align} ll := \\sum_{i}^{m} \\sum_{j}^{K} w_j^{(i)} ln~\\frac{\\frac{1}{\\sqrt{(2\\pi)^{n}\\vert\\Sigma_j\\vert}}~exp\\Big(-\\frac{1}{2}(x^{(i)}-\\mu_j)^{T}\\Sigma_j^{-1}(x^{(i)}-\\mu_j)\\Big)~\\phi_j}{w_j^{(i)}} \\end{align}</div> \n",
    "\n",
    "\n",
    "<p>Now, we need to maximize this lower bound for each of the three parameters. Many of the derivative on vector/matrix are based on <a href=\"http://www.math.uwaterloo.ca/~hwolkowi//matrixcookbook.pdf\">Matrix Cookbook</a></p> \n",
    "\n",
    "\n",
    "<h5 id=\"derivative-of-mu_j\">Derivative of $\\mu_j$</h5> \n",
    "\n",
    "<div>\\begin{align} \\nabla_{\\mu_j} ll & = \\nabla_{\\mu_j} \\sum_{i}^{m} w_j^{(i)} ln~\\frac{\\frac{1}{\\sqrt{(2\\pi)^{n}\\vert\\Sigma_j\\vert}}~exp\\Big(-\\frac{1}{2}(x^{(i)}-\\mu_j)^{T}\\Sigma_j^{-1}(x^{(i)}-\\mu_j)\\Big)~\\phi_j}{w_j^{(i)}} \\\\ & = \\nabla_{\\mu_j} \\sum_{i}^{m} w_j^{(i)} \\Big[ ln~\\frac{\\frac{1}{\\sqrt{(2\\pi)^{n}\\vert\\Sigma_j\\vert}}~\\phi_j}{w_j^{(i)}} + ln~exp\\Big(-\\frac{1}{2}(x^{(i)}-\\mu_j)^{T}\\Sigma_j^{-1}(x^{(i)}-\\mu_j)\\Big) \\Big] \\\\ & = \\nabla_{\\mu_j} \\sum_{i}^{m} w_j^{(i)} \\Big[ \\frac{1}{2}(x^{(i)}-\\mu_j)^{T}\\Sigma_j^{-1}(x^{(i)}-\\mu_j) \\Big] \\\\ & = -\\frac{1}{2} \\sum_{i}^{m} w_j^{(i)} \\nabla_{\\mu_j}\\Big[ (x^{(i)}-\\mu_j)^{T}\\Sigma_j^{-1}(x^{(i)}-\\mu_j) \\Big] \\\\ & \\big[ \\text{For } f(x) = x^T A x \\text{: } \\nabla_{x} f(x) = (A+A^T)x\\big] \\\\ & = \\frac{1}{2} \\sum_{i}^{m} w_j^{(i)} \\nabla_{(x^{i} - \\mu_j)}\\Big[ (x^{(i)}-\\mu_j)^{T}\\Sigma_j^{-1}(x^{(i)}-\\mu_j) \\Big] \\\\ & = \\frac{1}{2} \\sum_{i}^{m} w_j^{(i)} \\Big[ (\\Sigma_j^{-1} + (\\Sigma_j^{-1})^T) (x^{(i)} - \\mu_j) \\Big] \\\\ & \\big[ \\text{Note that } \\Sigma_j \\text{ is symmetric so we have } (\\Sigma_j^{-1})^T = (\\Sigma_j^{T})^{-1} =\\Sigma_j^{-1} \\big] \\\\ & = \\sum_{i}^{m} w_j^{(i)} \\Big[ \\Sigma_j^{-1} (x^{(i)} - \\mu_j) \\Big] \\\\ \\end{align}</div> \n",
    "\n",
    "\n",
    "<p>Set the last term zero, we have</p> \n",
    "\n",
    "\n",
    "<div>\\begin{align} \\nabla_{\\mu_j} ll & = 0 \\\\ \\sum_{i}^{m} w_j^{(i)} \\Big[ \\Sigma_j^{-1} (x^{(i)} - \\mu_j) \\Big] & = 0 \\\\ \\sum_{i}^{m} w_j^{(i)}(x^{(i)} - \\mu_j) & = 0 \\\\ \\sum_{i}^{m} w_j^{(i)} x^{(i)} & = \\sum_{i}^{m} w_j^{(i)} \\mu_j \\\\ \\mu_j & = \\frac{\\sum_{i}^{m} w_j^{(i)} x^{(i)}}{\\sum_{i}^{m} w_j^{(i)}} \\end{align}</div> \n",
    "\n",
    "\n",
    "<h5 id=\"derivative-of-sigma_j\">Derivative of $\\Sigma_j$</h5> \n",
    "\n",
    "<div>\\begin{align} \\nabla_{\\Sigma_j} ll & = \\nabla_{\\Sigma_j} \\sum_{i}^{m} w_j^{(i)} ln~\\frac{\\frac{1}{\\sqrt{(2\\pi)^{n}\\vert\\Sigma_j\\vert}}~exp\\Big(-\\frac{1}{2}(x^{(i)}-\\mu_j)^{T}\\Sigma_j^{-1}(x^{(i)}-\\mu_j)\\Big)~\\phi_j}{w_j^{(i)}} \\\\ & = \\sum_{i}^{m} w_j^{(i)} \\nabla_{\\Sigma_j} \\Big[ ln~\\frac{1}{\\sqrt{\\vert\\Sigma_j\\vert}} - \\frac{1}{2}(x^{(i)}-\\mu_j)^{T}\\Sigma_j^{-1}(x^{(i)}-\\mu_j) \\Big] \\\\ & = -\\frac{1}{2} \\sum_{i}^{m} w_j^{(i)} \\Big[ \\frac{\\partial~ln~\\vert\\Sigma_j\\vert}{\\partial \\Sigma_j} + \\frac{\\partial}{\\partial \\Sigma_j} (x^{(i)}-\\mu_j)^{T}\\Sigma_j^{-1}(x^{(i)}-\\mu_j) \\Big] \\\\ \\end{align}</div> \n",
    "\n",
    "<p>First, we consider the derivative of the first term in the square bracket:</p> \n",
    "\n",
    "\n",
    "<div>\\begin{align} \\frac{\\partial ln~\\vert\\Sigma_j\\vert}{\\partial \\Sigma_j} & = \\frac{1}{\\vert\\Sigma_j\\vert} \\frac{\\partial \\vert\\Sigma_j\\vert}{\\partial \\Sigma_j} \\\\ & = \\frac{1}{\\vert\\Sigma_j\\vert}\\vert\\Sigma_j\\vert (\\Sigma_j^{-1})^T \\\\ & = \\Sigma_j^{-1} \\end{align}</div> \n",
    "\n",
    "<p>Then, we do the second term</p> \n",
    "\n",
    "\n",
    "<div>\\begin{align} \\frac{\\partial}{\\partial \\Sigma_j} (x^{(i)}-\\mu_j)^{T}\\Sigma_j^{-1}(x^{(i)}-\\mu_j) & = -\\Sigma_j^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^{T} \\Sigma_j^{-1} \\\\ \\end{align}</div> \n",
    "\n",
    "\n",
    "<p>Combined these results back and set it to zero, we have:</p> \n",
    "\n",
    "\n",
    "<div>\\begin{align} \\nabla_{\\Sigma_j} ll & = -\\frac{1}{2} \\sum_i^{m} w_j^{(i)} \\Big[ \\Sigma_j^{-1} - \\Sigma_j^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^{T} \\Sigma_j^{-1} \\Big]\\\\ & = -\\frac{1}{2} \\sum_i^{m} w_j^{(i)} \\Big[ I - \\Sigma_j^{-1}(x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^{T} \\Big] \\Sigma_j^{-1} \\stackrel{set}{=} 0 \\\\ \\end{align}</div> \n",
    "\n",
    "<p>Rearrange the equation and we have:</p> \n",
    "\n",
    "<div>\\begin{align} \\sum_i^{m} w_j^{(i)} \\Big[ \\Sigma_j - (x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^{T} \\Big] &= 0 \\\\ \\sum_i^{m} w_j^{(i)} \\Sigma_j &= \\sum_i^{m} w_j^{(i)} (x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^{T} \\\\ \\Sigma_j & = \\frac{\\sum_i^{m} w_j^{(i)} (x^{(i)}-\\mu_j)(x^{(i)}-\\mu_j)^{T}}{\\sum_i^{m} w_j^{(i)}} \\end{align}</div> \n",
    "\n",
    "\n",
    "<h5 id=\"derivative-of-phi_j\">Derivative of $\\phi_j$</h5> \n",
    "\n",
    "<p>This is relatively simpler but we need to apply Lagrange multipliers because $\\sum_j \\phi_j = 1$.</p> \n",
    "\n",
    "<div>\\begin{align} ll & = \\sum_{i}^{m} \\sum_{l}^{k} w_l^{(i)} ln~\\frac{\\frac{1}{\\sqrt{(2\\pi)^{n}\\vert\\Sigma_l\\vert}}~exp\\Big(-\\frac{1}{2}(x^{(i)}-\\mu_l)^{T}\\Sigma_l^{-1}(x^{(i)}-\\mu_l)\\Big)~\\phi_l}{w_l^{(i)}} \\\\ & = \\sum_{i}^{m} \\sum_{l}^{k} w_l^{(i)} ln~\\phi_l \\\\ \\end{align}</div> \n",
    "\n",
    "\n",
    "<p>We need to construct Lagrangian, with $\\lambda$ as the Lagrange multiplier:</p> \n",
    "\n",
    "$$\\mathcal{L}(\\phi) = ll + \\lambda (\\sum_l^{k} \\phi_l - 1)$$\n",
    "\n",
    "\n",
    "<p>We will take derivative on $\\mathcal{L}$ and set it to zero:</p> \n",
    "\n",
    "<div>\\begin{align} \\frac{\\partial \\mathcal{L}(\\phi)}{\\partial \\phi_j} & = \\frac{\\partial}{\\partial \\phi_j} \\Big[ ll + \\lambda (\\sum_l^{k} \\phi_l - 1) \\Big] \\\\ & = \\sum_i w_j^{(i)} \\frac{1}{\\phi_j} + \\lambda \\stackrel{set}{=} 0\\\\ \\end{align}</div> \n",
    "\n",
    "<p>Rearrange and we will have $\\phi_j = -\\frac{\\sum_i w_j^{(i)}}{\\lambda}$. Recall that $\\sum_j \\phi_j = 1$, we have:</p> \n",
    "\n",
    "\n",
    "<div>\\begin{align} \\sum_j \\phi_j & = \\sum_j -\\frac{\\sum_i w_j^{(i)}}{\\lambda} = 1\\\\ \\lambda & = -\\sum_j \\sum_i w_j^{(i)} \\\\ & = -\\sum_j \\sum_i p(z^{(i)}=j\\vert x^{(i)}) \\\\ & = -\\sum_i 1 = -m \\end{align}</div> \n",
    "\n",
    "\n",
    "<p>Finally, we have:</p> \n",
    "\n",
    "$$\\phi_j = \\frac{\\sum_i w_j^{(i)}}{m}$$ <hr /> \n",
    "\n",
    "<h3 id=\"real-example\">Real example</h3> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM(object):\n",
    "    def __init__(self, X, k=2):\n",
    "        # dimension\n",
    "        X = np.asarray(X)\n",
    "        self.m, self.n = X.shape\n",
    "        self.data = X.copy()\n",
    "        # number of mixtures\n",
    "        self.k = k\n",
    "        \n",
    "    def _init(self):\n",
    "        # init mixture means/sigmas\n",
    "        self.mean_arr = np.asmatrix(np.random.random((self.k, self.n)))\n",
    "        self.sigma_arr = np.array([np.asmatrix(np.identity(self.n)) for i in range(self.k)])\n",
    "        self.phi = np.ones(self.k)/self.k\n",
    "        self.w = np.asmatrix(np.empty((self.m, self.k), dtype=float))\n",
    "        #print(self.mean_arr)\n",
    "        #print(self.sigma_arr)\n",
    "    \n",
    "    def fit(self, tol=1e-4):\n",
    "        self._init()\n",
    "        num_iters = 0\n",
    "        ll = 1\n",
    "        previous_ll = 0\n",
    "        while(ll-previous_ll > tol):\n",
    "            previous_ll = self.loglikelihood()\n",
    "            self._fit()\n",
    "            num_iters += 1\n",
    "            ll = self.loglikelihood()\n",
    "            print('Iteration %d: log-likelihood is %.6f'%(num_iters, ll))\n",
    "        print('Terminate at %d-th iteration:log-likelihood is %.6f'%(num_iters, ll))\n",
    "    \n",
    "    def loglikelihood(self):\n",
    "        ll = 0\n",
    "        for i in range(self.m):\n",
    "            tmp = 0\n",
    "            for j in range(self.k):\n",
    "                #print(self.sigma_arr[j])\n",
    "                tmp += sp.stats.multivariate_normal.pdf(self.data[i, :], \n",
    "                                                        self.mean_arr[j, :].A1, \n",
    "                                                        self.sigma_arr[j, :]) *\\\n",
    "                       self.phi[j]\n",
    "            ll += np.log(tmp) \n",
    "        return ll\n",
    "    \n",
    "    def _fit(self):\n",
    "        self.e_step()\n",
    "        self.m_step()\n",
    "        \n",
    "    def e_step(self):\n",
    "        # calculate w_j^{(i)}\n",
    "        for i in range(self.m):\n",
    "            den = 0\n",
    "            for j in range(self.k):\n",
    "                num = sp.stats.multivariate_normal.pdf(self.data[i, :], \n",
    "                                                       self.mean_arr[j].A1, \n",
    "                                                       self.sigma_arr[j]) *\\\n",
    "                      self.phi[j]\n",
    "                den += num\n",
    "                self.w[i, j] = num\n",
    "            self.w[i, :] /= den\n",
    "            assert self.w[i, :].sum() - 1 < 1e-4\n",
    "            \n",
    "    def m_step(self):\n",
    "        for j in range(self.k):\n",
    "            const = self.w[:, j].sum()\n",
    "            self.phi[j] = 1/self.m * const\n",
    "            _mu_j = np.zeros(self.n)\n",
    "            _sigma_j = np.zeros((self.n, self.n))\n",
    "            for i in range(self.m):\n",
    "                _mu_j += (self.data[i, :] * self.w[i, j])\n",
    "                _sigma_j += self.w[i, j] * ((self.data[i, :] - self.mean_arr[j, :]).T * (self.data[i, :] - self.mean_arr[j, :]))\n",
    "                #print((self.data[i, :] - self.mean_arr[j, :]).T * (self.data[i, :] - self.mean_arr[j, :]))\n",
    "            self.mean_arr[j] = _mu_j / const\n",
    "            self.sigma_arr[j] = _sigma_j / const\n",
    "        #print(self.sigma_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.multivariate_normal([0, 3], [[0.5, 0], [0, 0.8]], 20)\n",
    "X = np.vstack((X, np.random.multivariate_normal([20, 10], np.identity(2), 50)))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: log-likelihood is -401.757530\n",
      "Iteration 2: log-likelihood is -313.179330\n",
      "Iteration 3: log-likelihood is -273.071169\n",
      "Iteration 4: log-likelihood is -216.392571\n",
      "Iteration 5: log-likelihood is -215.602209\n",
      "Iteration 6: log-likelihood is -215.602209\n",
      "Terminate at 6-th iteration:log-likelihood is -215.602209\n"
     ]
    }
   ],
   "source": [
    "gmm = GMM(X)\n",
    "gmm.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.09072331,  2.9877357 ],\n",
       "        [20.06700999, 10.07334452]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.mean_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.5926634 , -0.13837292],\n",
       "        [-0.13837292,  0.2280045 ]],\n",
       "\n",
       "       [[ 0.86967146, -0.24890064],\n",
       "        [-0.24890064,  1.07548725]]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.sigma_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.28571429, 0.71428571])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm.phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
