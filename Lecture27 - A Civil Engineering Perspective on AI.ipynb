{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Industrialization of AI\n",
    "### Barriers to AI Solutions Today\n",
    "According to a McKinsey CEO survey in 2018, only 39% of the companies made significant progress in implementing AI applications. Up to 7% of the companies made no progress.\n",
    "\n",
    "<figure>\n",
    "  <div class=\"row\">\n",
    "    <div class=\"col one\">\n",
    "      <img src=\"images/companies.png\" />\n",
    "    </div>\n",
    "  </div>\n",
    "  <figcaption>\n",
    "    A Result of McKinsey CEO Survey\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "In addition, according to the survey, top barriers to AI applications are lack of talent and knowledge as well as AI technology and maturity.\n",
    "\n",
    "To develop their AI applications, the companies need to either build their own AI systems or deploy their applications on a third-party platform. However, issues exist on both paths:\n",
    "* **Building is infeasible**  \n",
    "  * Scarce resources in data scientists, ML engineers and system engineers  \n",
    "  * Little or no enterprise support from open source software  \n",
    "  * Major infrastructure requirements not satisfied  \n",
    "  * Long development timelines  \n",
    "  * High delivery risks  \n",
    "* **Lack of viable third-party platform**\n",
    "  * Limited to cloud deployment\n",
    "  * Limited customization\n",
    "  * Limited service\n",
    "  * Limited scalability\n",
    "  * Limited capabilities\n",
    "  \n",
    "<figure>\n",
    "  <div class=\"row\">\n",
    "    <div class=\"col\">\n",
    "      <img src=\"images/interoperability.png\" />\n",
    "    </div>\n",
    "  </div>\n",
    "  <figcaption>\n",
    "    Inter-operability between diverse systems\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Inter-operability between diverse systems is another main concern when developing AI applications. As shown in the figure above, there are many communications between systems:\n",
    "* Communications between Data/ML Process Builder and ML Systems/DL Systems/Data Systems\n",
    "* Communications between Distributed Communications Backend and ML Systems/DL Systems\n",
    "* Communications between Hardware Resource Management Module and Data Systems\n",
    "* Communications between Containers & Storage Volumes and Hardware Resource Management Module\n",
    "* Communications between Containers & Storage Volumes and Distributed Communications Backend\n",
    "\n",
    "### Transforming AI into Civil Engineering\n",
    "**Duty of a AI System Builder**:\n",
    "* **Build**\n",
    "  * Build a full, engineering-wise sound solution, not a toy or a demo\n",
    "  * Create Many copies\n",
    "  * Build at low expense\n",
    "  * Satisfy any client-specified conditions\n",
    "* **Understand**\n",
    "  * Understand how a solution is built\n",
    "  * Understand the feasibility of proposed solutions\n",
    "  * Explain why the solution works and why not if undesirable outcome\n",
    "* **Sustain**\n",
    "  * Operationalize the solution (deploy, train, etc.)\n",
    "  * Guarantee reproducible results\n",
    "  * Teach others to build the solution and reproduce the result\n",
    "\n",
    "New generation of AI programs should be:\n",
    "* **Reusable**: pluggable modules allowing enterprises to architect and customize AI rapidly\n",
    "* **Standard**: instruction sets and pre-built functions ensuring outcomes are reproducible and sound\n",
    "* **Comprehensive**: inventory of ML/System nuts and bolts meeting the needs for any data type, wide range of tasks, commodity hardware, and mission sizes.\n",
    "\n",
    "<figure>\n",
    "  <div class=\"row\">\n",
    "    <div class=\"col\">\n",
    "      <img src=\"images/first_principles.png\" />\n",
    "    </div>\n",
    "  </div>\n",
    "  <figcaption>\n",
    "    Designing ML systems with first principles\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "**First principles**:  \n",
    "To enable such a transformation of AI into civil engineering, principled ML process is required. First principles are sound, explainable and verifiable. Consider the figure above. According to first principles, instead of mixing everything together in a ML system, we should first explicitly define components of a ML process and then incorporate different techniques to implement the components. E.g., we choose a margin loss as our loss function. we choose neural network as our model structure. We use sparsity constraints in the objective function. We use gradient descent as our algorithm to solve the problem. We observe the change of value of objective function to decide when to stop.\n",
    "\n",
    "**Basic ML principles**: Consistency, Bias v/s variance, Sample complexity, Learning rate, Convergence, Error bound, Confidence, Stability, etc..\n",
    "\n",
    "<figure>\n",
    "  <div class=\"row\">\n",
    "    <div class=\"col one\">\n",
    "      <img src=\"images/PAC.png\" />\n",
    "    </div>\n",
    "  </div>\n",
    "  <figcaption>\n",
    "    Relations among ML principles in PAC Learning Theory\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "An example of managing different ML principles exists in Probably Approximately Correct (PAC) Learning Theory. Sample size $m$, representational complexity $H$, error rate $\\epsilon$ and failure probability $\\delta$ satisfy the following inequality:\n",
    "\n",
    "$$m \\geq \\frac{1}{\\epsilon}(ln|H|+ln(\\frac{1}{\\delta}))$$\n",
    "\n",
    "**Loss Augmentation**:  \n",
    "In **Bayesian Learning**, distribution parameters $\\theta$ is treated as a random variable. According to Bayes' Rule, the posteriori distribution of $\\theta$ after data is seen is:\n",
    "\n",
    "$$p(\\theta \\mid D) = \\frac{p(D \\mid \\theta)p(\\theta)}{p(D)} = \\frac{p(D \\mid \\theta)p(\\theta)}{\\int p(D \\mid \\theta)p(\\theta)d\\theta}$$\n",
    "\n",
    "In **Frequentist Regularization**, parameter $\\theta$ is treated as an unknown constant. The regularized estimate of $\\theta$ after data is seen is:\n",
    "\n",
    "$$\\mathop{\\arg\\max}\\limits_{\\theta} = \\mathcal{L}(\\{x_i, y_i\\}_{i=1}^{N};\\theta) + \\Omega(\\theta)$$\n",
    "\n",
    "The prior of $\\theta$ $p(\\theta)$ in Bayesian Learning, or the regularizer $\\Omega(\\theta)$ in Frequentist Regularization, encodes our prior knowledge about the domain.\n",
    "\n",
    "**Inference Methods**:  \n",
    "In **Variational Inference**, we want to infer the distribution of latent variable $Z$ given the observable variable $X$ by maximizing the expectation of the log probability of $X$ under the posterior of $Z$.\n",
    "\n",
    "<d-math block>\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{q_\\phi(z \\mid x)} log p(x)\n",
    "&= \\int q_\\phi(z \\mid x)logp(x)dz \\\\\n",
    "&= \\int q_\\phi(z \\mid x)log\\frac{p_\\theta(x, z)}{p_\\theta(z \\mid x)}dz \\\\\n",
    "&= \\int q_\\phi(z \\mid x)log\\frac{p_\\theta(x, z)}{q_\\phi(z \\mid x)}\\frac{q_\\phi(z \\mid x)}{p_\\theta(z \\mid x)}dz \\\\\n",
    "&= \\int q_\\phi(z \\mid x)log\\frac{p_\\theta(x, z)}{q_\\phi(z \\mid x)}dz + \\int q_\\phi(z \\mid x)log\\frac{q_\\phi(z \\mid x)}{p_\\theta(z \\mid x)}dz \\\\\n",
    "&= KL(q_\\phi(z \\mid x) || p_\\theta(z \\mid x)) + \\mathbb{E}_{q_\\phi(z \\mid x)} log\\frac{p_\\theta(x, z)}{q_\\phi(z \\mid x)}\n",
    "\\end{aligned}\n",
    "</d-math>\n",
    "\n",
    "As shown above, since $KL(q_\\phi(z \\mid x) || p_\\theta(z \\mid x)) \\geq 0$, we have the lower bound of the true objective function  \n",
    "is:\n",
    "\n",
    "$$\\mathcal{L}(\\theta, \\phi;x) = \\mathbb{E}_{q_\\phi(z \\mid x)} log\\frac{p_\\theta(x, z)}{q_\\phi(z \\mid x)}$$\n",
    "\n",
    "We maximize this variational lower bound of the true objective function instead, or equivalently, minimize the free energy \n",
    "\n",
    "$$F(\\theta, \\phi;x) = -\\mathbb{E}_{q_\\phi(z \\mid x)} log p(x) + KL(q_\\phi(z \\mid x) || p_\\theta(z \\mid x))$$\n",
    "\n",
    ".We further transform the variational lower bound as the following:\n",
    "\n",
    "<d-math block>\n",
    "\\begin{aligned}\n",
    "\\mathcal{L}(\\theta, \\phi;x)\n",
    "&= \\int q_\\phi(z \\mid x)log\\frac{p_\\theta(x, z)}{q_\\phi(z \\mid x)}dz \\\\\n",
    "&= \\int q_\\phi(z \\mid x)log\\frac{p_\\theta(x \\mid z)p(z)}{q_\\phi(z \\mid x)}dz \\\\\n",
    "&= \\int q_\\phi(z \\mid x)log p_\\theta(x \\mid z)dz + \\int q_\\phi(z \\mid x)log\\frac{p(z)}{q_\\phi(z \\mid x)}dz \\\\\n",
    "&= \\mathbb{E}_{q_\\phi(z \\mid x)} log p_\\theta(x \\mid z) - \\int q_\\phi(z \\mid x)log\\frac{q_\\phi(z \\mid x)}{p(z)}dz \\\\\n",
    "&= \\mathbb{E}_{q_\\phi(z \\mid x)} log p_\\theta(x \\mid z) - KL(q_\\phi(z \\mid x) || p(z))\n",
    "\\end{aligned}\n",
    "</d-math>\n",
    "\n",
    "E-step: We maximize $\\mathcal{L}(\\theta, \\phi;x)$ w.r.t. $\\phi$ with $\\theta$ fixed.  \n",
    "M-step: We maximize $\\mathcal{L}(\\theta, \\phi;x)$ w.r.t. $\\theta$ with $\\phi$ fixed.\n",
    "\n",
    "**Demistifying and Unifying different models**:  \n",
    "As an example, we want to unifying the zoo of Deep Generative Models (DGMs): GANs, VAEs, WS, VI, etc..\n",
    "\n",
    "Commonness: All with implicit distributions and black-box Neural Network models in terms of the formulation.  \n",
    "Difference: Different space complexity and loss functions, e.g., GAN uses an adversarial loss while VAE uses a maximum likelihood loss.\n",
    "\n",
    "**Designing proper model/algorithms with Guarantees**:  \n",
    "Assume we solve a regression problem as the following:\n",
    "\n",
    "$$\\mathop{\\arg\\max}\\limits_{\\theta} = \\mathcal{L}(\\{x_i, y_i\\}_{i=1}^{N};\\theta) + \\Omega(\\theta)$$\n",
    "\n",
    "To provide an algorithm with guarantees, we use an iterative-convergent gradient algorithm as the following:\n",
    "\n",
    "$$\\theta^{(t)} = \\theta^{(t-1)} + \\epsilon\\nabla\\mathcal{L}(\\theta^{(t-1)}, D^{(t)})$$\n",
    "\n",
    "**Good Generalizability of models**: \n",
    "\n",
    "<figure>\n",
    "  <div class=\"row\">\n",
    "    <div class=\"col\">\n",
    "      <img src=\"images/Overfitting.png\" />\n",
    "    </div>\n",
    "  </div>\n",
    "  <figcaption>\n",
    "    Models with different generalizability\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "We should avoid our models from underfitting and overfitting.\n",
    "\n",
    "**Case Analysis: Medical Imaging Report Generation**  \n",
    "In traditional way of generating imaging report, the radiology observations and findings regarding each area of the\n",
    "body are first examined in the imaging study, then the findings, the patient clinical history and the indication for the imaging study are combined to provide a diagnosis.\n",
    "\n",
    "However, there are difficulties in Automatic Imaging Report Generation:\n",
    "* Abnormal regions in medical images are difficult to identify.\n",
    "* The reports are typically long, containing multiple sentences.\n",
    "* Each sentence discusses a specific topic so localization of the image regions and tags that are relevant to this topic is required.\n",
    "\n",
    "<figure>\n",
    "  <div class=\"row\">\n",
    "    <div class=\"col\">\n",
    "      <img src=\"images/Imaging_Report_Generation.png\" />\n",
    "    </div>\n",
    "  </div>\n",
    "  <figcaption>\n",
    "    Models with different generalizability\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "Several models and techniques are used here:\n",
    "* Semi-supervised learning for lesion tag prediction.\n",
    "* Hierarchical LSTM for long-paragraph generation.\n",
    "* Visual-semantic co-attention to localize the relevant image regions and tags for each sentence to be generated.\n",
    "\n",
    "**Building v/s Crafting**:  \n",
    "To realize AI version of civil engineering, understanding the term of \"building\" is extremely important. Building entails the design of the system structure, the plan of building the system, the budget for the system, the requirement for materials (data, knowledge, infrastructure, etc.), the methods used in the system, the implementation details, the integration of different modules, the refinement/tuning of the system to get optimal performance, the deployment of the system on other platforms, the test of the system under normal and stress case, the maintenance and upgrade of the system and the security of the system.\n",
    "\n",
    "**Case Analysis: Solving a ML Program**  \n",
    "Assume we are solving a Machine Learning Program as the following. Our model is represented in the objective, our data is $y$ and our parameter is $\\theta$.\n",
    "\n",
    "$$\\mathop{\\arg\\max}\\limits_{\\theta} = \\mathcal{L}(\\{x_i, y_i\\}_{i=1}^{N};\\theta) + \\Omega(\\theta)$$\n",
    "\n",
    "We use an iterative convergent algorithm to solve it, usually **Gradient Descent** as the following.\n",
    "\n",
    "$$\\theta^{(t)} = \\theta^{(t-1)} + \\eta\\nabla\\mathcal{L}(\\theta^{(t-1)})$$\n",
    "\n",
    "However, if the regularizer is not differentiable (e.g., L-1 norm), we cannot use gradient descent here. Here is how **Proximal Gradient Descent** comes into place.\n",
    "\n",
    "<d-math block>\n",
    "\\begin{aligned}\n",
    "v^{(t)} &\\leftarrow \\theta^{(t-1)} - \\eta\\nabla_{\\theta^{(t-1)}}\\mathcal{L}(\\theta^{(t-1)}) \\\\\n",
    "\\theta^{(t)} &\\leftarrow \\mathop{\\arg\\min}\\limits_{z}\\frac{1}{2\\eta}||v^{(t)} - z||^{2} + \\Omega(z)\n",
    "\\end{aligned}\n",
    "</d-math>\n",
    "\n",
    "Here $\\mathop{\\arg\\min}\\limits_{z}\\frac{1}{2\\eta}||\\theta^{(t)} - z||^{2} + \\Omega(z)$ is the proximal map for $\\theta$. For every iteration, we first compute the gradient of  \n",
    "$\\theta$, and then projected $\\theta$ onto the proximal map. The time complexity of Proximal Gradient Descent is $O(1/\\eta t)$.\n",
    "\n",
    "To accelerate Proximal Gradient Descent, we have an accelerated version, which is also called **Fast Iterative Shrinkage-Thresholding Algorithm (FISTA)**.\n",
    "\n",
    "<d-math block>\n",
    "\\begin{aligned}\n",
    "v^{(t)} &\\leftarrow \\theta^{(t-1)} - \\eta\\nabla_{\\theta^{(t-1)}}\\mathcal{L}(\\theta^{(t-1)}) \\\\\n",
    "u^{(t)} &\\leftarrow \\mathop{\\arg\\min}\\limits_{z}\\frac{1}{2\\eta}||v^{(t)} - z||^{2} + \\Omega(z) \\\\\n",
    "\\theta^{(t)} &\\leftarrow u^{(t)} + \\frac{t-1}{t+2}(u^{(t)} - u^{(t-1)})\n",
    "\\end{aligned}\n",
    "</d-math>\n",
    "\n",
    "As shown above, for every iteration, the first two steps of the algorithm is the same with Proximal Gradient Descent. At the third step, the momentum is considered to update $\\theta$. With acceleration, the time complexity is boosted to $O(1/\\eta t^2)$.\n",
    "\n",
    "To use Moreau envelope as smooth approximation of the objective of Proximal Gradient Descent has a rich and long history in convex analysis. Inspired from proximal point algorithm, which is exactly the second step for every iteration of Proximal Gradient Descent, an approximation of the objective is given as the following:\n",
    "\n",
    "$$\\min\\limits_{\\theta} \\mathcal{L}(\\theta) + \\Omega(\\theta) \\approx \\min\\limits_{\\theta} M_{\\mathcal{L}}^{\\eta}(\\theta) + \\Omega(\\theta)$$\n",
    "\n",
    "The **Smooth Proximal Gradient Descent** first computes the proximal map of the last $\\theta$ on the differentiable part of the objective, and then computes the proximal map of the result of the first step on the non-differentiable part. Finally, it applies the momentum similar to Accelerated Proximal Gradient Descent.\n",
    "\n",
    "<d-math block>\n",
    "\\begin{aligned}\n",
    "v^{(t)} &\\leftarrow \\mathop{\\arg\\min}\\limits_{z}\\frac{1}{2\\eta}||\\theta^{(t-1)} - z||^{2} + \\mathcal{L}(z) \\\\\n",
    "u^{(t)} &\\leftarrow \\mathop{\\arg\\min}\\limits_{z}\\frac{1}{2\\eta}||v^{(t)} - z||^{2} + \\Omega(z) \\\\\n",
    "\\theta^{(t)} &\\leftarrow u^{(t)} + \\frac{t-1}{t+2}(u^{(t)} - u^{(t-1)})\n",
    "\\end{aligned}\n",
    "</d-math>\n",
    "\n",
    "**Data-Parallel for large-scale problems**:\n",
    "\n",
    "<figure>\n",
    "  <div class=\"row\">\n",
    "    <div class=\"col\">\n",
    "      <img src=\"images/data_parallel.png\" />\n",
    "    </div>\n",
    "  </div>\n",
    "  <figcaption>\n",
    "    Models with different generalizability\n",
    "  </figcaption>\n",
    "</figure>\n",
    "\n",
    "If data is too large to fit in a single worker, divide it among P workers. For every iteration, each worker receives a stale sub-updates and all the sub-updates are aggregated to get the total update.\n",
    "\n",
    "\n",
    "## Modularization and Standardization\n",
    "Our goal is to systematically order the nuts and bolts so that we can easily pick the components we need. There are three stages of nuts and bolts:\n",
    "\n",
    "<latex-js style=\"border: 1px dashed #aaa;\">\n",
    "\\begin{itemize}\n",
    "  \\item Data wrangling: projections, transformations and embeddings of data\n",
    "  \\item Machine learning: loss functions, regularizers/priors, training algorithms\n",
    "  \\item System harmonization: distributed ML, parameter servers\n",
    "\\end{itemize}\n",
    "\n",
    "\n",
    "</latex-js>\n",
    "\n",
    "Each stage can be further broken down into steps. Data wrangling steps are ingestion, cleaning and improvement, feature engineering, embedding, and integration. Machine learning steps are build, augment and tune, train, score, and experiment. The steps for system harmonization are deploy, storage, compatibility, user accounts and security, and project management.\n",
    "\n",
    "### Interoperability\n",
    "There should be standards for interfaces between nuts and bolts so that any algorithm can be used after any data wrangling method.\n",
    "\n",
    "### Process\n",
    "There should be a workbench where people can easily create, experiment, explain and manage their tasks. A graphical interface may be useful for this process.\n",
    "\n",
    "### Soundness \n",
    "If you have correctly modularized and implemented components, then you guarantee soundness of the system. You can use the theory and guarantees for each component to build the guarantees for the whole system.\n",
    "\n",
    "## Real-World Example: Texar\n",
    "Texar <d-cite key=\"hu2018texar\"></d-cite> is a toolkit for text generation built on these principles.\n",
    "\n",
    "### Language model example\n",
    "A language model computes the probability of a sentence $y = (y_1, y_2,....y_T)$ as $p_\\theta(y) = \\prod_{t=1}^T p_{\\theta}(y_t \\mid y_{t-1})$ according to the Markov assumption. In the simplest case, we can use an HMM to model this. We can allow more dependences by letting each hidden state be an LSTM. We can incorporate additional context by using a conditional language model that computes the conditional probability of a sentence $p_\\theta(y \\mid x) = \\prod_{t=1}^T p_{\\theta}(y_t \\mid y_{t-1}, x)$. The language model can be viewed as a decoder and the context can be modeled as an encoder. With this formulation, we can plug in many different options into the encoder.\n",
    "\n",
    "### Reinforcement learning \n",
    "We can use reinforcement learning to create a closed loop between data sampling and evaluation that allows us to continuously train. For instance, if our test metric is BLEU, and we have a decoder that generates samples $\\hat{y}$ which are used to evaluate the reward, then we can use a reinforcement learning process that automatically updates the decoder based on the BLEU metric after we sample a $\\hat{y}$.\n",
    "\n",
    "\n",
    "\n",
    "The key idea is that the model should be modular and composable; instead of building from scratch, we should be assembling composable parts.\n",
    "\n",
    "\n",
    "\n",
    "## Distributed Machine Learning\n",
    "\n",
    "You might have noticed that this year was the first [SysML](https://www.sysml.cc/) conference (last year's SysML was just a workshop).\n",
    "The goal of this new conference is to promote the co-design of machine learning systems and machine learning algorithms.\n",
    "One interesting area at the intersection of ML and systems is _distributed machine learning_.\n",
    "When implementing ML algorithms in a distributed system, there are a lot of interesting questions that arise such as which quantities to transfer between the machines, how often to transfer those quantities, etc.\n",
    "\n",
    "In this lecture, we'll discuss two interesting architectures for distributed machine learning: the _parameter server_\n",
    "architecture and the _sufficient vector_ architecture.\n",
    "\n",
    "### Parameter servers and bounded asynchrony\n",
    "\n",
    "Suppose that you're trying to train an ML model on a huge dataset, and you have many machines at your disposal.\n",
    "One natural idea is to shard the data across many worker machines, but store the whole\n",
    "model on a single master machine called the _parameter server_ (in practice, the PS could also be\n",
    " group of machines if the model is too big to fit on a single machine).\n",
    "\n",
    "\n",
    "<figure id=\"basic-structure\" class=\"l-body\">\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col two\">\n",
    "            <img src=\"images/parameter-server.png\" />\n",
    "        </div>\n",
    "    </div>\n",
    "</figure>\n",
    "\n",
    "\n",
    "(Interestingly, a very simple instance of the parameter server architecture occurs \n",
    " where you have a single machine with multiple worker threads accessing a model stored in shared memory.)\n",
    "\n",
    "\n",
    "In the most naive PS architecture, each worker retrieves the latest copy of the model from the master / parameter server,\n",
    "then computes model updates using its own subset of the dataset, and then sends its model update back\n",
    "to the master/parameter server, which aggregates all worker's updates.\n",
    "\n",
    "One problem with this naive architecture is that there might be stragglers --- some workers might take longer to compute their\n",
    "updates, and it's inefficient force the remaining workers to wait for the straggler to get its updates in.\n",
    "The solution to this straggler problem is to permit some degree of asynchrony --- model parameters are dispatched \n",
    "asynronously to the workers, before all workers' model updates from the previous iterataion are in.\n",
    "The trick to making this work is to enforce that the amount of asynchrony is bounded.\n",
    "With bounded asynchrony, you can prove convergence theorems.\n",
    "For example,  Theorem 5 in <d-cite key=\"dai\"></d-cite> proves that the \n",
    "difference between the current iterate's objective value and the optimal objective value decreases exponentially.\n",
    "Notice that this bound depends explicitly on the expectation and variance of the delay between all workers in the cluster.\n",
    "These two numbers can be controlled by the operator of the system.\n",
    "\n",
    "<figure id=\"basic-structure\" class=\"l-body\">\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col two\">\n",
    "            <img src=\"images/convergence.png\" />\n",
    "        </div>\n",
    "    </div>\n",
    "</figure>\n",
    "\n",
    "This theorem was the first of its kind when it was published in 2013. \n",
    "It was unique because it modeled parameters of the _system_.\n",
    "Usually, ML theorems just make assumptions about the smoothness of a function or the properties of the data.\n",
    "But that type of theorem is not very useful in practice, because real computer systems can behave in very strange ways.\n",
    "This theorem is interesting because it brings parameters of the real computer system into the analysis.\n",
    "\n",
    "### Sufficient factor broadcasting\n",
    "\n",
    "The trouble with the parameter server architecture is that it requires the whole model to be transmitted\n",
    "between each worker, and the parameter server, in every iteration.\n",
    "This is infeasible for huge models like DNNs which may be gigabytes in size.\n",
    "Fortunately, it turns out that for a certain class of models called _matrix parameterized models_,\n",
    "you can get away with just passing around a low-rank version of the model updates between the workers.\n",
    "This approach is called _sufficient factor broadcasting_.\n",
    "\n",
    "\n",
    "<figure id=\"basic-structure\" class=\"l-body\">\n",
    "    <div class=\"row\">\n",
    "        <div class=\"col two\">\n",
    "            <img src=\"images/sufficient-factors.png\" />\n",
    "        </div>\n",
    "    </div>\n",
    "</figure>\n",
    "\n",
    "\n",
    "In sufficient factor broadcasting, you do **not** have a parameter server which is the ground source \n",
    "of truth for the model weights.\n",
    "Instead, each worker maintains its own copy of the model weights.\n",
    "In every iteration, each worker sends the model updates, computed over its subset of the dataset, as a message to every other worker.\n",
    "Now, it may seem at first as if this is completely impractical -- if you do this in the naive way,\n",
    "the model update $$\\Delta W$$ is just as large as the model itself!\n",
    "\n",
    "However, it turns out that for so-called matrix parameterized models, each model update is low-rank,\n",
    "so rather than passing around the full update, each worker can just send the other workers the low rank factors.\n",
    "\n",
    "Matrix-parameterized models take the following form: \n",
    "\n",
    "$$ \\min_W \\frac{1}{N} \\sum_{i=1}^N f_i (W a_i) + h(W)$$\n",
    "\n",
    "where $$h$$ is a regularizer and $$f_i$$ is the loss for the $$i$$-th data point (which may, for example, implicitly depend on that data point's label).\n",
    "\n",
    "Examples of MPMs include distance metric learning, multi class logistic regression, topic models, sparse coding, neural networks, etc.\n",
    "In multiclass logistic regression, the rows of the parameter matrix are the coefficient vectors for the different classes; in sparse coding, the rows of the parameter matrix are the basis vectors used to reconstruct the observed data.  \n",
    "\n",
    "The trick behind the sufficient factor broadcasting framework is that the SGD update in a matrix-parameterized model is low-rank:\n",
    "\n",
    "$$ \\Delta W = u v^T \\quad u = \\frac{\\partial f(W a_i)}{\\partial (W a_i)} \\quad v = a_i $$\n",
    "\n",
    "Therefore, rather than passing around $$\\Delta W$$, you can just pass around $$u$$ and $$v$$.\n",
    "\n",
    "Interestingly, it's not only the SGD update that's low-rank: the SDCA <d-cite key=\"Shalev\"></d-cite> update is also low-rank.\n",
    "\n",
    "[This paper](http://www.cs.cmu.edu/~pengtaox/papers/uai16_sfb.pdf) provides a detailed analysis of the correctness of this approach.\n",
    "Interestingly, it turns out that each machine doesn’t need to send its updates to _every_ other worker, only to its nearest neighbors.\n",
    "You can prove that the result you get from this protocol is equivalent, under certain conditions, to the result you’d get if you broadcasted the updates to every other worker.\n",
    "\n",
    "### Parameter Server / Sufficient Factor Hybrid\n",
    "\n",
    "The parameter server idea and the sufficient factor idea can be combined.\n",
    "For example, if you’re training a neural network, note that the parameters for the early, convolutional layers take up way less space than the parameters for the later, fully-connected layers.\n",
    "Therefore, you can transmit the convolutional parameters using a parameter server master/slave setup (each worker sends its weight update to the master server, which sends to all the workers), while you can transmit the fully-connected parameters using a peer-to-peer sufficient factor setup.\n",
    "\n",
    "## Programming Interfaces and Software Frameworks\n",
    "\n",
    "Another exciting research direction at the intersection of systems and machine learning are programaming interfaces\n",
    "and software frameworks for deep learning, such as Cavs, a DL framework recently developed by Petuum.\n",
    "\n",
    "Modern DL frameworks are either static graphs, like TensorFlow, or dynamic graphs, like PyTorch.\n",
    "The advantage of the static graph approach is that you can optimize the computational graph.\n",
    "However, the disadvantage is that it's hard for frameworks based on static graphs to handle inputs of variable length, \n",
    "such as sentences with variable number of words or images with a variable size.\n",
    "You have to resort to hacks like zero-padding all sentences to be the length of the largest possible sentence, or creating a number of different static graphs, one for each possible sentence length.\n",
    "\n",
    "Cavs takes a different approach, inspired by graph processing software frameworks.\n",
    "Graph-processing software often follows a vertex-centered approach called _vertex programming_.\n",
    "In vertex programming, the programmer defines each vertex's role, in terms of its inputs from neighboring vertices\n",
    "and its outputs to neighboring vertices. \n",
    "\n",
    "In Cavs, the programmer writes a vertex program, which is optimized.\n",
    "However, at runtime, the graph that is fed to the vertex program can be of any size.\n",
    "Cavs therefore gives the best of both worlds --- the optimizations of a static graph, but without the hasslse.\n",
    "\n",
    "As you can see from the comparison below, Cavs (green bars) is much faster than TensorFlow (brown bars):\n",
    "\n",
    "## Explainable AI\n",
    "\n",
    "Explainability is an important area of research/engineering these days.\n",
    "It's important to keep in mind that explainability is not just about showing the weights of the model.\n",
    "There are several different types of explainability that are equally important: \n",
    "\n",
    "* Data explainability: how the data is processed\n",
    "* Model explainability: what you’ve learned, e.g. feature weights\n",
    "* Inference explainability: how each result is inferred\n",
    "* Process explainability: factors beyond or complementary to the mechanisms / mathematics of ML.\n",
    "\n",
    "### Data explainability\n",
    "\n",
    "An example of a data explainability concern is: how is HTML transformed into an “ML-ready” form?  For example, you might first strip HTML tags, then remote stopwords/punctuation, then embed everything.\n",
    "\n",
    "In order to debug potential problems that arise, you need to be able to interactively visualize the input and output at every stage of this process.\n",
    "\n",
    "### Model explainability\n",
    "\n",
    "Some model explainability concerns are:\n",
    "* Weights: which input features are most important to the result? \n",
    "* Boundaries: which input values cause the result to change qualitatively?\n",
    "* Bases: what are the concepts and examples that are important to the model?\n",
    "\n",
    "### Inference explainability\n",
    "\n",
    "For explainability, it’s not sufficient to just look at the static weights of the model, as different input points may take different paths through the model.\n",
    "Therefore, you need to understand what happens as data travels through the model:\n",
    "* Which weights are activated by the data\n",
    "* Which decision boundaries are close to the data?\n",
    "* What bases are activated by the data?\n",
    "\n",
    "Recent work on contextual explanation networks <d-cite key=\"alshedivat2017contextual\"></d-cite> allow you to incorporate explanations directly into the model.  Intuitively, you train a “personalized” model for every data point.\n",
    "\n",
    "### Process explainability\n",
    "\n",
    "How do factors beyond data, model, and inference affect ML results?\n",
    "* Are there multiple users/tenants sharing the cloud or datacenter resources?\n",
    "* What if one of the computers crashes or becomes slow?\n",
    "* What if the computer network becomes unreliable or slow?\n",
    "* What if there are security breaches? Data outages?\n",
    "\n",
    "These questions all arise in multi-tenant environments like the cloud or a corporate network.\n",
    "\n",
    "## Summary\n",
    "There is still much to do in AI. \n",
    "We are still in the alchemy stage as opposed to the chemistry stage, and certainly not at the chemical engineering stage yet.\n",
    "In the future, when we hopefully will have greater insight into the “periodic table” of machine learning, we will be able to reason about what happens when we combine different ML components, just like chemists know what happens when you combine two different chemicals.\n",
    "But we are not there yet.\n",
    "\n",
    "Once the _science_ of AI is more mature, the real _engineering_ of AI can commence.\n",
    "Just like in 2019 you don't need a physics professor to fix the wiring in your house, in the future one hopefully won't need a PhD\n",
    "in machine learning in order to know how to build AI systems.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
